{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Travelport Event-Driven Microservice practices This site shares some best practices to adopt cloud native, event-driven microservice implementation with Kafka, CI/CD pipeline, Spring Boot Cloud Stream, and API management. The simple solution presents the following components: Repositories part of the proof of technology API management","title":"Home"},{"location":"#travelport-event-driven-microservice-practices","text":"This site shares some best practices to adopt cloud native, event-driven microservice implementation with Kafka, CI/CD pipeline, Spring Boot Cloud Stream, and API management. The simple solution presents the following components:","title":"Travelport Event-Driven Microservice practices"},{"location":"#repositories-part-of-the-proof-of-technology","text":"","title":"Repositories part of the proof of technology"},{"location":"#api-management","text":"","title":"API management"},{"location":"concepts/fit-for-purpose/","text":"Fit for purpose In this note we want to list some of the criteria to consider and assess during an event-driven architecture establishment work and as part of a continuous application governance. This is not fully exhaustive, but give good foundations for analysis and study. Fit for purpose practices should be done under a bigger program about application development governance and data governance. We can look at least to the following major subject: Cloud native applications With the adoption of cloud native and microservice applications (12 factors app), the following needs to be addressed: Responsiveness with elastic scaling and resilience to failure. Which leads to adopt the ' reactive manifesto' and consider messaging as a way to communicate between apps. Elastic also may lead to multi cloud deployment practice. Address data sharing using a push model to improve decoupling, and performance. Instead of having each service using REST end point to pull the data from other services, each service push the change to their main business entity to a event backbone. Each future service which needs those data, pull from the messaging system. Adopting common pattern like command query responsibility segregation to help implementing complex queries, joining different business entities owned by different microservices, event sourcing , transactional outbox and SAGA . Addressing data eventual consistency to propagate change to other components versus ACID transaction. Support always-on approach with the deployment to multiple data centers (at least three) being active/active and being able to propagate data in all data centers. So the net: do we need to implement event-driven microservices because of those needs? Modern data pipeline As new business applications need to react to events in real time, the adoption of event backbone is really part of the IT toolbox. Some existing deployment consider this to be their new data hub, where all the data about the 'customer' is accessible. Therefore, it is natural to assess the data movement strategy and offload some of those ETL jobs running at night, as most of those works are done already inside of the applications generating those data, but not those data are visible inside the backbone. We detailed the new architecture in this modern data lake discussion, so from a fit for purpose point of view, we need to assess what those ETL jobs were doing and how much of those data is now visible to other to consume. With Event Backbone like Kafka, any consumer can join the consumption at any point of time, within the retention period. So if new data is kept like 10 days, within those 10 days a consumer can continuously get the data, no more wait for the next morning, just connected to the topic you need to. MQ Versus Kafka Consider queue system. like IBM MQ, for: Exactly once delivery, and to participate into two phase commit transaction Asynchronous request / reply communication: the semantic of the communication is for one component to ask a second command to do something on its data. This is a command pattern with delay on the response. Recall messages in queue are kept until consumer(s) got them. Consider Kafka as pub/sub and persistence system for: Publish events as immutable facts of what happen in an application Get continuous visibility of the data Streams Keep data once consumed, for future consumers, for replay-ability Scale horizontally the message consumption Direct product feature comparison Kafka IBM MQ Kafka is a pub/sub engine with streams and connectors MQ is a queue, pub/sub engine with file transfer, MQTT, AMQP and other capabilities All topics are persistent Queues and topics can be persistent or non persistent All subscribers are durable Subscribers can be durable or non durable Adding brokers to requires little work (changing a configuration file) Adding QMGRs requires some work (Add the QMGRs to the cluster, add cluster channels. Queues and Topics need to be added to the cluster.) Topics can be spread across brokers (partitions) with a command Queues and topics can be spread across a cluster by adding them to clustered QMGRs Producers and Consumers are aware of changes made to the cluster All MQ clients require a CCDT file to know of changes if not using a gateway QMGR Can have n number of replication partitions Can have 2 replicas (RDQM) of a QMGR, Multi Instance QMGRs Simple load balancing Load balancing can be simple or more complex using weights and affinity Can reread messages Cannot reread messages that have been already processed All clients connect using a single connection method MQ has Channels which allow different clients to connect, each having the ability to have different security requirements Data Streams processing built in, using Kafka topic for efficiency Stream processing is not built in, but using third party libraries, like MicroProfile Reactive Messaging, ReactiveX, etc. Has connection security, authentication security, and ACLs (read/write to Topic) Has connection security, channel security, authentication security, message security/encryption, ACLs for each Object, third party plugins (Channel Exits) Built on Java, so can run on any platform that support Java 8+ Latest native on AIX, IBM i, Linux systems, Solaris, Windows, z/OS, run as Container Monitoring by using statistics provided by Kafka CLI, open source tools, Prometheus Monitoring using PCF API, MQ Explorer, MQ CLI (runmqsc), Third Party Tools (Tivoli, CA APM, Help Systems, Open Source, etc) Events and Messages There is a long history of messaging in IT systems. You can easily see an event driven solution and events in the context of messaging systems and messages. However, there are different characteristics that are worth considering: Messaging: Messages transport a payload and messages are persisted until consumed. Message consumers are typically directly targeted and related to the producer who cares that the message has been delivered and processed. Events: Events are persisted as a replay-able stream history. Event consumers are not tied to the producer. An event is a record of something that has happened and so can't be changed. (You can't change history.) Messaging versus event streaming We recommend reading this article and this one , to get insight on messaging (focusing on operations / actions to be performed by a system or service) versus events (focusing on the state / facts of a system with no knowledge of the downstream processing). To summarize messaging (like MQ) are to support: Transient Data: data is only stored until a consumer has processed the message, or it expires. Request / reply most of the time. Targeted reliable delivery: targeted to the entity that will process the request or receive the response. Reliable with transaction support. Highly Coupled producers and consumers For events: Stream History: consumers are interested in historic events, not just the most recent. Scalable Consumption: A single event is consumed by many consumers with limited impact as the number of consumers grow. Immutable Data Loosely coupled / decoupled producers and consumers","title":"Fit for purpose"},{"location":"concepts/fit-for-purpose/#fit-for-purpose","text":"In this note we want to list some of the criteria to consider and assess during an event-driven architecture establishment work and as part of a continuous application governance. This is not fully exhaustive, but give good foundations for analysis and study. Fit for purpose practices should be done under a bigger program about application development governance and data governance. We can look at least to the following major subject:","title":"Fit for purpose"},{"location":"concepts/fit-for-purpose/#cloud-native-applications","text":"With the adoption of cloud native and microservice applications (12 factors app), the following needs to be addressed: Responsiveness with elastic scaling and resilience to failure. Which leads to adopt the ' reactive manifesto' and consider messaging as a way to communicate between apps. Elastic also may lead to multi cloud deployment practice. Address data sharing using a push model to improve decoupling, and performance. Instead of having each service using REST end point to pull the data from other services, each service push the change to their main business entity to a event backbone. Each future service which needs those data, pull from the messaging system. Adopting common pattern like command query responsibility segregation to help implementing complex queries, joining different business entities owned by different microservices, event sourcing , transactional outbox and SAGA . Addressing data eventual consistency to propagate change to other components versus ACID transaction. Support always-on approach with the deployment to multiple data centers (at least three) being active/active and being able to propagate data in all data centers. So the net: do we need to implement event-driven microservices because of those needs?","title":"Cloud native applications"},{"location":"concepts/fit-for-purpose/#modern-data-pipeline","text":"As new business applications need to react to events in real time, the adoption of event backbone is really part of the IT toolbox. Some existing deployment consider this to be their new data hub, where all the data about the 'customer' is accessible. Therefore, it is natural to assess the data movement strategy and offload some of those ETL jobs running at night, as most of those works are done already inside of the applications generating those data, but not those data are visible inside the backbone. We detailed the new architecture in this modern data lake discussion, so from a fit for purpose point of view, we need to assess what those ETL jobs were doing and how much of those data is now visible to other to consume. With Event Backbone like Kafka, any consumer can join the consumption at any point of time, within the retention period. So if new data is kept like 10 days, within those 10 days a consumer can continuously get the data, no more wait for the next morning, just connected to the topic you need to.","title":"Modern data pipeline"},{"location":"concepts/fit-for-purpose/#mq-versus-kafka","text":"Consider queue system. like IBM MQ, for: Exactly once delivery, and to participate into two phase commit transaction Asynchronous request / reply communication: the semantic of the communication is for one component to ask a second command to do something on its data. This is a command pattern with delay on the response. Recall messages in queue are kept until consumer(s) got them. Consider Kafka as pub/sub and persistence system for: Publish events as immutable facts of what happen in an application Get continuous visibility of the data Streams Keep data once consumed, for future consumers, for replay-ability Scale horizontally the message consumption","title":"MQ Versus Kafka"},{"location":"concepts/fit-for-purpose/#direct-product-feature-comparison","text":"Kafka IBM MQ Kafka is a pub/sub engine with streams and connectors MQ is a queue, pub/sub engine with file transfer, MQTT, AMQP and other capabilities All topics are persistent Queues and topics can be persistent or non persistent All subscribers are durable Subscribers can be durable or non durable Adding brokers to requires little work (changing a configuration file) Adding QMGRs requires some work (Add the QMGRs to the cluster, add cluster channels. Queues and Topics need to be added to the cluster.) Topics can be spread across brokers (partitions) with a command Queues and topics can be spread across a cluster by adding them to clustered QMGRs Producers and Consumers are aware of changes made to the cluster All MQ clients require a CCDT file to know of changes if not using a gateway QMGR Can have n number of replication partitions Can have 2 replicas (RDQM) of a QMGR, Multi Instance QMGRs Simple load balancing Load balancing can be simple or more complex using weights and affinity Can reread messages Cannot reread messages that have been already processed All clients connect using a single connection method MQ has Channels which allow different clients to connect, each having the ability to have different security requirements Data Streams processing built in, using Kafka topic for efficiency Stream processing is not built in, but using third party libraries, like MicroProfile Reactive Messaging, ReactiveX, etc. Has connection security, authentication security, and ACLs (read/write to Topic) Has connection security, channel security, authentication security, message security/encryption, ACLs for each Object, third party plugins (Channel Exits) Built on Java, so can run on any platform that support Java 8+ Latest native on AIX, IBM i, Linux systems, Solaris, Windows, z/OS, run as Container Monitoring by using statistics provided by Kafka CLI, open source tools, Prometheus Monitoring using PCF API, MQ Explorer, MQ CLI (runmqsc), Third Party Tools (Tivoli, CA APM, Help Systems, Open Source, etc)","title":"Direct product feature comparison"},{"location":"concepts/fit-for-purpose/#events-and-messages","text":"There is a long history of messaging in IT systems. You can easily see an event driven solution and events in the context of messaging systems and messages. However, there are different characteristics that are worth considering: Messaging: Messages transport a payload and messages are persisted until consumed. Message consumers are typically directly targeted and related to the producer who cares that the message has been delivered and processed. Events: Events are persisted as a replay-able stream history. Event consumers are not tied to the producer. An event is a record of something that has happened and so can't be changed. (You can't change history.)","title":"Events and Messages"},{"location":"concepts/fit-for-purpose/#messaging-versus-event-streaming","text":"We recommend reading this article and this one , to get insight on messaging (focusing on operations / actions to be performed by a system or service) versus events (focusing on the state / facts of a system with no knowledge of the downstream processing). To summarize messaging (like MQ) are to support: Transient Data: data is only stored until a consumer has processed the message, or it expires. Request / reply most of the time. Targeted reliable delivery: targeted to the entity that will process the request or receive the response. Reliable with transaction support. Highly Coupled producers and consumers For events: Stream History: consumers are interested in historic events, not just the most recent. Scalable Consumption: A single event is consumed by many consumers with limited impact as the number of consumers grow. Immutable Data Loosely coupled / decoupled producers and consumers","title":"Messaging versus event streaming"},{"location":"concepts/terms-and-definitions/","text":"Terms & Definitions Events Events are notifications of change of state. Notifications are issued, or published and interested parties can subscribe and take action on the events. Typically, the issuer of the notification has no knowledge of what action is taken and receives no corresponding feedback that the notification has been processed. Events are notifications of change of state. Typically, events represent the change of state of something of interest to the business. Events are records of something that has happened. Events can't be changed, that is, they are immutable. (We can't change something that has happened in the past). Event streams An event stream is a continuous unbounded series of events. The start of the stream may have occurred before we started to process the stream. The end of the stream is at some unknown point in the future. Events are ordered by the point in time at which each event occurred. When developing event driven solutions, you will typically see two types of event streams: Event streams whose events are defined and published into a stream as part of a solution. Event streams that connect to a real-time event stream, for example from an IOT device, a voice stream from a telephone system, a video stream, or ship or plane locations from global positioning systems. Event backbone The event backbone is the communication layer in the event driven architecture. It provides the connection between event driven capabilities and in the Cloud Native , it becomes the Pub/Sub communication layer for event driven microservices. At this high level we would consider two types of relevant technologies for the event backbone, Message Brokers and Event Logs . Both technology types could be used to achieve the event communication style, with the \"Publish and subscribe\" model however, it is also important to consider other capabilities which are frequently used within event driven solutions: Keeping an Event Log as a time sequenced as it happened recording of events (Source of the truth). Enabling direct replay of events. Enabling Event Sourcing as a way of recording state changes in distributed systems. Enabling programmatic access to the continuous event stream . When viewed across these wider event driven capabilities, an event log style technology can provide a central component which can support all of these capabilities, whereas a message broker would have to be extended with other components. Selecting the Event Backbone for the reference architecture For the event-driven architecture, we defined the following characteristics to be essential for the event backbone: Publish-subscribe event communication between event producers and consumers Facilitate many consumers with shared central \u201csource of truth\u201d. Capability to store events for a given period of time (event log). This is the shared source of the truth for events. Ability for consumers to subscribe to events. Provide replay of events from history for evolving application instances. Provide programmatic access to continuous stream of events, with minimum time lag. Must be highly scalable and resilient to cloud deployment levels. While choosing an event backbone for your event-driven application development, the following functional and non-functional requirements should be taken into consideration: Persistence When source systems generate events, the consumers of those are interested in those events may not be online or available at the same time. So you need a way to store these messages for a configurable period of time until they are consumed and acted upon. Event backbone should be able to provide such event persistence. Observability At times, you need an overall view of how events are ingested by source systems and getting processed by consumers. It could be a management console where events can be observed. Event backbone should provide such observability. Fault tolerance Event backbone could be made of several components. If one of them becomes unavailable, there should not be any impact on the event processors dependent on the backbone. Event backbone needs to provide this resiliency. High availability Event backbone provides persistence of messages/events. If one of the components of the backbone becomes unavailable, there should not be any impact on the availability of these messages/events. Event backbone should be highly available. Performance Event backbone should provide means of accelerating the event processing operations (e.g. parallelising event processing) thereby providing enhanced performance. Delivery guarantees Event backbone should support guaranteed delivery both for producer and consumer. It should support the delivery guarantee options of at least once , at most once , and exactly once . Security The data residing in the event backbone should be secured, at rest as well as in transit. Only authenticated and authorized users should be able to publish and consume messages from the backbone. Topic specific authorizations will also help blocking access by unauthorized consumers. Event backbone should provide these security measures. Stateful operations for events streams Sometimes, source systems generate a continuous flow of 'inter-related' events (e.g. IoT sensors sending data every second). In order to process such messages correctly, the event backbone needs to support for stateful operations like windowing, joins, aggregations. and any type of real time analytics. Event routing options In EDA, event consumers may not be online at all times. So, it should be easier for consumers to subscribe to a topic when it comes online. On-failure hooks Event backbone can support pre-configured actions/behaviors for certain messages. E.g. if a consumer fails to process a message more than a certain number of times, that message can be sent to another topic for re-trying the processing action. Looking across these capabilities, the potential technologies, the amount of adoption and community activity around the technologies, and the considerations listed above, we selected Apache Kafka as our event backbone for the event-driven reference architecture. Event sources When you consider an event-driven architecture, think about event producers and event consumers as the interaction points with events. As you develop event-driven applications following a microservices architecture, the microservices you develop play the role of both event producers and event consumers, with the events being passed as the communication payload between them. However, as you look at the wider opportunities that being event driven offers, you need to widen your view and consider event sources that come from beyond the application code you are writing. These are events that may be produced from outside our immediate system but have business relevance or enable us to gain valuable insights into things that are affecting your business. Here is a list of common event sources: IoT devices or sensors showing device status changes Click Stream data from web or mobile applications Mobile applications (HTTP to Back-end for Front-end service and then to topic) Geospatial data Weather alerts Social media feeds Real-time voice feeds Other messaging backbone Data change event streams from databases (change data capture) IoT devices and sensors With IoT devices and sensors you typically have a gateway providing the connectivity for the device, and a level of event enrichment and filtering. In terms of domain driven design you would see the device and gateway as being the technical domain and the event-driven reference architecture as providing the infrastructure for the applications in a business domain. In practice, the IoT gateway or platform provides the connectivity and is the point of filtering and consolidation of events so that only business-relevant events are passed up to the business domain. The gateway can also be the point where the technical event is enhanced to relate to something recognizable at the business level. One example of this is to relate a device number or identifier in the event to something that the business recognizes. Clickstream data Clickstream data is often used to understand the behavior of users as they navigate their way through web or mobile apps. It provides a recording of the actions they take, such as the clicks, the mouse-movements, and the gestures. Analysis of the clickstream data can lead to a deep understanding of how users actually interact with the application. It enables you to detect where users struggle and to look for ways to improve the experience. Processing the clickstream in real time in an event-driven architecture can also give rise to the opportunity to take direct action in response to what a user is currently doing, or more accurately has just done. There are various \"collectors\" that enable collection of standard clickstream events and allow custom actions to be collected as events typically through tags in Javascript. Within the Apache Open Source communities the Divolte collector is an example of one of these collectors that directly publishes events to Kafka topics. Event standards and schemas Where you have control as the producer of an event we should consider having an event schema and following a standard to provide the best opportunity for portability of the solutions across cloud environments. With a lack of formal standards, a working group under the Cloud Native Computing Foundation (CNCF) has recently been formed to define and propose Cloud Events as the standard. Microservices The event-driven reference architecture provides support for event-driven microservices - microservices which are connected through and communicate via the pub/sub communication protocol within an event backbone. With Kafka as the event backbone and pub/sub messaging provider, microservices can use the Kafka API's to publish and listen for events. On cloud-native platforms, microservices are the application architecture of choice. As businesses become event-driven, event driven pattern needs to extend into our microservices application space. This means that your microservices are still doing REST calls to well-known microservice but they must respond to and send out events, or in event-driven terms they need to be both event producers and consumers to enforce strong decoupling. With the adoption of microservices, the focus on synchronous communication between services has increased. Service mesh packages such as Istio help with the management of communication, service discovery, load balancing, and visibility in this synchronous communication environment. With event-driven microservices, the communication point becomes the Pub/Sub layer of the event backbone. By adopting an event-based approach for intercommunication between microservices, the microservices applications are naturally responsive (event-driven). This approach enhances the loose coupling nature of microservices because it decouples producers and consumers. Further, it enables the sharing of data across microservices through the event log. These event style characteristics are increasingly important considerations when you develop microservices style applications. In practical terms microservices applications are a combination of synchronous API-driven, and asynchronous event-driven communication styles. For the implementation point of view a set of established patterns are used, such as Database per Service, Event Sourcing, Command Query Responsibility Segregation, Saga, ... Event-driven apps with containers While the serverless approach with Cloud Functions provides a simplified event-based programming model, the majority of microservices applications today are developed for and deployed to a container-based cloud-native stack. Within the cloud-native landscape, Kubernetes is the standard platform for container orchestration, and therefore becomes the base for the container platform in the event-driven architecture. As before, the event backbone is the Pub/Sub communication provider and event log for shared data for the microservices. In this context microservices are developed as direct consumers and producers of events on the backbone via topics. The extra work in this environment is in managing consumer instances to respond to the demand of the event stream. You must determine how many consumer instances need to be running to keep pace with, or always be immediately available to execute, the microservice in response to an arriving event. Commands A command, is an instruction to do something. Typically, commands are directed to a particular consumer. The consumer runs the required command or process, and passes back a confirmation to the issuer stating that the command has been processed. The Command concept plays a key role in the Command-Query Responsibility Segregation pattern that is more commonly known as CQRS . Loose coupling Loose coupling is one of the main benefits of event-driven processing. It allows event producers to emit events without any knowledge about who is going to consume those events. Likewise, event consumers don't need to be aware of the event emitters. Because of this, event consuming modules and event producer modules can be implemented in different languages or use technologies that are different and appropriate for specific jobs. Loosely coupled modules are better suited to evolve independently and, when implemented correctly, result in a significant decrease in system complexity. Loose coupling, however, does not mean \u201cno coupling\u201d. An event consumer consumes events that are useful in achieving its goals and in doing so establishes what data it needs and the type and format of that data. The event producer emits events that it hopes are understood and useful to consumers thus establishing an implicit contract with potential consumers. For example, an event notification in XML format must conform to a certain schema that must be known by both the consumer and the producer. One of the most important things that you can do to reduce coupling in an event-driven system is to reduce the number of distinct event types that flow between modules. To do this you must pay attention to the cohesiveness of those modules. Cohesion Cohesion is the degree to which related things are encapsulated together in the same software module. For the purposes of this EDA discussion, a module is defined as an independently deployable software unit that has high cohesion. Cohesion is strongly related to coupling in the sense that a highly cohesive module communicates less with other modules, thus reducing the number of events most importantly, the number of event types in the system. The less frequently modules interact with each other, the less coupled they are. Achieving cohesion in software while optimizing module size for flexibility and adaptability is difficult, but something to strive for. Designing for cohesion starts with a holistic understanding of the problem domain and good analysis work. Sometimes it must also take into account the constraints of the supporting software environment. Monolithic implementations and implementations that are excessively fine-grained must be avoided.","title":"EDA Concepts"},{"location":"concepts/terms-and-definitions/#terms-definitions","text":"","title":"Terms &amp; Definitions"},{"location":"concepts/terms-and-definitions/#events","text":"Events are notifications of change of state. Notifications are issued, or published and interested parties can subscribe and take action on the events. Typically, the issuer of the notification has no knowledge of what action is taken and receives no corresponding feedback that the notification has been processed. Events are notifications of change of state. Typically, events represent the change of state of something of interest to the business. Events are records of something that has happened. Events can't be changed, that is, they are immutable. (We can't change something that has happened in the past).","title":"Events"},{"location":"concepts/terms-and-definitions/#event-streams","text":"An event stream is a continuous unbounded series of events. The start of the stream may have occurred before we started to process the stream. The end of the stream is at some unknown point in the future. Events are ordered by the point in time at which each event occurred. When developing event driven solutions, you will typically see two types of event streams: Event streams whose events are defined and published into a stream as part of a solution. Event streams that connect to a real-time event stream, for example from an IOT device, a voice stream from a telephone system, a video stream, or ship or plane locations from global positioning systems.","title":"Event streams"},{"location":"concepts/terms-and-definitions/#event-backbone","text":"The event backbone is the communication layer in the event driven architecture. It provides the connection between event driven capabilities and in the Cloud Native , it becomes the Pub/Sub communication layer for event driven microservices. At this high level we would consider two types of relevant technologies for the event backbone, Message Brokers and Event Logs . Both technology types could be used to achieve the event communication style, with the \"Publish and subscribe\" model however, it is also important to consider other capabilities which are frequently used within event driven solutions: Keeping an Event Log as a time sequenced as it happened recording of events (Source of the truth). Enabling direct replay of events. Enabling Event Sourcing as a way of recording state changes in distributed systems. Enabling programmatic access to the continuous event stream . When viewed across these wider event driven capabilities, an event log style technology can provide a central component which can support all of these capabilities, whereas a message broker would have to be extended with other components.","title":"Event backbone"},{"location":"concepts/terms-and-definitions/#selecting-the-event-backbone-for-the-reference-architecture","text":"For the event-driven architecture, we defined the following characteristics to be essential for the event backbone: Publish-subscribe event communication between event producers and consumers Facilitate many consumers with shared central \u201csource of truth\u201d. Capability to store events for a given period of time (event log). This is the shared source of the truth for events. Ability for consumers to subscribe to events. Provide replay of events from history for evolving application instances. Provide programmatic access to continuous stream of events, with minimum time lag. Must be highly scalable and resilient to cloud deployment levels. While choosing an event backbone for your event-driven application development, the following functional and non-functional requirements should be taken into consideration:","title":"Selecting the Event Backbone for the reference architecture"},{"location":"concepts/terms-and-definitions/#persistence","text":"When source systems generate events, the consumers of those are interested in those events may not be online or available at the same time. So you need a way to store these messages for a configurable period of time until they are consumed and acted upon. Event backbone should be able to provide such event persistence.","title":"Persistence"},{"location":"concepts/terms-and-definitions/#observability","text":"At times, you need an overall view of how events are ingested by source systems and getting processed by consumers. It could be a management console where events can be observed. Event backbone should provide such observability.","title":"Observability"},{"location":"concepts/terms-and-definitions/#fault-tolerance","text":"Event backbone could be made of several components. If one of them becomes unavailable, there should not be any impact on the event processors dependent on the backbone. Event backbone needs to provide this resiliency.","title":"Fault tolerance"},{"location":"concepts/terms-and-definitions/#high-availability","text":"Event backbone provides persistence of messages/events. If one of the components of the backbone becomes unavailable, there should not be any impact on the availability of these messages/events. Event backbone should be highly available.","title":"High availability"},{"location":"concepts/terms-and-definitions/#performance","text":"Event backbone should provide means of accelerating the event processing operations (e.g. parallelising event processing) thereby providing enhanced performance.","title":"Performance"},{"location":"concepts/terms-and-definitions/#delivery-guarantees","text":"Event backbone should support guaranteed delivery both for producer and consumer. It should support the delivery guarantee options of at least once , at most once , and exactly once .","title":"Delivery guarantees"},{"location":"concepts/terms-and-definitions/#security","text":"The data residing in the event backbone should be secured, at rest as well as in transit. Only authenticated and authorized users should be able to publish and consume messages from the backbone. Topic specific authorizations will also help blocking access by unauthorized consumers. Event backbone should provide these security measures.","title":"Security"},{"location":"concepts/terms-and-definitions/#stateful-operations-for-events-streams","text":"Sometimes, source systems generate a continuous flow of 'inter-related' events (e.g. IoT sensors sending data every second). In order to process such messages correctly, the event backbone needs to support for stateful operations like windowing, joins, aggregations. and any type of real time analytics.","title":"Stateful operations for events streams"},{"location":"concepts/terms-and-definitions/#event-routing-options","text":"In EDA, event consumers may not be online at all times. So, it should be easier for consumers to subscribe to a topic when it comes online.","title":"Event routing options"},{"location":"concepts/terms-and-definitions/#on-failure-hooks","text":"Event backbone can support pre-configured actions/behaviors for certain messages. E.g. if a consumer fails to process a message more than a certain number of times, that message can be sent to another topic for re-trying the processing action. Looking across these capabilities, the potential technologies, the amount of adoption and community activity around the technologies, and the considerations listed above, we selected Apache Kafka as our event backbone for the event-driven reference architecture.","title":"On-failure hooks"},{"location":"concepts/terms-and-definitions/#event-sources","text":"When you consider an event-driven architecture, think about event producers and event consumers as the interaction points with events. As you develop event-driven applications following a microservices architecture, the microservices you develop play the role of both event producers and event consumers, with the events being passed as the communication payload between them. However, as you look at the wider opportunities that being event driven offers, you need to widen your view and consider event sources that come from beyond the application code you are writing. These are events that may be produced from outside our immediate system but have business relevance or enable us to gain valuable insights into things that are affecting your business.","title":"Event sources"},{"location":"concepts/terms-and-definitions/#here-is-a-list-of-common-event-sources","text":"IoT devices or sensors showing device status changes Click Stream data from web or mobile applications Mobile applications (HTTP to Back-end for Front-end service and then to topic) Geospatial data Weather alerts Social media feeds Real-time voice feeds Other messaging backbone Data change event streams from databases (change data capture)","title":"Here is a list of common event sources:"},{"location":"concepts/terms-and-definitions/#iot-devices-and-sensors","text":"With IoT devices and sensors you typically have a gateway providing the connectivity for the device, and a level of event enrichment and filtering. In terms of domain driven design you would see the device and gateway as being the technical domain and the event-driven reference architecture as providing the infrastructure for the applications in a business domain. In practice, the IoT gateway or platform provides the connectivity and is the point of filtering and consolidation of events so that only business-relevant events are passed up to the business domain. The gateway can also be the point where the technical event is enhanced to relate to something recognizable at the business level. One example of this is to relate a device number or identifier in the event to something that the business recognizes.","title":"IoT devices and sensors"},{"location":"concepts/terms-and-definitions/#clickstream-data","text":"Clickstream data is often used to understand the behavior of users as they navigate their way through web or mobile apps. It provides a recording of the actions they take, such as the clicks, the mouse-movements, and the gestures. Analysis of the clickstream data can lead to a deep understanding of how users actually interact with the application. It enables you to detect where users struggle and to look for ways to improve the experience. Processing the clickstream in real time in an event-driven architecture can also give rise to the opportunity to take direct action in response to what a user is currently doing, or more accurately has just done. There are various \"collectors\" that enable collection of standard clickstream events and allow custom actions to be collected as events typically through tags in Javascript. Within the Apache Open Source communities the Divolte collector is an example of one of these collectors that directly publishes events to Kafka topics.","title":"Clickstream data"},{"location":"concepts/terms-and-definitions/#event-standards-and-schemas","text":"Where you have control as the producer of an event we should consider having an event schema and following a standard to provide the best opportunity for portability of the solutions across cloud environments. With a lack of formal standards, a working group under the Cloud Native Computing Foundation (CNCF) has recently been formed to define and propose Cloud Events as the standard.","title":"Event standards and schemas"},{"location":"concepts/terms-and-definitions/#microservices","text":"The event-driven reference architecture provides support for event-driven microservices - microservices which are connected through and communicate via the pub/sub communication protocol within an event backbone. With Kafka as the event backbone and pub/sub messaging provider, microservices can use the Kafka API's to publish and listen for events. On cloud-native platforms, microservices are the application architecture of choice. As businesses become event-driven, event driven pattern needs to extend into our microservices application space. This means that your microservices are still doing REST calls to well-known microservice but they must respond to and send out events, or in event-driven terms they need to be both event producers and consumers to enforce strong decoupling. With the adoption of microservices, the focus on synchronous communication between services has increased. Service mesh packages such as Istio help with the management of communication, service discovery, load balancing, and visibility in this synchronous communication environment. With event-driven microservices, the communication point becomes the Pub/Sub layer of the event backbone. By adopting an event-based approach for intercommunication between microservices, the microservices applications are naturally responsive (event-driven). This approach enhances the loose coupling nature of microservices because it decouples producers and consumers. Further, it enables the sharing of data across microservices through the event log. These event style characteristics are increasingly important considerations when you develop microservices style applications. In practical terms microservices applications are a combination of synchronous API-driven, and asynchronous event-driven communication styles. For the implementation point of view a set of established patterns are used, such as Database per Service, Event Sourcing, Command Query Responsibility Segregation, Saga, ...","title":"Microservices"},{"location":"concepts/terms-and-definitions/#event-driven-apps-with-containers","text":"While the serverless approach with Cloud Functions provides a simplified event-based programming model, the majority of microservices applications today are developed for and deployed to a container-based cloud-native stack. Within the cloud-native landscape, Kubernetes is the standard platform for container orchestration, and therefore becomes the base for the container platform in the event-driven architecture. As before, the event backbone is the Pub/Sub communication provider and event log for shared data for the microservices. In this context microservices are developed as direct consumers and producers of events on the backbone via topics. The extra work in this environment is in managing consumer instances to respond to the demand of the event stream. You must determine how many consumer instances need to be running to keep pace with, or always be immediately available to execute, the microservice in response to an arriving event.","title":"Event-driven apps with containers"},{"location":"concepts/terms-and-definitions/#commands","text":"A command, is an instruction to do something. Typically, commands are directed to a particular consumer. The consumer runs the required command or process, and passes back a confirmation to the issuer stating that the command has been processed. The Command concept plays a key role in the Command-Query Responsibility Segregation pattern that is more commonly known as CQRS .","title":"Commands"},{"location":"concepts/terms-and-definitions/#loose-coupling","text":"Loose coupling is one of the main benefits of event-driven processing. It allows event producers to emit events without any knowledge about who is going to consume those events. Likewise, event consumers don't need to be aware of the event emitters. Because of this, event consuming modules and event producer modules can be implemented in different languages or use technologies that are different and appropriate for specific jobs. Loosely coupled modules are better suited to evolve independently and, when implemented correctly, result in a significant decrease in system complexity. Loose coupling, however, does not mean \u201cno coupling\u201d. An event consumer consumes events that are useful in achieving its goals and in doing so establishes what data it needs and the type and format of that data. The event producer emits events that it hopes are understood and useful to consumers thus establishing an implicit contract with potential consumers. For example, an event notification in XML format must conform to a certain schema that must be known by both the consumer and the producer. One of the most important things that you can do to reduce coupling in an event-driven system is to reduce the number of distinct event types that flow between modules. To do this you must pay attention to the cohesiveness of those modules.","title":"Loose coupling"},{"location":"concepts/terms-and-definitions/#cohesion","text":"Cohesion is the degree to which related things are encapsulated together in the same software module. For the purposes of this EDA discussion, a module is defined as an independently deployable software unit that has high cohesion. Cohesion is strongly related to coupling in the sense that a highly cohesive module communicates less with other modules, thus reducing the number of events most importantly, the number of event types in the system. The less frequently modules interact with each other, the less coupled they are. Achieving cohesion in software while optimizing module size for flexibility and adaptability is difficult, but something to strive for. Designing for cohesion starts with a holistic understanding of the problem domain and good analysis work. Sometimes it must also take into account the constraints of the supporting software environment. Monolithic implementations and implementations that are excessively fine-grained must be avoided.","title":"Cohesion"},{"location":"patterns/api_mgt/","text":"Modernization from API lifecycle This note summarizes some of the best practices for introducing API management in development practices and architecture patterns within an enterprise setting. Move from a pure API gateway to an API Management system An API Gateway helps provide security, control, integration, and optimized access to a full range of mobile, web, application programming interface (API), service-oriented architecture (SOA), B2B and cloud workloads. Gateways are used in the following patterns: As a Security Gateway , placed between the consumer facing firewall and the system of records facing firewall (DMZ). It is used for both policy enforcement and consistent security policies across business channels. As an API Gateway , both as an internal and external gateway, with centralized service governance and policy enforcement, and with traffic monitoring. To provide connectivity (HTTP) and mediation (XML, JSON) services in the internal network, close to the system of record. An API Management system supports a broader scope of features for API lifecycle management, including: API lifecycle management to activate, retire, or stage an API product. API governance with security, access, and versioning. Analytics, dashboards, third party data offload for usage analysis. API socialization based on a portal that allows self-service for the developer community. An API developer toolkit to facilitate the creation and testing of APIs. Classical pain points Some of the familiar pain points that indicate it is time to consider adopting a broader API management product include: Current API details like endpoints, request/response message format, error conditions, test messages, SLAs are not easily available. Difficult to tell which subscribers are really using the API and how often, without building a custom solution. Difficult to differentiate between business-critical subscribers versus low value subscribers. Managing different lines of business and organizations is complex. No dynamic scaling built into the solution, which often means making hardware investments for max load or worst availability scenarios. Difficult to move from SOAP based web services to RESTful services to GraphQL No support for AsynchAPI to automate and formalize the documentation or code generation of your event-driven APIs. Ensure consistent security rules Integrating CI/CD pipelines with API life cycle Enterprise APIs across boundaries If you consider a typical API management product, it includes a set of components as presented in the figure below that could be deployed on-premise servers, on Kubernetes platform or on several Cloud provider regions. API served by applications or microservices can be deployed in multiple regions but still be managed by one central API management server. An API developer signs on to the API management cloud services account and accesses the API developer User interface or CLI toolkit. The developer creates the synch API and implements business logic. He maps and integrates the API data model to the back-end schema through the transformation and connectivity service. He tests and deploys the API to the runtime and publishes to API management. He can also create Asynch APIs from a messaging system by binding channels to topic or queue and define message payload definition. API owner signs on to the API management cloud services account and accesses the API management component. She includes the synch API endpoint to existing API products, and plans and specifies access control. She publishes the API to the developer portal for external discovery by application developers. An application developer accesses the developer portal, uses search, and discovers the API. The application developer uses the API in an application and deploys that application to the device. The device user opens the application that issues the API request. The request is handled by the API gateway, which performs load balancing and security validation for all API requests. The API gateway validates access policies with API management and invokes the API. The API polyglot runtime executes the API and obtains the data payload from the back end. The API response is sent back to the API gateway. Alternatively, APIs exposed by enterprise applications can be executed on that enterprise application runtime. The API gateway forwards the response to the calling application. The API gateway reports usage metrics and analytics to the API management. API developers and API owners can log on to the API analytics visualization component to view dashboards on API usage metrics and other analytics. So a across cloud providers deployment will look like in the diagram below: On the left side (green boxes), the consumers of the API register to a Developer portal to get all the metadata about the API they want to consume. They register their application as API subscriber. Their application can run on the cloud or on-premise. The API Gateway services are colocated with target rail services to reduce latency. It is deployed as StatefulSet on OpenShift cluster. It exposes the Booking APIs and ensure security policies, traffic monitoring,... The Developer Portals can be separated, or centralized depending on API characteristics exposed from different clouds (for example different Developer Portals for internal and external APIs). Here it is deployed on the Cloud Provider as a container inside OpenShift cluster. The Analytic service is also a StatefulSet and get metrics from the gateway. The figure above also illustrates the rail services are accessing remote consolidators and this traffic can also goes to the API gateway. Those services will also integrate with existing backend services running on-premise, deployed or not on OpenShift. The management service for the API management product, is here running on-premise to illustrate that it is a central deployment to manage multiple gateways. Gateway services, Developer Portal services, and Analytics services are scoped to single region unlike Management system, which can communicate across availability zones. Kubernetes deployment The different API management services run in OpenShift and can ensure the high availability of each of the components. Integration with messaging definition Asynch API AsyncAPI is an open source initiative that focuses on making Event-Driven Architectures (EDAs) as easy to work with as REST APIs. The AsyncAPI specification (currently at 2.0.0) establishes standards for your events, covering everything from documentation to code generation, and discovery to event management. The goal is to enable better governance of your asynchronous APIs. Schema registry Developer experiences and practices Swagger The OpenAPI document can be created top-down with a Swagger UI or bottom up using Annotation in the Java JAXRS resource classes. Either ways the API can be uploaded to the API management product. The important parts are to define the operations exposed and the request / response structure of the data model. Testing Further Readings Agile integration IBM red book","title":"API management"},{"location":"patterns/api_mgt/#modernization-from-api-lifecycle","text":"This note summarizes some of the best practices for introducing API management in development practices and architecture patterns within an enterprise setting.","title":"Modernization from API lifecycle"},{"location":"patterns/api_mgt/#move-from-a-pure-api-gateway-to-an-api-management-system","text":"An API Gateway helps provide security, control, integration, and optimized access to a full range of mobile, web, application programming interface (API), service-oriented architecture (SOA), B2B and cloud workloads. Gateways are used in the following patterns: As a Security Gateway , placed between the consumer facing firewall and the system of records facing firewall (DMZ). It is used for both policy enforcement and consistent security policies across business channels. As an API Gateway , both as an internal and external gateway, with centralized service governance and policy enforcement, and with traffic monitoring. To provide connectivity (HTTP) and mediation (XML, JSON) services in the internal network, close to the system of record. An API Management system supports a broader scope of features for API lifecycle management, including: API lifecycle management to activate, retire, or stage an API product. API governance with security, access, and versioning. Analytics, dashboards, third party data offload for usage analysis. API socialization based on a portal that allows self-service for the developer community. An API developer toolkit to facilitate the creation and testing of APIs.","title":"Move from a pure API gateway to an API Management system"},{"location":"patterns/api_mgt/#classical-pain-points","text":"Some of the familiar pain points that indicate it is time to consider adopting a broader API management product include: Current API details like endpoints, request/response message format, error conditions, test messages, SLAs are not easily available. Difficult to tell which subscribers are really using the API and how often, without building a custom solution. Difficult to differentiate between business-critical subscribers versus low value subscribers. Managing different lines of business and organizations is complex. No dynamic scaling built into the solution, which often means making hardware investments for max load or worst availability scenarios. Difficult to move from SOAP based web services to RESTful services to GraphQL No support for AsynchAPI to automate and formalize the documentation or code generation of your event-driven APIs. Ensure consistent security rules Integrating CI/CD pipelines with API life cycle","title":"Classical pain points"},{"location":"patterns/api_mgt/#enterprise-apis-across-boundaries","text":"If you consider a typical API management product, it includes a set of components as presented in the figure below that could be deployed on-premise servers, on Kubernetes platform or on several Cloud provider regions. API served by applications or microservices can be deployed in multiple regions but still be managed by one central API management server. An API developer signs on to the API management cloud services account and accesses the API developer User interface or CLI toolkit. The developer creates the synch API and implements business logic. He maps and integrates the API data model to the back-end schema through the transformation and connectivity service. He tests and deploys the API to the runtime and publishes to API management. He can also create Asynch APIs from a messaging system by binding channels to topic or queue and define message payload definition. API owner signs on to the API management cloud services account and accesses the API management component. She includes the synch API endpoint to existing API products, and plans and specifies access control. She publishes the API to the developer portal for external discovery by application developers. An application developer accesses the developer portal, uses search, and discovers the API. The application developer uses the API in an application and deploys that application to the device. The device user opens the application that issues the API request. The request is handled by the API gateway, which performs load balancing and security validation for all API requests. The API gateway validates access policies with API management and invokes the API. The API polyglot runtime executes the API and obtains the data payload from the back end. The API response is sent back to the API gateway. Alternatively, APIs exposed by enterprise applications can be executed on that enterprise application runtime. The API gateway forwards the response to the calling application. The API gateway reports usage metrics and analytics to the API management. API developers and API owners can log on to the API analytics visualization component to view dashboards on API usage metrics and other analytics. So a across cloud providers deployment will look like in the diagram below: On the left side (green boxes), the consumers of the API register to a Developer portal to get all the metadata about the API they want to consume. They register their application as API subscriber. Their application can run on the cloud or on-premise. The API Gateway services are colocated with target rail services to reduce latency. It is deployed as StatefulSet on OpenShift cluster. It exposes the Booking APIs and ensure security policies, traffic monitoring,... The Developer Portals can be separated, or centralized depending on API characteristics exposed from different clouds (for example different Developer Portals for internal and external APIs). Here it is deployed on the Cloud Provider as a container inside OpenShift cluster. The Analytic service is also a StatefulSet and get metrics from the gateway. The figure above also illustrates the rail services are accessing remote consolidators and this traffic can also goes to the API gateway. Those services will also integrate with existing backend services running on-premise, deployed or not on OpenShift. The management service for the API management product, is here running on-premise to illustrate that it is a central deployment to manage multiple gateways. Gateway services, Developer Portal services, and Analytics services are scoped to single region unlike Management system, which can communicate across availability zones.","title":"Enterprise APIs across boundaries"},{"location":"patterns/api_mgt/#kubernetes-deployment","text":"The different API management services run in OpenShift and can ensure the high availability of each of the components.","title":"Kubernetes deployment"},{"location":"patterns/api_mgt/#integration-with-messaging-definition","text":"","title":"Integration with messaging definition"},{"location":"patterns/api_mgt/#asynch-api","text":"AsyncAPI is an open source initiative that focuses on making Event-Driven Architectures (EDAs) as easy to work with as REST APIs. The AsyncAPI specification (currently at 2.0.0) establishes standards for your events, covering everything from documentation to code generation, and discovery to event management. The goal is to enable better governance of your asynchronous APIs.","title":"Asynch API"},{"location":"patterns/api_mgt/#schema-registry","text":"","title":"Schema registry"},{"location":"patterns/api_mgt/#developer-experiences-and-practices","text":"","title":"Developer experiences and practices"},{"location":"patterns/api_mgt/#swagger","text":"The OpenAPI document can be created top-down with a Swagger UI or bottom up using Annotation in the Java JAXRS resource classes. Either ways the API can be uploaded to the API management product. The important parts are to define the operations exposed and the request / response structure of the data model.","title":"Swagger"},{"location":"patterns/api_mgt/#testing","text":"","title":"Testing"},{"location":"patterns/api_mgt/#further-readings","text":"Agile integration IBM red book","title":"Further Readings"},{"location":"patterns/cqrs/","text":"Command-Query Responsibility Segregation (CQRS) Problems and Constraints A domain model encapsulates domain data with the behavior for maintaining the correctness of that data as it is modified, structuring the data based on how it is stored in the database and to facilitate managing the data. Multiple clients can independently update the data concurrently. Different clients may not use the data the way the domain model structures it, and may not agree with each other on how it should be structured. When a domain model becomes overburdened managing complex aggregate objects, concurrent updates, and numerous cross-cutting views, how can it be refactored to separate different aspects of how the data is used? An application accesses data both to read and to modify it. The primitive data tasks are often expressed as create, read, update, and delete (CRUD); using them is known as CRUDing the data. Application code often does not make much distinction between the tasks; individual operations may mix reading the data with changing the data as needed. This simple approach works well when all clients of the data can use the same structure and contention is low. A single domain model can manage the data, make it accessible as domain objects, and ensure updates maintain its consistency. However, this approach becomes inadequate when different clients want different views across multiple sets of data, when the data is too widely used, and/or when multiple clients updating the data my unknowingly conflict with each other. For example, in a microservices architecture, each microservice should store and manage its own data, but a user interface may need to display data from several microservices. A query that gathers bits of data from multiple sources can be inefficient (time and bandwidth consumed accessing multiple data sources, CPU consumed transforming data, memory consumed by intermediate objects) and must be repeated each time the data is accessed. Another example is an enterprise database of record managing data required by multiple applications. It can become overloaded with too many clients needing too many connections to run too many threads performing too many transactions--such that the database becomes a performance bottleneck and can even crash. Another example is maintaining consistency of the data while clients concurrently make independent updates to the data. While each update may be consistent, they may conflict with each other. Database locking ensures that the updates don't change the same data concurrently, but doesn't ensure multiple independent changes result in a consistent data model. When data usage is more complex than a single domain model can facilitate, a more sophisticated approach is needed. Solution and Pattern Refactor a domain model to separate operations for querying data and operations for updating data so that they may be handled independently. The CQRS pattern strictly segregates operations that read data from operations that update data. An operation can read data (the R in CRUD) or can write data (the CUD in CRUD), but not both. This separation can make using data much more manageable in several respects. The read operations and the write operations are simpler to implement because their functionality is more finely focused. The operations can be developed independently, potentially by separate teams. The operations can be optimized independently and can evolve independently, following changing user requirements more easily. These optimized operations can scale better, perform better, and security can be applied more precisely. The full CQRS pattern uses separate read and write databases. In doing so, the pattern segregates not just the APIs for accessing data or the models for managing data, but even segregates the database itself into two, a read/write database that is effectively write-only and one or more read-only databases. The adoption of the pattern can be applied in phases, incrementally from existing code. To illustrate this, we will use four stages that could be used incrementally or a developer can go directly from stage 0 to 3 without considering the others: Stage 0: Typical application data access Stage 1: Separate read and write APIs Stage 2: Separate read and write models Stage 3: Separate read and write databases Typical application data access Before even beginning to apply the pattern, let\u2019s consider the typical app design for accessing data. This diagram shows an app with a domain model for accessing data persisted in a database of record, i.e. a single source of truth for that data. The domain model has an API that at a minimum enables clients to perform CRUD tasks on domain objects within the model. The domain model is an object representation of the database documents or records. It is comprised of domain objects that represent individual documents or records and the business logic for managing and using them. Domain-Driven Design (DDD) models these domain objects as entities \u2014\u201cobjects that have a distinct identity that runs through time and different representations\u201d\u2014and aggregates \u2014\u201ca cluster of domain objects that can be treated as a single unit\u201d; the aggregate root maintains the integrity of the aggregate as a whole. Ideally, the domain model\u2019s API should be more domain-specific than simply CRUDing of data. Instead, it should expose higher-level operations that represent business functionality like findCustomer() , placeOrder() , transferFunds() , and so on. These operations read and update data as needed, sometimes doing both in a single operation. They are correct as long as they fit the way the business works. Separate read and write APIs The first and most visible step in applying the CQRS pattern is splitting the CRUD API into separate read and write APIs. This diagram shows the same domain model as before, but its single CRUD API is split into retrieve and modify APIs. The two APIs share the existing domain model but split the behavior: Read : The retrieve API is used to read the existing state of the objects in the domain model without changing that state. The API treats the domain state as read only. Write : The modify API is used to change the objects in the domain model. Changes are made using CUD tasks: create new domain objects, update the state in existing ones, and delete ones that are no longer needed. The operations in this API do not return result values, they return success (ack or void) or failure (nak or throw an exception). The create operation might return the primary of key of the entity, which can be generated either by the domain model or in the data source. This separation of APIs is an application of the Command Query Separation (CQS) pattern, which says to clearly separate methods that change state from those that don\u2019t. To do so, each of an object\u2019s methods can be in one of two categories (but not both): Query : Returns a result. Does not change the system\u2019s state nor cause any side effects that change the state. Command (a.k.a. modifiers or mutators): Changes the state of a system. Does not return a value, just an indication of success or failure. With this approach, the domain model works the same and provides access to the data the same as before. What has changed is the API for using the domain model. Whereas a higher-level operation might previously have both changed the state of the application and returned a part of that state, now each such operation is redesigned to only do one or the other. When the domain model splits its API into read and write operations, clients using the API must likewise split their functionality into querying and updating functionality. Most new web based applications are based in the single page application, with components and services that use and encapsulate remote API. So this separation of backend API fits well with modern web applications. This stage depends on the domain model being able to implement both the retrieve and modify APIs. A single domain model requires the retrieve and modify behavior to have similar, corresponding implementations. For them to evolve independently, the two APIs will need to be implemented with separate read and write models. Separate read and write models The second step in applying the CQRS pattern is to split the domain model into separate read and write models. This doesn\u2019t just change the API for accessing domain functionality, it also changes the design of how that functionality is structured and implemented. This diagram shows that the domain model becomes the basis for a write model that handles changes to the domain objects, along with a separate read model used to access the state of the app. Naturally, the read model implements the retrieve API and the write model implements the modify API. Now, the application consists not only of separate APIs for querying and updating the domain objects, there\u2019s also separate business functionality for doing so. Both the read business functionality and the write business functionality share the same database. The write model is implemented by specializing the domain model to focus solely on maintaining the valid structure of domain objects when changing their state and by applying any business rules. Meanwhile, responsibility for returning domain objects is shifted to a separate read model. The read model defines data transfer objects (DTOs) designed specifically for the model to return just the data the client wants in a structure the client finds convenient. The read model knows how to gather the data used to populate the DTOs. DTOs encapsulate little if any domain functionality, they just bundle data into a convenient package that can easily be transmitted using a single method call, especially between processes. The read model should be able to implement the retrieve API by implementing the necessary queries and executing them. If the retrieve API is already built to return domain objects as results, the read model can continue to do so, or better yet, implements DTO types that are compatible with the domain objects and returns those. Likewise, the modify API was already implemented using the domain model, so the write model should preserve that. The write model may enhance the implementation to more explicitly implement a command interface or use command objects. This phase assumes that the read and write models can both be implemented using the same database of record the domain model has been using. To the extent this is true, the implementations of reading and writing can evolve independently and be optimized independently. This independence may become increasingly limited since they are both bound to the same database with a single schema or data model. To enable the read and write models to evolve independently, they may each need their own database. Separate read and write databases The third step in applying the CQRS pattern\u2014-which implements the complete CQRS pattern solution\u2014-is splitting the database of record into separate read and write databases. This diagram shows the write model and read model, each supported by its own database. The overall solution consists of two main parts: the write solution that supports updating the data and the read solution that supports querying the data. The two parts are connected by the event bus. The write model has its own read/write database and the read model has its own read-only database. The read/write database still serves as the database of record (the single source of truth for the data) but is mostly used write-only: mostly written to and rarely read. Reading is offloaded onto a separate read database that contains the same data but is used read-only. The query database is effectively a cache of the database of record, with all of the inherit benefits and complexity of the Caching pattern. The query database contains a copy of the data in the database of record, with the copy structured and staged for easier access by the clients using the retrieve API. As a copy, overhead is needed to keep the copy synchronized with changes in the original. Latency in this synchronization process creates eventual consistency, during which the data copy is stale. The separate databases enable the separate read and write models and their respective retrieve and modify APIs to truly evolve independently. Not only can the read model or write model\u2019s implementation change without changing the other, but how each stores its data can be changed independently. This solution offers the following advantages: Scaling : The query load is moved from the write database to the read database. If the database of record is a scalability bottleneck and a lot of the load on it is caused by queries, unloading those query responsibilities can significantly improve the scalability of the combined data access. Performance : The schemas of the two databases can be different, enabling them to be designed and optimized independently for better performance. The write database can be optimized for data consistency and correctness, with capabilities such as stored procedures that fit the write model and assist with data updates. The read database can store the data in units that better fit the read model and are better optimized for querying, with larger rows requiring fewer joins. Notice that the design for this stage is significantly more complex than the design for the previous stage. Separate databases with copies of the same data may make data modeling and using data easier, but they require significant overhead to synchronize the data and keep the copies consistent. CQRS employs a couple of design features that support keeping the databases synchronized: Command Bus for queuing commands (optional): A more subtle and optional design decision is to queue the commands produced by the modify API, shown in the diagram as the command bus. This can significantly increase the throughput of multiple apps updating the database, as well as serialize updates to help avoid--or at least detect--merge conflicts. With the bus, a client making an update does not block synchronously while the change is written to the database. Rather, the request to change the database is captured as a command ( Design Patterns ) and put on a message queue, after which the client can proceed with other work. Asynchronously in the background, the write model processes the commands at the maximum sustainable rate that the database can handle, without the database ever becoming overloaded. If the database becomes temporarily unavailable, the commands queue and will be processed when the database becomes available once more. Event Bus for publishing update events (required): Whenever the write database is updated, a change notification is published as an event on the event bus. Interested parties can subscribe to the event bus to be notified when the database is updated. One such party is an event processor for the query database, which receives update events and processes them by updating the query database accordingly. In this way, every time the write database is updated, a corresponding update is made to the read database to keep it in sync. The connection between the command bus and the event bus is facilitated by an application of the Event Sourcing pattern , which keeps a change log that is suitable for publishing. Event sourcing maintains not only the current state of the data but also the history of how that current state was reached. For each command on the command bus, the write model performs these tasks to process the command: Logs the change Updates the database with the change Creates an update event describing the change and publishes it to the event bus The changes that are logged can be the commands from the command bus or the update events published to the event bus Considerations Keep these decisions in mind while applying this pattern: Client impact : Applying CQRS not only changes how data is stored and accessed, but also changes the APIs that clients use to access data. This means that each client must be redesigned to use the new APIs. Riskiness : A lot of the complexity of the pattern solution involves duplicating the data in two databases and keeping them synchronized. Risk comes from querying data that is stale or downright wrong because of problems with the synchronization. Eventual consistency : Clients querying data must expect that updates will have latency. In a microservices architecture, eventual data consistency is a given and acceptable in many of cases. Command queuing : Using a command bus as part of the write solution to queue the commands is optional but powerful. In addition to the benefits of queuing, the command objects can easily be stored in the change log and easily be converted into notification events. (In the next section, we illustrate a way to use event bus to queue commands as well.) Change log : The log of changes to the database of record can be either the list of commands from the command bus or the list of event notifications published on the event bus. The Event Sourcing pattern assumes it\u2019s a log of events, but that pattern doesn\u2019t include the command bus. An event list may be easier to scan as a history, whereas a command list is easier to replay. Create keys : Strict interpretation of the Command Query Separation (CQS) pattern says that command operations do not have return types. A possible exception is commands that create data: An operation that creates a new record or document typically returns the key for accessing the new data, which is convenient for the client. However, if the create operation is invoked asynchronously by a command on a command bus, the write model will need to perform a callback on the client to return the key. Messaging queues and topics : While messaging is used to implement both the command bus and event bus, the two busses use messaging differently. The command bus guarantees exactly once delivery. The event bus broadcasts each event to all interested event processors. Query database persistence : The database of record is always persistent. The query database is a cache that can be a persistent cache or an in-memory cache. If the cache is in-memory and is lost, it must be rebuilt completely from the database of record. Security : Controls on reading data and updating data can be applied separately using the two parts of the solution. Combining event sourcing and CQRS The CQRS application pattern is frequently associated with event sourcing: when doing event sourcing and domain driven design, we event source the aggregates or root entities. Aggregate creates events that are persisted. On top of the simple create, update and read by ID operations, the business requirements want to perform complex queries that can't be answered by a single aggregate. By just using event sourcing to be able to respond to a query like \"what are the orders of a customer\", then we have to rebuild the history of all orders and filter per customer. It is a lot of computation. This is linked to the problem of having conflicting domain models between query and persistence. As introduced in previous section, creations and updates are done as state notification events (change of state), and are persisted in the event log/store. The following figure, presents two separate microservices, one supporting the write model, and multiple other supporting the queries: The query part is separate processes that consume change log events and build a projection for future queries. The \"write\" part may persist in SQL while the read may use document oriented database with strong indexing and query capabilities. Or use in-memory database, or distributed cache... They do not need to be in the same language. With CQRS and ES the projections are retroactives. New query equals implementing new projection and read the events from the beginning of time or the recent committed state and snapshot. Read and write models are strongly decoupled and can evolve independently. It is important to note that the 'Command' part can still handle simple queries, primary-key based, like get order by id, or queries that do not involve joins. The event backbone, use a pub/sub model, and Kafka is a good candidate as an implementation technology. With this structure, the Read model microservice will most likely consume events from multiple topics to build the data projection based on joining those data streams. A query, to assess if the cold-chain was respected on the fresh food order shipment, will go to the voyage, container metrics, and order to be able to answer this question. This is where CQRS shines. We can note that, we can separate the API definition and management in a API gateway. The shipment order microservice is implementing this pattern. Some implementation items to consider: Consistency (ensure the data constraints are respected for each data transaction): CQRS without event sourcing has the same consistency guarantees as the database used to persist data and events. With Event Sourcing the consistency could be different, one for the write model and one for the read model. On write model, strong consistency is important to ensure the current state of the system is correct, so it leverages transaction, lock and sharding. On read side, we need less consistency, as they mostly work on stale data. Locking data on the read operation is not reasonable. Scalability : Separating read and write as two different microservices allows for high availability. Caching at the read level can be used to increase performance response time, and can be deployed as multiple standalone instances (Pods in Kubernetes). It is also possible to separate the query implementations between different services. Functions as service / serverless are good technology choices to implement complex queries. Availability : The write model sacrifices consistency for availability. This is a fact. The read model is eventually consistent so high availability is possible. In case of failure the system disables the writing of data but still is able to read them as they are served by different databases and services. With CQRS, the write model can evolve over time without impacting the read model, as long as the event model doesn't change. The read model requires additional tables, but they are often simpler and easier to understand. CQRS results in an increased number of objects, with commands, operations, events,... and packaging in deployable components or containers. It adds potentially different type of data sources. It is more complex. Some challenges to always consider: How to support event structure version management? How much data to keep in the event store (history)? How to adopt data duplication which results to eventual data consistency?. The CQRS pattern was introduced by Greg Young , and described in Martin Fowler's work on microservices. As you can see in previous figure, as soon as we see two arrows from the same component, we have to ask ourselves how does it work: the write model has to persist Order in its own database and then sends OrderCreated event to the topic... Should those operations be atomic and controlled with transaction? We detail this in next section. Keeping the write model on Mainframe It is very important to note that the system of records and transaction processing is still easier to run on mainframe to support strong consistency. But with the move to cloud native development, it does not mean we have to move all the system of records to the cloud. Data pipelines can be put in place, but CQRS should help by keeping the write model on the current system of records and without impacting the current MIPS utilization move data in the eventual consistency world of the cloud native, distributed computing world. In the figure above, the write model follows the current transaction processing on the mainframe, change data capture push data to Event backbone for getting real time visibility into the distributed world. The read is the costly operation, dues to the joins to be done to build the projection views needed to expose data depending of the business use cases. This is more true with distributed microservices. All the write operations for the business entities kept in the mainframe's system of records are still done via the transaction. Reference data can be injected in one load job to topic and persisted in the event store so streaming applications can leverage them by joining with transactional data. The consistency challenges As introduced in the previous section, there is a potential problem of data inconsistency: once a command saves changes into the database, the consumers do not see the new or updated data until event notification completes processing. With traditional Java service, using JPA and JMS, the save and send operations can be part of the same XA transaction and both succeed or fail. With event sourcing pattern, the source of trust is the event source, which acts as a version control system, as shown in the diagram below. The steps for synchronizing changes to the data are: The write model creates the event and publishes it The consumer receives the event and extracts its payload The consumer updates its local datasource with the payload data If the consumer fails to process the update, it can persist the event to an error log Each error in the log can be replayed A command line interface replays an event via an admin API, which searches in the topic using this order id to replay the save operation This implementation causes a problem for the createOrder(order): string operation: The Order Service is supposed to return the new order complete with the order id that is a unique key, a key most likely created by the database. If updating the database fails, there is no new order yet and so no database key to use as the order ID. To avoid this problem, if the underlying technology supports assigning the new order's key, the service can generate the order ID and use that as the order's key in the database. It is important to clearly study the Kafka consumer API and the different parameters on how to support the read offset. We are addressing those implementation best practices in our consumer note. CQRS and Change Data Capture There are other ways to support this dual operations level: When using Kafka, Kafka Connect has the capability to subscribe to databases via JDBC, allowing to poll tables for updates and then produce events to Kafka. There is an open-source change data capture solution based on extracting change events from database transaction logs, Debezium that helps to respond to insert, update and delete operations on databases and generate events accordingly. It supports databases like MySQL, Postgres, MongoDB and others. Write the order to the database and in the same transaction write to an event table ( \"outbox pattern\" ). Then use a polling to get the events to send to Kafka from this event table and delete the row in the table once the event is sent. Use the Change Data Capture from the database transaction log and generate events from this log. The IBM Infosphere CDC product helps to implement this pattern. For more detail about this solution see this product tour . The CQRS implementation using CDC will look like in the following diagram: What is important to note is that the event needs to be flexible on the data payload. On the view side, updates to the view part need to be idempotent. Delay in the view There is a delay between the data persistence and the availability of the data in the Read model. For most business applications, it is perfectly acceptable. In web based data access most of the data are at stale. When there is a need for the client, calling the query operation, to know if the data is up-to-date, the service can define a versioning strategy. When the order data was entered in a form within a single page application, the \"create order\" operation should return the order with its unique key freshly created and the Single Page Application will have the last data. Here is an example of such operation: @POST public Response create ( OrderCreate dto ) { Order order = new Order ( UUID . randomUUID (). toString (), dto . getProductID (),...); // ... return Response . ok (). entity ( order ). build () } Schema change What to do when we need to add attribute to event?. So we need to create a versioninig schema for event structure. You need to use flexible schema like json schema, Apache Avro or protocol buffer and may be, add an event adapter (as a function?) to translate between the different event structures. Code reference The following project includes two sub modules, each deployable as a microservice to illustrate the command and query part: https://github.com/ibm-cloud-architecture/refarch-kc-order-ms Further readings https://www.codeproject.com/Articles/555855/Introduction-to-CQRS http://udidahan.com/2009/12/09/clarified-cqrs https://martinfowler.com/bliki/CQRS.html https://microservices.io/patterns/data/cqrs.html https://community.risingstack.com/when-to-use-cqrs https://dzone.com/articles/concepts-of-cqrs https://martinfowler.com/bliki/CommandQuerySeparation.html https://www.martinfowler.com/eaaCatalog/domainModel.html https://dddcommunity.org/learning-ddd/what_is_ddd/ https://martinfowler.com/bliki/EvansClassification.html https://martinfowler.com/bliki/DDD_Aggregate.html https://martinfowler.com/eaaCatalog/dataTransferObject.html https://en.wikipedia.org/wiki/Command_pattern https://www.pearson.com/us/higher-education/program/Gamma-Design-Patterns-Elements-of-Reusable-Object-Oriented-Software/PGM14333.html","title":"CQRS"},{"location":"patterns/cqrs/#command-query-responsibility-segregation-cqrs","text":"","title":"Command-Query Responsibility Segregation (CQRS)"},{"location":"patterns/cqrs/#problems-and-constraints","text":"A domain model encapsulates domain data with the behavior for maintaining the correctness of that data as it is modified, structuring the data based on how it is stored in the database and to facilitate managing the data. Multiple clients can independently update the data concurrently. Different clients may not use the data the way the domain model structures it, and may not agree with each other on how it should be structured. When a domain model becomes overburdened managing complex aggregate objects, concurrent updates, and numerous cross-cutting views, how can it be refactored to separate different aspects of how the data is used? An application accesses data both to read and to modify it. The primitive data tasks are often expressed as create, read, update, and delete (CRUD); using them is known as CRUDing the data. Application code often does not make much distinction between the tasks; individual operations may mix reading the data with changing the data as needed. This simple approach works well when all clients of the data can use the same structure and contention is low. A single domain model can manage the data, make it accessible as domain objects, and ensure updates maintain its consistency. However, this approach becomes inadequate when different clients want different views across multiple sets of data, when the data is too widely used, and/or when multiple clients updating the data my unknowingly conflict with each other. For example, in a microservices architecture, each microservice should store and manage its own data, but a user interface may need to display data from several microservices. A query that gathers bits of data from multiple sources can be inefficient (time and bandwidth consumed accessing multiple data sources, CPU consumed transforming data, memory consumed by intermediate objects) and must be repeated each time the data is accessed. Another example is an enterprise database of record managing data required by multiple applications. It can become overloaded with too many clients needing too many connections to run too many threads performing too many transactions--such that the database becomes a performance bottleneck and can even crash. Another example is maintaining consistency of the data while clients concurrently make independent updates to the data. While each update may be consistent, they may conflict with each other. Database locking ensures that the updates don't change the same data concurrently, but doesn't ensure multiple independent changes result in a consistent data model. When data usage is more complex than a single domain model can facilitate, a more sophisticated approach is needed.","title":"Problems and Constraints"},{"location":"patterns/cqrs/#solution-and-pattern","text":"Refactor a domain model to separate operations for querying data and operations for updating data so that they may be handled independently. The CQRS pattern strictly segregates operations that read data from operations that update data. An operation can read data (the R in CRUD) or can write data (the CUD in CRUD), but not both. This separation can make using data much more manageable in several respects. The read operations and the write operations are simpler to implement because their functionality is more finely focused. The operations can be developed independently, potentially by separate teams. The operations can be optimized independently and can evolve independently, following changing user requirements more easily. These optimized operations can scale better, perform better, and security can be applied more precisely. The full CQRS pattern uses separate read and write databases. In doing so, the pattern segregates not just the APIs for accessing data or the models for managing data, but even segregates the database itself into two, a read/write database that is effectively write-only and one or more read-only databases. The adoption of the pattern can be applied in phases, incrementally from existing code. To illustrate this, we will use four stages that could be used incrementally or a developer can go directly from stage 0 to 3 without considering the others: Stage 0: Typical application data access Stage 1: Separate read and write APIs Stage 2: Separate read and write models Stage 3: Separate read and write databases","title":"Solution and Pattern"},{"location":"patterns/cqrs/#typical-application-data-access","text":"Before even beginning to apply the pattern, let\u2019s consider the typical app design for accessing data. This diagram shows an app with a domain model for accessing data persisted in a database of record, i.e. a single source of truth for that data. The domain model has an API that at a minimum enables clients to perform CRUD tasks on domain objects within the model. The domain model is an object representation of the database documents or records. It is comprised of domain objects that represent individual documents or records and the business logic for managing and using them. Domain-Driven Design (DDD) models these domain objects as entities \u2014\u201cobjects that have a distinct identity that runs through time and different representations\u201d\u2014and aggregates \u2014\u201ca cluster of domain objects that can be treated as a single unit\u201d; the aggregate root maintains the integrity of the aggregate as a whole. Ideally, the domain model\u2019s API should be more domain-specific than simply CRUDing of data. Instead, it should expose higher-level operations that represent business functionality like findCustomer() , placeOrder() , transferFunds() , and so on. These operations read and update data as needed, sometimes doing both in a single operation. They are correct as long as they fit the way the business works.","title":"Typical application data access"},{"location":"patterns/cqrs/#separate-read-and-write-apis","text":"The first and most visible step in applying the CQRS pattern is splitting the CRUD API into separate read and write APIs. This diagram shows the same domain model as before, but its single CRUD API is split into retrieve and modify APIs. The two APIs share the existing domain model but split the behavior: Read : The retrieve API is used to read the existing state of the objects in the domain model without changing that state. The API treats the domain state as read only. Write : The modify API is used to change the objects in the domain model. Changes are made using CUD tasks: create new domain objects, update the state in existing ones, and delete ones that are no longer needed. The operations in this API do not return result values, they return success (ack or void) or failure (nak or throw an exception). The create operation might return the primary of key of the entity, which can be generated either by the domain model or in the data source. This separation of APIs is an application of the Command Query Separation (CQS) pattern, which says to clearly separate methods that change state from those that don\u2019t. To do so, each of an object\u2019s methods can be in one of two categories (but not both): Query : Returns a result. Does not change the system\u2019s state nor cause any side effects that change the state. Command (a.k.a. modifiers or mutators): Changes the state of a system. Does not return a value, just an indication of success or failure. With this approach, the domain model works the same and provides access to the data the same as before. What has changed is the API for using the domain model. Whereas a higher-level operation might previously have both changed the state of the application and returned a part of that state, now each such operation is redesigned to only do one or the other. When the domain model splits its API into read and write operations, clients using the API must likewise split their functionality into querying and updating functionality. Most new web based applications are based in the single page application, with components and services that use and encapsulate remote API. So this separation of backend API fits well with modern web applications. This stage depends on the domain model being able to implement both the retrieve and modify APIs. A single domain model requires the retrieve and modify behavior to have similar, corresponding implementations. For them to evolve independently, the two APIs will need to be implemented with separate read and write models.","title":"Separate read and write APIs"},{"location":"patterns/cqrs/#separate-read-and-write-models","text":"The second step in applying the CQRS pattern is to split the domain model into separate read and write models. This doesn\u2019t just change the API for accessing domain functionality, it also changes the design of how that functionality is structured and implemented. This diagram shows that the domain model becomes the basis for a write model that handles changes to the domain objects, along with a separate read model used to access the state of the app. Naturally, the read model implements the retrieve API and the write model implements the modify API. Now, the application consists not only of separate APIs for querying and updating the domain objects, there\u2019s also separate business functionality for doing so. Both the read business functionality and the write business functionality share the same database. The write model is implemented by specializing the domain model to focus solely on maintaining the valid structure of domain objects when changing their state and by applying any business rules. Meanwhile, responsibility for returning domain objects is shifted to a separate read model. The read model defines data transfer objects (DTOs) designed specifically for the model to return just the data the client wants in a structure the client finds convenient. The read model knows how to gather the data used to populate the DTOs. DTOs encapsulate little if any domain functionality, they just bundle data into a convenient package that can easily be transmitted using a single method call, especially between processes. The read model should be able to implement the retrieve API by implementing the necessary queries and executing them. If the retrieve API is already built to return domain objects as results, the read model can continue to do so, or better yet, implements DTO types that are compatible with the domain objects and returns those. Likewise, the modify API was already implemented using the domain model, so the write model should preserve that. The write model may enhance the implementation to more explicitly implement a command interface or use command objects. This phase assumes that the read and write models can both be implemented using the same database of record the domain model has been using. To the extent this is true, the implementations of reading and writing can evolve independently and be optimized independently. This independence may become increasingly limited since they are both bound to the same database with a single schema or data model. To enable the read and write models to evolve independently, they may each need their own database.","title":"Separate read and write models"},{"location":"patterns/cqrs/#separate-read-and-write-databases","text":"The third step in applying the CQRS pattern\u2014-which implements the complete CQRS pattern solution\u2014-is splitting the database of record into separate read and write databases. This diagram shows the write model and read model, each supported by its own database. The overall solution consists of two main parts: the write solution that supports updating the data and the read solution that supports querying the data. The two parts are connected by the event bus. The write model has its own read/write database and the read model has its own read-only database. The read/write database still serves as the database of record (the single source of truth for the data) but is mostly used write-only: mostly written to and rarely read. Reading is offloaded onto a separate read database that contains the same data but is used read-only. The query database is effectively a cache of the database of record, with all of the inherit benefits and complexity of the Caching pattern. The query database contains a copy of the data in the database of record, with the copy structured and staged for easier access by the clients using the retrieve API. As a copy, overhead is needed to keep the copy synchronized with changes in the original. Latency in this synchronization process creates eventual consistency, during which the data copy is stale. The separate databases enable the separate read and write models and their respective retrieve and modify APIs to truly evolve independently. Not only can the read model or write model\u2019s implementation change without changing the other, but how each stores its data can be changed independently. This solution offers the following advantages: Scaling : The query load is moved from the write database to the read database. If the database of record is a scalability bottleneck and a lot of the load on it is caused by queries, unloading those query responsibilities can significantly improve the scalability of the combined data access. Performance : The schemas of the two databases can be different, enabling them to be designed and optimized independently for better performance. The write database can be optimized for data consistency and correctness, with capabilities such as stored procedures that fit the write model and assist with data updates. The read database can store the data in units that better fit the read model and are better optimized for querying, with larger rows requiring fewer joins. Notice that the design for this stage is significantly more complex than the design for the previous stage. Separate databases with copies of the same data may make data modeling and using data easier, but they require significant overhead to synchronize the data and keep the copies consistent. CQRS employs a couple of design features that support keeping the databases synchronized: Command Bus for queuing commands (optional): A more subtle and optional design decision is to queue the commands produced by the modify API, shown in the diagram as the command bus. This can significantly increase the throughput of multiple apps updating the database, as well as serialize updates to help avoid--or at least detect--merge conflicts. With the bus, a client making an update does not block synchronously while the change is written to the database. Rather, the request to change the database is captured as a command ( Design Patterns ) and put on a message queue, after which the client can proceed with other work. Asynchronously in the background, the write model processes the commands at the maximum sustainable rate that the database can handle, without the database ever becoming overloaded. If the database becomes temporarily unavailable, the commands queue and will be processed when the database becomes available once more. Event Bus for publishing update events (required): Whenever the write database is updated, a change notification is published as an event on the event bus. Interested parties can subscribe to the event bus to be notified when the database is updated. One such party is an event processor for the query database, which receives update events and processes them by updating the query database accordingly. In this way, every time the write database is updated, a corresponding update is made to the read database to keep it in sync. The connection between the command bus and the event bus is facilitated by an application of the Event Sourcing pattern , which keeps a change log that is suitable for publishing. Event sourcing maintains not only the current state of the data but also the history of how that current state was reached. For each command on the command bus, the write model performs these tasks to process the command: Logs the change Updates the database with the change Creates an update event describing the change and publishes it to the event bus The changes that are logged can be the commands from the command bus or the update events published to the event bus","title":"Separate read and write databases"},{"location":"patterns/cqrs/#considerations","text":"Keep these decisions in mind while applying this pattern: Client impact : Applying CQRS not only changes how data is stored and accessed, but also changes the APIs that clients use to access data. This means that each client must be redesigned to use the new APIs. Riskiness : A lot of the complexity of the pattern solution involves duplicating the data in two databases and keeping them synchronized. Risk comes from querying data that is stale or downright wrong because of problems with the synchronization. Eventual consistency : Clients querying data must expect that updates will have latency. In a microservices architecture, eventual data consistency is a given and acceptable in many of cases. Command queuing : Using a command bus as part of the write solution to queue the commands is optional but powerful. In addition to the benefits of queuing, the command objects can easily be stored in the change log and easily be converted into notification events. (In the next section, we illustrate a way to use event bus to queue commands as well.) Change log : The log of changes to the database of record can be either the list of commands from the command bus or the list of event notifications published on the event bus. The Event Sourcing pattern assumes it\u2019s a log of events, but that pattern doesn\u2019t include the command bus. An event list may be easier to scan as a history, whereas a command list is easier to replay. Create keys : Strict interpretation of the Command Query Separation (CQS) pattern says that command operations do not have return types. A possible exception is commands that create data: An operation that creates a new record or document typically returns the key for accessing the new data, which is convenient for the client. However, if the create operation is invoked asynchronously by a command on a command bus, the write model will need to perform a callback on the client to return the key. Messaging queues and topics : While messaging is used to implement both the command bus and event bus, the two busses use messaging differently. The command bus guarantees exactly once delivery. The event bus broadcasts each event to all interested event processors. Query database persistence : The database of record is always persistent. The query database is a cache that can be a persistent cache or an in-memory cache. If the cache is in-memory and is lost, it must be rebuilt completely from the database of record. Security : Controls on reading data and updating data can be applied separately using the two parts of the solution.","title":"Considerations"},{"location":"patterns/cqrs/#combining-event-sourcing-and-cqrs","text":"The CQRS application pattern is frequently associated with event sourcing: when doing event sourcing and domain driven design, we event source the aggregates or root entities. Aggregate creates events that are persisted. On top of the simple create, update and read by ID operations, the business requirements want to perform complex queries that can't be answered by a single aggregate. By just using event sourcing to be able to respond to a query like \"what are the orders of a customer\", then we have to rebuild the history of all orders and filter per customer. It is a lot of computation. This is linked to the problem of having conflicting domain models between query and persistence. As introduced in previous section, creations and updates are done as state notification events (change of state), and are persisted in the event log/store. The following figure, presents two separate microservices, one supporting the write model, and multiple other supporting the queries: The query part is separate processes that consume change log events and build a projection for future queries. The \"write\" part may persist in SQL while the read may use document oriented database with strong indexing and query capabilities. Or use in-memory database, or distributed cache... They do not need to be in the same language. With CQRS and ES the projections are retroactives. New query equals implementing new projection and read the events from the beginning of time or the recent committed state and snapshot. Read and write models are strongly decoupled and can evolve independently. It is important to note that the 'Command' part can still handle simple queries, primary-key based, like get order by id, or queries that do not involve joins. The event backbone, use a pub/sub model, and Kafka is a good candidate as an implementation technology. With this structure, the Read model microservice will most likely consume events from multiple topics to build the data projection based on joining those data streams. A query, to assess if the cold-chain was respected on the fresh food order shipment, will go to the voyage, container metrics, and order to be able to answer this question. This is where CQRS shines. We can note that, we can separate the API definition and management in a API gateway. The shipment order microservice is implementing this pattern. Some implementation items to consider: Consistency (ensure the data constraints are respected for each data transaction): CQRS without event sourcing has the same consistency guarantees as the database used to persist data and events. With Event Sourcing the consistency could be different, one for the write model and one for the read model. On write model, strong consistency is important to ensure the current state of the system is correct, so it leverages transaction, lock and sharding. On read side, we need less consistency, as they mostly work on stale data. Locking data on the read operation is not reasonable. Scalability : Separating read and write as two different microservices allows for high availability. Caching at the read level can be used to increase performance response time, and can be deployed as multiple standalone instances (Pods in Kubernetes). It is also possible to separate the query implementations between different services. Functions as service / serverless are good technology choices to implement complex queries. Availability : The write model sacrifices consistency for availability. This is a fact. The read model is eventually consistent so high availability is possible. In case of failure the system disables the writing of data but still is able to read them as they are served by different databases and services. With CQRS, the write model can evolve over time without impacting the read model, as long as the event model doesn't change. The read model requires additional tables, but they are often simpler and easier to understand. CQRS results in an increased number of objects, with commands, operations, events,... and packaging in deployable components or containers. It adds potentially different type of data sources. It is more complex. Some challenges to always consider: How to support event structure version management? How much data to keep in the event store (history)? How to adopt data duplication which results to eventual data consistency?. The CQRS pattern was introduced by Greg Young , and described in Martin Fowler's work on microservices. As you can see in previous figure, as soon as we see two arrows from the same component, we have to ask ourselves how does it work: the write model has to persist Order in its own database and then sends OrderCreated event to the topic... Should those operations be atomic and controlled with transaction? We detail this in next section.","title":"Combining event sourcing and CQRS"},{"location":"patterns/cqrs/#keeping-the-write-model-on-mainframe","text":"It is very important to note that the system of records and transaction processing is still easier to run on mainframe to support strong consistency. But with the move to cloud native development, it does not mean we have to move all the system of records to the cloud. Data pipelines can be put in place, but CQRS should help by keeping the write model on the current system of records and without impacting the current MIPS utilization move data in the eventual consistency world of the cloud native, distributed computing world. In the figure above, the write model follows the current transaction processing on the mainframe, change data capture push data to Event backbone for getting real time visibility into the distributed world. The read is the costly operation, dues to the joins to be done to build the projection views needed to expose data depending of the business use cases. This is more true with distributed microservices. All the write operations for the business entities kept in the mainframe's system of records are still done via the transaction. Reference data can be injected in one load job to topic and persisted in the event store so streaming applications can leverage them by joining with transactional data.","title":"Keeping the write model on Mainframe"},{"location":"patterns/cqrs/#the-consistency-challenges","text":"As introduced in the previous section, there is a potential problem of data inconsistency: once a command saves changes into the database, the consumers do not see the new or updated data until event notification completes processing. With traditional Java service, using JPA and JMS, the save and send operations can be part of the same XA transaction and both succeed or fail. With event sourcing pattern, the source of trust is the event source, which acts as a version control system, as shown in the diagram below. The steps for synchronizing changes to the data are: The write model creates the event and publishes it The consumer receives the event and extracts its payload The consumer updates its local datasource with the payload data If the consumer fails to process the update, it can persist the event to an error log Each error in the log can be replayed A command line interface replays an event via an admin API, which searches in the topic using this order id to replay the save operation This implementation causes a problem for the createOrder(order): string operation: The Order Service is supposed to return the new order complete with the order id that is a unique key, a key most likely created by the database. If updating the database fails, there is no new order yet and so no database key to use as the order ID. To avoid this problem, if the underlying technology supports assigning the new order's key, the service can generate the order ID and use that as the order's key in the database. It is important to clearly study the Kafka consumer API and the different parameters on how to support the read offset. We are addressing those implementation best practices in our consumer note.","title":"The consistency challenges"},{"location":"patterns/cqrs/#cqrs-and-change-data-capture","text":"There are other ways to support this dual operations level: When using Kafka, Kafka Connect has the capability to subscribe to databases via JDBC, allowing to poll tables for updates and then produce events to Kafka. There is an open-source change data capture solution based on extracting change events from database transaction logs, Debezium that helps to respond to insert, update and delete operations on databases and generate events accordingly. It supports databases like MySQL, Postgres, MongoDB and others. Write the order to the database and in the same transaction write to an event table ( \"outbox pattern\" ). Then use a polling to get the events to send to Kafka from this event table and delete the row in the table once the event is sent. Use the Change Data Capture from the database transaction log and generate events from this log. The IBM Infosphere CDC product helps to implement this pattern. For more detail about this solution see this product tour . The CQRS implementation using CDC will look like in the following diagram: What is important to note is that the event needs to be flexible on the data payload. On the view side, updates to the view part need to be idempotent.","title":"CQRS and Change Data Capture"},{"location":"patterns/cqrs/#delay-in-the-view","text":"There is a delay between the data persistence and the availability of the data in the Read model. For most business applications, it is perfectly acceptable. In web based data access most of the data are at stale. When there is a need for the client, calling the query operation, to know if the data is up-to-date, the service can define a versioning strategy. When the order data was entered in a form within a single page application, the \"create order\" operation should return the order with its unique key freshly created and the Single Page Application will have the last data. Here is an example of such operation: @POST public Response create ( OrderCreate dto ) { Order order = new Order ( UUID . randomUUID (). toString (), dto . getProductID (),...); // ... return Response . ok (). entity ( order ). build () }","title":"Delay in the view"},{"location":"patterns/cqrs/#schema-change","text":"What to do when we need to add attribute to event?. So we need to create a versioninig schema for event structure. You need to use flexible schema like json schema, Apache Avro or protocol buffer and may be, add an event adapter (as a function?) to translate between the different event structures.","title":"Schema change"},{"location":"patterns/cqrs/#code-reference","text":"The following project includes two sub modules, each deployable as a microservice to illustrate the command and query part: https://github.com/ibm-cloud-architecture/refarch-kc-order-ms","title":"Code reference"},{"location":"patterns/cqrs/#further-readings","text":"https://www.codeproject.com/Articles/555855/Introduction-to-CQRS http://udidahan.com/2009/12/09/clarified-cqrs https://martinfowler.com/bliki/CQRS.html https://microservices.io/patterns/data/cqrs.html https://community.risingstack.com/when-to-use-cqrs https://dzone.com/articles/concepts-of-cqrs https://martinfowler.com/bliki/CommandQuerySeparation.html https://www.martinfowler.com/eaaCatalog/domainModel.html https://dddcommunity.org/learning-ddd/what_is_ddd/ https://martinfowler.com/bliki/EvansClassification.html https://martinfowler.com/bliki/DDD_Aggregate.html https://martinfowler.com/eaaCatalog/dataTransferObject.html https://en.wikipedia.org/wiki/Command_pattern https://www.pearson.com/us/higher-education/program/Gamma-Design-Patterns-Elements-of-Reusable-Object-Oriented-Software/PGM14333.html","title":"Further readings"},{"location":"patterns/event-sourcing/","text":"Event Sourcing Problems and Constraints Most business applications are state based persistent where any update changes the previous state of business entities. The database keeps the last committed update. But some business application needs to explain how it reaches its current state . For that the application needs to keep history of business facts. Traditional domain oriented implementation builds a domain data model and map it to a RDBMS. As an example, in the simple Order model below, the database record will keep the last state of the Order entity, with the different addresses and the last ordered items in separate tables. If you need to implement a query that looks at what happened to the order over a time period, you need to change the model and add historical records, basically building a log table. Designing a service to manage the life cycle of this order will, most of the time, add a \"delete operation\" to remove data. For legal reason, most businesses do not remove data. As an example, a business ledger has to include new record(s) to compensate a previous transaction. There is no erasing of previously logged transactions. It is always possible to understand what was done in the past. Most business application needs to keep this capability. Solution and Pattern Event sourcing persists the state of a business entity, such an Order, as a sequence of state-changing events or immutable \"facts\" ordered over time. Event sourcing has its roots in the domain-driven design community. When the state of a system changes, an application issues a notification event of the state change. Any interested parties can become consumers of the event and take required actions. The state-change event is immutable stored in an event log or event store in time order. The event log becomes the principal source of truth. The system state can be recreated from a point in time by reprocessing the events. The history of state changes becomes an audit record for the business and is often a useful source of data for business analysts to gain insights into the business. You can see the \"removing an item\" event in the log is a new event. With this capability, we can count how often a specific product is removed for the shopping cart. In some cases, the event sourcing pattern is implemented completely within the event backbone. Kafka topic and partitions are the building blocks for event sourcing. However, you can also consider implementing the pattern with an external event store, which provides optimizations for how the data may be accessed and used. For example IBM Db2 Event store can provide the handlers and event store connected to the backbone and can provide optimization for down stream analytical processing of the data. An event store needs to store only three pieces of information: The type of event or aggregate. The sequence number of the event. The data as a serialized entity. More data can be added to help with diagnosis and audit, but the core functionality only requires a narrow set of fields. This gives rise to a very simple data design that can be heavily optimized for appending and retrieving sequences of records. With a central event logs, as provides by Kafka, producers append events to the log, and consumers read them from an offset (a sequence number). To get the final state of an entity, the consumer needs to replay all the events, which means replaying the changes to the state from the last committed offset or from the last snapshot or the origin of \"time\". Advantages The main goal is to be able to understand what happens to a business entity over time. But there are a set of interesting things that can be done: We can rebuild the data view within a microservice after it crashes, by reloading the event log. As events are ordered with time, we can apply complex event processing with temporal queries, time window operations, and looking at non-event. Be able to reverse the state and correct data with new events. Considerations When replaying the events, it may be important to avoid generating side effects. A common side effect is to send a notification on state change to other consumers. So the consumer of events need to be adapted to the query and business requirements. For example, if the code needs to answer to the question: \"what happened to the order ID 75 over time?\" then there is no side effect, only a report can be created each time the consumer runs. Sometime it may be too long to replay hundreds of events. In that case we can use snapshot, to capture the current state of an entity, and then replay events from the most recent snapshot. This is an optimization technique not needed for all event sourcing implementations. When state change events are in low volume there is no need for snapshots. Kafka is supporting the event sourcing pattern with the topic and partition . In our reference implementation we are validating event sourcing with Kafka in the Order microservice . The event sourcing pattern is well described in this article on microservices.io . It is a very important pattern to support eventual data consistency between microservices and for data synchronization between system as the event store becomes the source of truth. Another common use case, where event sourcing helps, is when developers push a new code version that corrupts the data: being able to see what was done on the data, and being able to reload from a previous state helps fixing problems. Command sourcing Command sourcing is a similar pattern as the event sourcing one, but the commands that modify the states are persisted instead of the events. This allows commands to be processed asynchronously, which can be relevant when the command execution takes a lot of time. One derived challenge is that the command may be executed multiple times, especially in case of failure. Therefore, it has to be idempotent ( making multiple identical requests has the same effect as making a single request). Finally, there is a need also to perform validation of the command to avoid keeping wrong commands in queue. For example, AddItem command is becoming AddItemValidated , then once persisted to a database it becomes an event as ItemAdded . So mixing command and event sourcing is a common practice. Business transactions are not ACID and span multiple services, they are more a serie of steps, each step is supported by a microservice responsible to update its own entity. We talk about \"eventual data consistency\". The event backbone needs to guarantee that events are delivered at least once and the microservices are responsible to manage their offset from the stream source and deal with inconsistency, by detecting duplicate events. At the microservice level, updating data and emitting event needs to be an atomic operation, to avoid inconsistency if the service crashes after the update to the datasource and before emitting the event. This can be done with an eventTable added to the microservice datasource and an event publisher that reads this table on a regular basis and change the state of the event once published. Another solution is to have a database transaction log reader or miner responsible to publish event on new row added to the log. One other approach to avoid the two-phase commit and inconsistency is to use an Event Store or Event Sourcing pattern to keep track of what is done on the business entity with enough information to rebuild the data state. Events are becoming facts describing state changes done on the business entity. Compendium Martin Fowler - event sourcing pattern Microservice design pattern from Chris Richardson Greg Young video on event sourcing at the goto; conference","title":"Event Sourcing"},{"location":"patterns/event-sourcing/#event-sourcing","text":"","title":"Event Sourcing"},{"location":"patterns/event-sourcing/#problems-and-constraints","text":"Most business applications are state based persistent where any update changes the previous state of business entities. The database keeps the last committed update. But some business application needs to explain how it reaches its current state . For that the application needs to keep history of business facts. Traditional domain oriented implementation builds a domain data model and map it to a RDBMS. As an example, in the simple Order model below, the database record will keep the last state of the Order entity, with the different addresses and the last ordered items in separate tables. If you need to implement a query that looks at what happened to the order over a time period, you need to change the model and add historical records, basically building a log table. Designing a service to manage the life cycle of this order will, most of the time, add a \"delete operation\" to remove data. For legal reason, most businesses do not remove data. As an example, a business ledger has to include new record(s) to compensate a previous transaction. There is no erasing of previously logged transactions. It is always possible to understand what was done in the past. Most business application needs to keep this capability.","title":"Problems and Constraints"},{"location":"patterns/event-sourcing/#solution-and-pattern","text":"Event sourcing persists the state of a business entity, such an Order, as a sequence of state-changing events or immutable \"facts\" ordered over time. Event sourcing has its roots in the domain-driven design community. When the state of a system changes, an application issues a notification event of the state change. Any interested parties can become consumers of the event and take required actions. The state-change event is immutable stored in an event log or event store in time order. The event log becomes the principal source of truth. The system state can be recreated from a point in time by reprocessing the events. The history of state changes becomes an audit record for the business and is often a useful source of data for business analysts to gain insights into the business. You can see the \"removing an item\" event in the log is a new event. With this capability, we can count how often a specific product is removed for the shopping cart. In some cases, the event sourcing pattern is implemented completely within the event backbone. Kafka topic and partitions are the building blocks for event sourcing. However, you can also consider implementing the pattern with an external event store, which provides optimizations for how the data may be accessed and used. For example IBM Db2 Event store can provide the handlers and event store connected to the backbone and can provide optimization for down stream analytical processing of the data. An event store needs to store only three pieces of information: The type of event or aggregate. The sequence number of the event. The data as a serialized entity. More data can be added to help with diagnosis and audit, but the core functionality only requires a narrow set of fields. This gives rise to a very simple data design that can be heavily optimized for appending and retrieving sequences of records. With a central event logs, as provides by Kafka, producers append events to the log, and consumers read them from an offset (a sequence number). To get the final state of an entity, the consumer needs to replay all the events, which means replaying the changes to the state from the last committed offset or from the last snapshot or the origin of \"time\".","title":"Solution and Pattern"},{"location":"patterns/event-sourcing/#advantages","text":"The main goal is to be able to understand what happens to a business entity over time. But there are a set of interesting things that can be done: We can rebuild the data view within a microservice after it crashes, by reloading the event log. As events are ordered with time, we can apply complex event processing with temporal queries, time window operations, and looking at non-event. Be able to reverse the state and correct data with new events.","title":"Advantages"},{"location":"patterns/event-sourcing/#considerations","text":"When replaying the events, it may be important to avoid generating side effects. A common side effect is to send a notification on state change to other consumers. So the consumer of events need to be adapted to the query and business requirements. For example, if the code needs to answer to the question: \"what happened to the order ID 75 over time?\" then there is no side effect, only a report can be created each time the consumer runs. Sometime it may be too long to replay hundreds of events. In that case we can use snapshot, to capture the current state of an entity, and then replay events from the most recent snapshot. This is an optimization technique not needed for all event sourcing implementations. When state change events are in low volume there is no need for snapshots. Kafka is supporting the event sourcing pattern with the topic and partition . In our reference implementation we are validating event sourcing with Kafka in the Order microservice . The event sourcing pattern is well described in this article on microservices.io . It is a very important pattern to support eventual data consistency between microservices and for data synchronization between system as the event store becomes the source of truth. Another common use case, where event sourcing helps, is when developers push a new code version that corrupts the data: being able to see what was done on the data, and being able to reload from a previous state helps fixing problems.","title":"Considerations"},{"location":"patterns/event-sourcing/#command-sourcing","text":"Command sourcing is a similar pattern as the event sourcing one, but the commands that modify the states are persisted instead of the events. This allows commands to be processed asynchronously, which can be relevant when the command execution takes a lot of time. One derived challenge is that the command may be executed multiple times, especially in case of failure. Therefore, it has to be idempotent ( making multiple identical requests has the same effect as making a single request). Finally, there is a need also to perform validation of the command to avoid keeping wrong commands in queue. For example, AddItem command is becoming AddItemValidated , then once persisted to a database it becomes an event as ItemAdded . So mixing command and event sourcing is a common practice. Business transactions are not ACID and span multiple services, they are more a serie of steps, each step is supported by a microservice responsible to update its own entity. We talk about \"eventual data consistency\". The event backbone needs to guarantee that events are delivered at least once and the microservices are responsible to manage their offset from the stream source and deal with inconsistency, by detecting duplicate events. At the microservice level, updating data and emitting event needs to be an atomic operation, to avoid inconsistency if the service crashes after the update to the datasource and before emitting the event. This can be done with an eventTable added to the microservice datasource and an event publisher that reads this table on a regular basis and change the state of the event once published. Another solution is to have a database transaction log reader or miner responsible to publish event on new row added to the log. One other approach to avoid the two-phase commit and inconsistency is to use an Event Store or Event Sourcing pattern to keep track of what is done on the business entity with enough information to rebuild the data state. Events are becoming facts describing state changes done on the business entity.","title":"Command sourcing"},{"location":"patterns/event-sourcing/#compendium","text":"Martin Fowler - event sourcing pattern Microservice design pattern from Chris Richardson Greg Young video on event sourcing at the goto; conference","title":"Compendium"},{"location":"patterns/saga/","text":"Saga Problems and Constraints With the adoption of one data source per microservice, there is an interesting challenge on how to support long running transaction cross microservices. With event backbone two phase commit is not an option. Solution and Pattern Introduced in 1987 by Hector Garcaa-Molrna Kenneth Salem paper the Saga pattern help to support a long running transaction that can be broken up to a collection of sub transactions that can be interleaved any way with other transactions. With microservice each transaction updates data within a single service, each subsequent steps may be triggered by previous completion. The following figure, based on our solution implementation , illustrates those concepts for an order transaction: When the order is created, the business process says, we need to allocate a \"voyage\", assign containers and update the list of containers to load on the ship. Those actions / commands are chained. The final state (in this schema, not in the reality, as the process has more steps) is the Order assigned state in the order microservice. With a unique application implementation, the integrity between order, voyage and container tables will be done via transactions. With distributed system we could not apply two phase commit transaction so the Saga pattern will help. SAGA pattern supports two types of implementation: Choreography and Orchestration. Services choreography With Choreography each service produces and listens to other service\u2019s events and decides if an action should be taken or not. The first service executes a transaction to its own data store and then publishes an event ( OrderCreated event (1)) as fact about its business entity update. It maintains the business entity status, (order.status) to the Pending state until the saga is completed. This event is listened by one or more services which execute local transactions and publish new events (VoyageAllocated (3), ReeferAssigned (4), PaymentProcessed (5)). The distributed transaction ends when the last service executes its local transaction or when a service does not publish any events or the event published is not polled by any of the saga\u2019s participants. For example, the Order microservice gets all the events from the other service and changed the Order state to be Accepted . In case of failure, the source microservice is keeping state and timer to monitor for the expected completion events. When a message from any service is missing, the source service, needs to trigger a compensation process: Rolling back a distributed transaction does not come for free. Normally you have to implement another operation/transaction to compensate for what has been done before. This will be a new event sent by the service responsible of the transaction integrity. In the order example, in the rare case where one of the service is not able to provide a positive response, no voyage found, or no Reefer container found, then the order needs to change to 'Uncompleted' status, and an event to the orders topic will claim the orderID is now uncompleted (OrderUncompleted event Step 1 above) . Any service that has something allocated for this orderId will 'unroll' their changes in their own data source (Steps 2,3,4 below). Also it is important to note, that if one of the service is taking time to answer this may not be a problem as the order is in pending state. If the business requirement stipulates to address an order within a small time period then the compensation process may start. Uncompleted orders can be reviewed by a business user for manual handling. Email can be automatically sent to the customer about issue related to his order. There are a lot of different ways to handle order issue at the business level. Services orchestration With orchestration, one service is responsible to drive each participant on what to do and when. It uses the different topics to control the saga by issuing event commands to the different service. It uses the event backbone as a queue processing to support the asynchronous invocations. In this case the event should be exactly once delivered and idempotent. Each participant produces response in their context and to the order topic. The orchestration layer needs to keep a state machine and acts once all the expected responses are received. If anything fails, the orchestrator is also responsible for coordinating the compensation process by sending rollback events with orderID and their respective impacted entity key (voyageID, reeferID, transactionID). Each participant will undo its previous operations. Orchestrator is a State Machine where each transformation corresponds to a command or message. See also this article from Chris Richardson on the Saga pattern. We have implemented the choreography saga pattern in the order management, voyage and refeer management microservices within the EDA reference implementation solution with a detailed explanation of the integration tests to validate the happy path , and the exception path with compensation.","title":"SAGA"},{"location":"patterns/saga/#saga","text":"","title":"Saga"},{"location":"patterns/saga/#problems-and-constraints","text":"With the adoption of one data source per microservice, there is an interesting challenge on how to support long running transaction cross microservices. With event backbone two phase commit is not an option.","title":"Problems and Constraints"},{"location":"patterns/saga/#solution-and-pattern","text":"Introduced in 1987 by Hector Garcaa-Molrna Kenneth Salem paper the Saga pattern help to support a long running transaction that can be broken up to a collection of sub transactions that can be interleaved any way with other transactions. With microservice each transaction updates data within a single service, each subsequent steps may be triggered by previous completion. The following figure, based on our solution implementation , illustrates those concepts for an order transaction: When the order is created, the business process says, we need to allocate a \"voyage\", assign containers and update the list of containers to load on the ship. Those actions / commands are chained. The final state (in this schema, not in the reality, as the process has more steps) is the Order assigned state in the order microservice. With a unique application implementation, the integrity between order, voyage and container tables will be done via transactions. With distributed system we could not apply two phase commit transaction so the Saga pattern will help. SAGA pattern supports two types of implementation: Choreography and Orchestration.","title":"Solution and Pattern"},{"location":"patterns/saga/#services-choreography","text":"With Choreography each service produces and listens to other service\u2019s events and decides if an action should be taken or not. The first service executes a transaction to its own data store and then publishes an event ( OrderCreated event (1)) as fact about its business entity update. It maintains the business entity status, (order.status) to the Pending state until the saga is completed. This event is listened by one or more services which execute local transactions and publish new events (VoyageAllocated (3), ReeferAssigned (4), PaymentProcessed (5)). The distributed transaction ends when the last service executes its local transaction or when a service does not publish any events or the event published is not polled by any of the saga\u2019s participants. For example, the Order microservice gets all the events from the other service and changed the Order state to be Accepted . In case of failure, the source microservice is keeping state and timer to monitor for the expected completion events. When a message from any service is missing, the source service, needs to trigger a compensation process: Rolling back a distributed transaction does not come for free. Normally you have to implement another operation/transaction to compensate for what has been done before. This will be a new event sent by the service responsible of the transaction integrity. In the order example, in the rare case where one of the service is not able to provide a positive response, no voyage found, or no Reefer container found, then the order needs to change to 'Uncompleted' status, and an event to the orders topic will claim the orderID is now uncompleted (OrderUncompleted event Step 1 above) . Any service that has something allocated for this orderId will 'unroll' their changes in their own data source (Steps 2,3,4 below). Also it is important to note, that if one of the service is taking time to answer this may not be a problem as the order is in pending state. If the business requirement stipulates to address an order within a small time period then the compensation process may start. Uncompleted orders can be reviewed by a business user for manual handling. Email can be automatically sent to the customer about issue related to his order. There are a lot of different ways to handle order issue at the business level.","title":"Services choreography"},{"location":"patterns/saga/#services-orchestration","text":"With orchestration, one service is responsible to drive each participant on what to do and when. It uses the different topics to control the saga by issuing event commands to the different service. It uses the event backbone as a queue processing to support the asynchronous invocations. In this case the event should be exactly once delivered and idempotent. Each participant produces response in their context and to the order topic. The orchestration layer needs to keep a state machine and acts once all the expected responses are received. If anything fails, the orchestrator is also responsible for coordinating the compensation process by sending rollback events with orderID and their respective impacted entity key (voyageID, reeferID, transactionID). Each participant will undo its previous operations. Orchestrator is a State Machine where each transformation corresponds to a command or message. See also this article from Chris Richardson on the Saga pattern. We have implemented the choreography saga pattern in the order management, voyage and refeer management microservices within the EDA reference implementation solution with a detailed explanation of the integration tests to validate the happy path , and the exception path with compensation.","title":"Services orchestration"},{"location":"patterns/transactional-outbox/","text":"Transactional outbox pattern When distributed transaction is not supported by the messaging middleware (like current Kafka version), it is important to ensure consistency between the records in the database and the events published. In the reference implementation we used the approach to publish to the topic as soon as an order is received via the API and then the same code, is consuming this events to persist to the database. With this approach if write to the topic operation fails, the application can return an error to the user, if the write operation to the database fails, the code can reload from the non-committed record. But there is another solution presented by the transactional outbox. For detailed information about this pattern see the documentation of the pattern in Chris Richardson's site: Transactional outbox . To summarize this pattern, the approach is to use an outbox table to keep the messages to sent and a message relay process to publish events inserted into database to the event backbone. In modern solution this relay is a change data capture agent. The following schema illustrates the approach: To get a concrete example of this pattern we have developed a deep dive lab using Quarkus Debezium outbox pattern and a DB2 database, or see our outbox on quarkus with Debezium and Postgresql implementation","title":"Outbox"},{"location":"patterns/transactional-outbox/#transactional-outbox-pattern","text":"When distributed transaction is not supported by the messaging middleware (like current Kafka version), it is important to ensure consistency between the records in the database and the events published. In the reference implementation we used the approach to publish to the topic as soon as an order is received via the API and then the same code, is consuming this events to persist to the database. With this approach if write to the topic operation fails, the application can return an error to the user, if the write operation to the database fails, the code can reload from the non-committed record. But there is another solution presented by the transactional outbox. For detailed information about this pattern see the documentation of the pattern in Chris Richardson's site: Transactional outbox . To summarize this pattern, the approach is to use an outbox table to keep the messages to sent and a message relay process to publish events inserted into database to the event backbone. In modern solution this relay is a change data capture agent. The following schema illustrates the approach: To get a concrete example of this pattern we have developed a deep dive lab using Quarkus Debezium outbox pattern and a DB2 database, or see our outbox on quarkus with Debezium and Postgresql implementation","title":"Transactional outbox pattern"},{"location":"technology/ibmcloudpaks/","text":"This article summarizes... Introduction IBM Cloud\u00ae Paks are AI-powered software for hybrid cloud that can help you fully implement intelligent workflows in your business to accelerate digital transformation. IBM Cloud Paks allow you to tap into the power of IBM Watson\u00ae to apply AI to your business to predict and shape future outcomes, automate complex processes, optimize your employees\u2019 time and create more meaningful and secure customer experiences. Built on Red Hat\u00ae OpenShift\u00ae, IBM Cloud Paks have a common foundation of enterprise components that accelerate development, deliver seamless integration, and help enhance collaboration and efficiency. Use cases Architecture","title":"IBM CloudPaks"},{"location":"technology/ibmcloudpaks/#introduction","text":"IBM Cloud\u00ae Paks are AI-powered software for hybrid cloud that can help you fully implement intelligent workflows in your business to accelerate digital transformation. IBM Cloud Paks allow you to tap into the power of IBM Watson\u00ae to apply AI to your business to predict and shape future outcomes, automate complex processes, optimize your employees\u2019 time and create more meaningful and secure customer experiences. Built on Red Hat\u00ae OpenShift\u00ae, IBM Cloud Paks have a common foundation of enterprise components that accelerate development, deliver seamless integration, and help enhance collaboration and efficiency.","title":"Introduction"},{"location":"technology/ibmcloudpaks/#use-cases","text":"","title":"Use cases"},{"location":"technology/ibmcloudpaks/#architecture","text":"","title":"Architecture"},{"location":"technology/kafka-overview/","text":"In this article we are summarizing what Apache Kafka is and grouping some references, notes and tips we gathered working with Kafka while producing the different assets for this Event Driven Architecture references. This content does not replace the excellent introduction every developer using Kafka should read. Introduction Kafka is a distributed real time event streaming platform with the following key capabilities: Publish and subscribe streams of records. Data are stored so consuming applications can pull the information they need, and keep track of what they have seen so far. It can handle hundreds of read and write operations per second from many producers and consumers. Atomic broadcast, send a record once, every subscriber gets it once. Store streams of data records on disk and replicate them within the distributed cluster for fault-tolerance. Persist data for a given time period before delete. Can grow elastically and transparently with no downtime. Built on top of the ZooKeeper synchronization service to keep topic, partitions and metadata highly available. Use cases The typical use cases where Kafka helps are: Centralize online data pipeline to decouple applications and microservices. Expose data to any application to consume. Pub/sub messaging for cloud native applications to improve communication inter microservices. Aggregation of event coming from multiple producers. Monitor distributed applications to produce centralized feed of operational data. Logs collector from multiple services. Implement event sourcing pattern out of the box, using configuration to keep message for a long time period. Data are replicated between brokers within the cluster and cross availability zones if needed. Manage loosely coupled communication between microservices. (See this note where we present a way to support a service mesh solution using asynchronous event) Architecture The diagram below presents Kafka's key components: Brokers Kafka runs as a cluster of broker servers that can, in theory, span multiple data centers. Each brokers manages data replication, topic/partition management, offset management and all the interesting delivery semantic. To cover multiple data centers within the same cluster, the network latency between data centers needs to be very low, at the 15ms or less, as there is a lot of communication between Kafka brokers and between Kafka brokers and zookeeper servers. The Kafka cluster stores streams of records in topics . Topic is referenced by producer to send data to, and subscribed by consumers to get data. Data in topic is persisted to file systems for a retention time period (Defined at the topic level). The file system can be network based. In the figure above, the Kafka brokers are allocated on three servers, with data within the topic are replicated two times. In production, it is recommended to use at least five nodes to authorize planned failure and un-planned failure, and when doing replicas, use a replica factor at least equals to three. Topics Topics represent end points to publish and consume records. Each record consists of a key, a value (the data payload as byte array), a timestamp and some metadata. Producers publish data records to topic and consumers subscribe to topics. When a record is produced without specifying a partition, a partition will be chosen using a hash of the key. If the record did not provide a timestamp, the producer will stamp the record with its current time (creation time or log append time). Producers hold a pool of buffers to keep records not yet transmitted to the server. Kafka store log data in its log.dir and topic maps to subdirectories in this log directory. Kafka uses topics with a pub/sub combined with queue model: it uses the concept of consumer group to divide the processing over a collection of consumer processes, running in parallel, and messages can be broadcasted to multiple groups. Consumer performs asynchronous pull to the connected brokers via the subscription to a topic. The figure below illustrates one topic having multiple partitions, replicated within the broker cluster: Partitions Partitions are basically used to parallelize the event processing when a single server would not be able to process all events, using the broker clustering. So to manage increase in the load of messages, Kafka uses partitions. Each broker may have zero or more partitions per topic. When creating topic we specify the number of partition to use. Kafka tolerates up to N-1 server failures without losing any messages. N is the replication factor for a given partition. Each partition is a time ordered immutable sequence of records, that are persisted for a long time period. It is a log. Topic is a labelled log. Consumers see messages in the order they are stored in the log. Each partition is replicated across a configurable number of servers for fault tolerance. The number of partition will depend on characteristics like the number of consumers, the traffic pattern, etc... You can have 2000 partitions per broker. Each partitioned message has a unique sequence id called offset (\"abcde, ab, a ...\" in the figure above are offsets). Those offset ids are defined when events arrived at the broker level, and are local to the partition. They are immutable. When a consumer reads a topic, it actually reads data from all the partitions. As a consumer reads data from a partition, it advances its offset. To read an event the consumer needs to use the topic name, the partition number and the last offset to read from. Brokers keep offset information in an hidden topic. Partitions guarantee that data with the same keys will be sent to the same consumer and in order. Partitions are saved to disk as append log. The older records are deleted after a given time period or if the size of log goes over a limit. It is possible to compact the log. The log compaction means, the last known value for each message key is kept. Compacted Topics are used in Streams processing for stateful operator to keep aggregate or grouping by key. You can read more about log compaction from the Kafka doc . Replication Each partition can be replicated across a number of servers. The replication factor is captured by the number of brokers to be used for replication. To ensure high availability it should be set to at least a value of three. Partitions have one leader and zero or more followers. The leader manages all the read and write requests for the partition. The followers replicate the leader content. We are addressing data replication in the high availability section below. Zookeeper Zookeeper is used to persist the component and platform states and it runs in cluster to ensure high availability. One zookeeper server is the leader and other are used in backup. Kafka does not keep state regarding consumers and producers. Depends on Kafka version, offsets are maintained in Zookeeper or in Kafka : newer versions use an internal Kafka topic called __consumer_offsets. In any case consumers can read next message (or from a specific offset) correctly even during broker server outrages. Access Controls are saved in Zookeeper Consumer group This is the way to group consumers so the processing of event is parallelized. The number of consumers in a group is the same as the number of partition defined in a topic. We are detailing consumer group implementation in this note High Availability As a distributed cluster, Kafka brokers ensure high availability to process new events. Topic has replication factor to support not loosing data in case of broker failure. You need at least 3 brokers to ensure availability and a replication factor set to 3 for each topic, so no data should be lost. In production it is recommended to use 5 brokers cluster to ensure the quorum is always set, but replica factor can still be set to 3. The brokers need to run on separate physical machines, and when cluster extends over multiple availability zone, a rack awareness configuration can be defined. Partition enables data locality, elasticity, scalability, high performance, parallelism, and fault tolerance. Each partition is replicated at least 3 times and allocated in different brokers. One replicas is the leader . In the case of broker failure (broker 1 in figure below), one of the existing partition in the remaining running brokers will take the leader role (e.g. red partition in broker 3): Replication and partition leadership The keys in the data record determine the partitioning of data in Kafka . The records with the same key will be in the same partition. As Kafka is keeping its cluster states in Apache Zookeeper , you also need to have at least a three node cluster for zookeeper. Writes to Zookeeper are only be performed on changes to the membership of consumer groups or on changes to the Kafka cluster itself. Assuming you are using the most recent Kafka version (after 0.9), it is possible to have a unique zookeeper cluster for multiple Kafka clusters. But the latency between Kafka and zookeeper needs to be under few milliseconds (< 15ms) anyway. Zookeepers and Brokers should have high availability communication via dual network, and each broker and node allocated on different racks and blades. Consumers and producers are using a list of bootstrap server names (also named advertiser.listeners) to contact the cluster. The list is used for cluster discovery, it does not need to keep the full set of server names or ip addresses. A Kafka cluster has exactly one broker that acts as the controller. Per design Kafka aims to run within a single data center. But it is still recommended to use multiple racks connected with low latency dual networks. With multiple racks you will have better fault tolerance, as one rack failure will impact only one broker. There is a configuration property to assign Kafka broker using rack awareness. (See this configuration from the product documentation). As introduced on the topic section above, data are replicated between brokers. The following diagram illustrates the best case scenario where followers fetch data from the partition leader, acknowledge the replications: Usually replicas is done in-sync, and the configuration settings specify the number of replicas in-sync needed: for example, a replicas 3 can have a minimum in-sync of 2, to tolerate 1 out of sync replica (1 broker outage). The leader maintains a set of in-sync-replicas (ISR) brokers: all the nodes which are up-to-date with the leader\u2019s log, and actively acknowledging new writes. Every write goes through the leader and is propagated to every node in the In Sync Replica set, or ISR. Followers consume messages from the leader just as a normal Kafka consumer would and apply them to their own log. Having the followers pull from the leader has the nice property of allowing the follower to naturally batch together log entries they are applying to their log. Once all nodes in the ISR have acknowledged the request, the leader considers it committed, and can acknowledge to the client. A message is considered committed when all in-sync replicas for that partition have applied it to their log. If a leader fails, followers elect a new one. The leadership of partitions is dynamic and changes as servers come and go. Applications do not need to take specific actions to handle the change in the leadership of a partition. The Kafka client library automatically reconnects to the new leader, although you will see increased latency while the cluster settles. Any replica in the ISR is eligible to be elected leader. When a leader waits to get acknowledge before committing a message there will be more potential leaders. With (#failure + 1) replicas there is no data lost. But there is a risk of having the single broker separated from the zookeeper cluster when network partition occurs. To tolerate f failures, both the majority vote and the ISR approach will wait for the same number of replicas to acknowledge before committing a message. Having higher replicas number like 5, will duplicate 5 times the data (more disk used) and impact throughput as data is sent 1+4 times over the network. Another important design distinction is that Kafka does not require that crashed nodes recover with all their data intact. Kafka protocol for allowing a replica to rejoin the ISR ensures that before rejoining, it must fully re-sync again even if it lost unflushed data in its crash. When a producer sends message, it can control how to get the response from the committed message: wait for all replicas to succeed, wait for one acknowledge, fire and forget. Consumers receive only committed messages. Always assess the latency requirements and consumers needs. Throughput is linked to the number of partitions within a topic and having more consumers running in parallel. Consumers and producers should better run on separate servers than the brokers nodes. Running in parallel, also means the order of event arrivals will be lost. Most of the time, consumers are processing events from a unique partition and Kafka record to partition assignment will guarantee that records with the same key hashcode will be in the same partition. So orders are preserved within a partition. But if consumer needs to read from multiple partitions then if ordered records is needed, the consumer needs to rebuild the order with some complex logic. For high availability assess any potential single point of failure, such as server, rack, network, power supply... We recommend reading this event stream article for planning your Kafka on Kubernetes installation. For the consumers code update, the recreation of the consumer instance within the consumer group will trigger the partition rebalancing. This includes all the state of the aggregated data calculations that were persisted on disk. Until this process is finished real-time events are not processed. It is possible to limit this impact by setting the group.initial.rebalance.delay.ms to delay the rebalancing process one one instance of the consumer dies. Nevertheless the rebalancing will still occur when the updated consumer will rejoin the consumer group. When consumers are stream processing using Kafka streams, it is important to note that during the rollover the downstream processing will see a lag in event arrival: the time for the consumer to reread from the last committed offset. So if end to end timing is becoming important, we need to setup a standby consumer cluster (cluster B). This consumer group has different name, but does the same processing logic, and is consuming the same events from the same topic as the active consumer group cluster (cluster A). The difference is that they do not send events to the downstream topic until they are set up active. So to process the release cluster B is set active while cluster A is set inactive. The downstream will not be that much impacted. Finally to be exhaustive, the control of the segment size for the change log topic, may be considered to avoid having the stream processing doing a lot of computation to reload its state when it restarts. To add new broker, you can deploy the runtime to a new server / rack / blade, and give it a unique ID. Broker will process new topic, but it is possible to use tool to migrate some existing topic/ partitions to the new server. The tool is used to reassign partitions across brokers. An ideal partition distribution would ensure even data load and partition sizes across all brokers. High Availability in the context of Kubernetes deployment The combination of Kafka with Kubernetes seems to be a sound approach, but it is not that easy to achieve. Kubernetes workloads prefer to be stateless, Kafka is a stateful platform and manages its own brokers, and replications across known servers. It knows the underlying infrastructure. In Kubernetes, nodes and pods may change dynamically. Clients need to be able to access each of the broker directly once they get the connection metadata. Having a service which will round robin across all brokers in the cluster will not work with Kafka. The figure below illustrates a Kubernetes deployment, where zookeeper and Kafka brokers are allocated to 3 worker nodes, with some event driven microservices deployed in separate worker nodes. Those microservices are consumers and producers of events from one to many topics. The advantages of deploying Kafka on Kubernetes cluster is to facilitate the management of stateful sets, by scheduling both the persistence volume and broker pods in a clean rolling rehydration. Services add a logical name to access brokers for any deployed workload within the cluster. The virtual network also enables transparent TLS communication between components. For any Kubernetes deployment real high availability is constrained by the application / workload deployed on it. The Kubernetes platform supports high availability by having at least the following configuration: At least three master nodes (always an odd number of nodes). One is active at master, the others are in standby. The election of the master is using the quorum algorithm. Three proxy nodes. At least three worker nodes, but with zookeeper and Kafka clusters, we may need to have at least three more nodes as we do not want to have zookeeper and Kafka brokers sharing the same host as other pods if the Kafka traffic is supposed to grow. Externalize the management stack to three manager nodes Shared storage outside of the cluster to support private image registry, audit logs, and statefulset data persistence (like the Kafka broker file systems). Use etcd cluster: See recommendations from this article . The virtual IP manager assigns virtual IP addresses to master and proxy nodes and monitors the health of the cluster. It leverages etcd for storing information, so it is important that etcd is high available too and connected to low latency network below 10ms. Traditionally disaster recovery and high availability were always consider separated subjects. Now active/active deployment where workloads are deployed in different data centers, is becoming a common request. For sure, you need multiple Kafka Brokers, which will connect to the same ZooKeeper Ensemble running at least five nodes (you can tolerate the loss of one server during the planned maintenance of another server). One Zookeeper server acts as a lead and the two others as stand-by. The diagram above illustrates a simple deployment where zookeeper servers and Kafka brokers are running in pods, in different worker nodes. It is a viable solution to start deploying solution on top of Kafka. When you have bigger cluster, it may be interesting to separate Zookeeper from Kafka nodes to limit the risk of failover, as zookeeper keeps state of the Kafka cluster topology and metadata. You will limit to have both the zookeeper leader and one Kafka broker dying at the same time. We use Kubernetes anti-affinity to ensure they are scheduled onto separate worker nodes that the ones used by zookeeper. It uses the labels on pods with a rule like: **Kafka** pod should not run on same node as zookeeper pods . Here is an example of such spec: apiVersion : v1 kind : Pod metadata : name : with-pod-affinity spec : affinity : podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : labelSelector : matchExpressions : - key : name operator : In values : - gc-zookeeper topologyKey : kubernetes.io/hostname We recommend reading the \"running zookeeper in k8s tutorial\" for understanding such configuration. For optimum performance, provision a fast storage class for persistence volume. Kafka uses the log.dirs property to configure the driver to persist logs. So you need to define multiple volumes/ drives to support log.dirs . Zookeeper should not be used by other applications deployed in k8s cluster, it has to be dedicated for one Kafka cluster only. In a multi-cluster configuration being used for disaster recovery purposes, messages sent between clusters will have different offsets in the two clusters. It is usual to use timestamps for position information when restarting applications for recovery after a disaster. For Kafka streaming with stateful processing like joins, event aggregation and correlation coming from multiple partitions, it is not easy to achieve high availability cross clusters: in the strictest case every event must be processed by the streaming service exactly once. Which means: producer emits data to different sites and be able to re-emit in case of failure. Brokers are known by producer via a list of hostnames and port numbers. communications between zookeepers and cluster nodes are redundant and safe for data losses consumers ensure idempotence... They have to tolerate data duplication and manage data integrity in their persistence layer. Within Kafka's boundary, data will not be lost, when doing proper configuration, also to support high availability the complexity moves to the producer and the consumer implementation. Kafka configuration is an art and you need to tune the parameters by use case: Partition replication for at least 3 replicas. Recall that in case of node failure, coordination of partition re-assignments is provided with ZooKeeper. End to end latency needs to be measured from producer (when a message is sent) to consumer (when it is read). A consumer is able to get a message when the brokers finish replicating to all in-synch replicas. Use the producer buffering capability to pace the message to the broker. Can use memory or time based threshold via producer properties. Define the number of partitions to drive consumer parallelism. More consumers running in parallel the higher is the throughput. When using multiple partitions the global ordering of message is lost. Assess the retention hours to control when old messages in topic can be deleted. It is possible to keep messages forever, and for some application it makes fully sense. Control the maximum message size the server can receive. Zookeeper is not CPU intensive and each server should have a least 2 GB of heap space and 4GB reserved. Two CPUs per server should be sufficient. Servers keep their entire state machine in memory, and write every mutation to a durable WAL (Write Ahead Log) on persistent storage. To prevent the WAL from growing without bound, ZooKeeper servers periodically snapshot their in memory state to storage. Use fast and dynamically provisioned persistence storage for both WAL and snapshot. Performance Considerations Performance will vary depending of the current Kafka broker nodes load: in Kubernetes deployment, with small production topology, nodes may shared with other pods. It is recommended to control the environment with dedicated nodes for Kafka to achieve higher throughput. Performance will always depend on numerous factors including message throughput, message size, hardware, configuration settings, ... Performance may be linked to different focuses: Resilience: ensuring replication and not loosing data Throughput: ensuring message processing performance Payload size: support larger message Resilience When defining a topic, we need to specify the replicas factor to match the be at least 3 and then set the minimum number of in-sync replicas that specifies how may replicas must acknowledge a write to satisfy a producer that requests acknowledgments from all replicas. ( min.insync.replicas ). The replication of message data between brokers can consume a lot of network bandwidth so isolating replication traffic from application traffic can benefit performance. To achieve this, all replication traffic is configured to flow on a dedicated internal network. Throughput To achieve higher throughput the messages are not replicated across brokers and the acknowledgement can be set to only one broker. Expose resiliency to failures. The number of producers and consumers are aligned, and the number of partitions matches the number of consumers. All consumers are in the same consumer group. Measurement has to be done from the producer code. With 12 producers on a 3 brokers cluster and small payload (128 bytes), with 24 consumers the measured throughput is around 2.3 M messages / second. Payload size From measurement tests done using Kafka producer performance tool, there is a 1/log(s) curve, where below 10k bytes the performances are correct and then slowly degrade from 3000 msg /s (10k bytes msg) to 65 msg/s (515kb msg). To do performance test the event-streams-sample-producer github provides producer tool in Java, using a group of threads to run in multi cores machine. This project can be dockerized, and deployed in k8s. It uses the Kafka tool named: ProducerPerformance.java in the jar: <dependency> <groupId> org.apache.Kafka </groupId> <artifactId> Kafka-tools </artifactId> </dependency> Parameter considerations There are a lot of factors and parameters that needs to be tuned to improve performance at the brokers threading level ( num.replica.fetchers, num.io.threads, num.network.threads, log.cleaner.threads ) and the pod resources constraints. See configuration documentation . Openshift specifics When exposing the Kafka broker via Routes, the traffic is encrypted with TLS, so client needs to deal with TLS certificates and encryption. Routes are exposed via DNS and HAProxy router. The router will act as middleman between Kafka clients and brokers, adding latency, and it can become bottleneck. The traffic generated by client needs to be sized and in case of the router needs to be scaled up, and even isolate the routing by adding a separate router for the Kafka routes. Disaster Recovery With the current implementation it is recommended to have one cluster per data center / availability zone. Consumers and producers are co-located to the brokers cluster. When there are needs to keep some part of the data replicated in both data center, you need to assess what kind of data can be aggregated, and if Kafka mirroring tool can be used. The tool consumes from a source cluster, from a given topic, and produces to a destination cluster with the same named topic. It keeps the message key for partitioning, so order is preserved. The above diagram is using Kafka MirrorMaker with a master to slave deployment. Within the data center 2, the brokers are here to manage the topics and events. When there is no consumer running, nothing happen. Consumers and producers can be started when DC1 fails. This is the active/passive model. In fact, we could have consumers within the DC2 processing topics to manage a read-only model, keeping in memory their projection view, as presented in the CQRS pattern . The second solution is to use one mirror maker in each site, for each topic. This is an active - active topology: consumers and producers are on both sites. But to avoid infinite loop, we need to use naming convention for the topic, or only produce in the cluster of the main topic. Consumers consume from the replicated topic. When you want to deploy solution that spreads over multiple regions to support global streaming, you need to address the following challenges: How do you make data available to applications across multiple data centers? How to serve data closer to the geography? How to be compliant on regulations, like GDPR? How to address no duplication of records? Kafka 2.4 introduces the capability for a consumer to read messages from the closest replica using some rack-id and specific algorithm. This capability will help to extend the cluster to multiple data center and avoid having consumers going over WAN communication. Solution Considerations There are a set of design considerations to assess for each Kafka solution: Topics Performance is more a function of number of partitions than topics. Expect that each topic has at least one partition. When considering latency you should aim for limiting to hundreds of topic-partition per broker node. What of the most important question is what topics to use?. What is an event type? Should we use one topic to support multiple event types? Let define that an event type is linked to a main business entity like an Order, a ship, a FridgeredContainer. OrderCreated, OrderCancelled, OrderUpdated, OrderClosed are events linked to the states of the Order. The order of those events matter. So the natural approach is to use one topic per data type or schema, specially when using the topic as Event Sourcing where event order is important to build the audit log. You will use a unique partition to support that. The orderID is the partition key and all events related to the order are in the same topic. The important requirement to consider is the sequencing or event order. When event order is very important then use a unique partition, and use the entity unique identifier as key. Ordering is not preserved across partitions. When dealing with entity, independent entities may be in separate topics, when strongly related one may stay together. Other best practices: When event order is important use the same topic and use the entity unique identifier as partition key. When two entities are related together by containment relationship then they can be in the same topic. Different entities are separated to different topics. It is possible to group topics in coarse grained one when we discover that several consumers are listening to the same topics. Clearly define the partition key as it could be an compound key based on multiple entities. With Kafka stream, state store or KTable, you should separate the changelog topic from the others. Producers When developing a record producer you need to assess the following: What is the expected throughput to send events? Event size * average throughput combined with the expected latency help to compute buffer size. Can the producer batch events together to send them in batch over one send operation? Is there a risk for loosing communication? Tune the RETRIES_CONFIG and buffer size Assess once to exactly once delivery requirement. Look at idempotent producer. See implementation considerations discussion Consumers From the consumer point of view a set of items need to be addressed during design phase: Do you need to group consumers for parallel consumption of events? What is the processing done once the record is processed out of the topic? And how a record is supposed to be consumed?. How to persist consumer committed position? (the last offset that has been stored securely) Assess if offsets need to be persisted outside of Kafka?. From version 0.9 offset management is more efficient, and synchronous or asynchronous operations can be done from the consumer code. Does record time sensitive, and it is possible that consumers fall behind, so when a consumer restarts he can bypass missed records? Do the consumer needs to perform joins, aggregations between multiple partitions? See implementation considerations discussion Deployment In this section we provide the instructions for getting Kafka deployed in your vanilla kubernetes environment through the Strimzi kubernetes operator or getting the IBM Event Streams product (based on Kafka) deployed on your IBM Cloud Private/OpenShift cluster or in your IBM Cloud account as a managed service. Kubernetes Operator It is important to note that the deployment and management of stateful application in Kubernetes should, now, use the proposed Operator Framework introduced by Red Hat and Google. One important contribution is the Strimzi Kafka operator that simplifies the deployment of Kafka within k8s by adding a set of operators to deploy and manage Kafka clusters, topics, users and more. IBM Event Streams IBM Event Streams is an event-streaming platform based on the open-source Apache Kafka\u00ae project. It can be installed on IBM Cloud Private (ICP) cluster, OpenShift cluster or as a hosted service in IBM Cloud. Instructions for installing IBM Event Streams on your cluster as well as getting an instance as a hosted service in IBM Cloud can be found here","title":"Kafka Summary"},{"location":"technology/kafka-overview/#introduction","text":"Kafka is a distributed real time event streaming platform with the following key capabilities: Publish and subscribe streams of records. Data are stored so consuming applications can pull the information they need, and keep track of what they have seen so far. It can handle hundreds of read and write operations per second from many producers and consumers. Atomic broadcast, send a record once, every subscriber gets it once. Store streams of data records on disk and replicate them within the distributed cluster for fault-tolerance. Persist data for a given time period before delete. Can grow elastically and transparently with no downtime. Built on top of the ZooKeeper synchronization service to keep topic, partitions and metadata highly available.","title":"Introduction"},{"location":"technology/kafka-overview/#use-cases","text":"The typical use cases where Kafka helps are: Centralize online data pipeline to decouple applications and microservices. Expose data to any application to consume. Pub/sub messaging for cloud native applications to improve communication inter microservices. Aggregation of event coming from multiple producers. Monitor distributed applications to produce centralized feed of operational data. Logs collector from multiple services. Implement event sourcing pattern out of the box, using configuration to keep message for a long time period. Data are replicated between brokers within the cluster and cross availability zones if needed. Manage loosely coupled communication between microservices. (See this note where we present a way to support a service mesh solution using asynchronous event)","title":"Use cases"},{"location":"technology/kafka-overview/#architecture","text":"The diagram below presents Kafka's key components:","title":"Architecture"},{"location":"technology/kafka-overview/#brokers","text":"Kafka runs as a cluster of broker servers that can, in theory, span multiple data centers. Each brokers manages data replication, topic/partition management, offset management and all the interesting delivery semantic. To cover multiple data centers within the same cluster, the network latency between data centers needs to be very low, at the 15ms or less, as there is a lot of communication between Kafka brokers and between Kafka brokers and zookeeper servers. The Kafka cluster stores streams of records in topics . Topic is referenced by producer to send data to, and subscribed by consumers to get data. Data in topic is persisted to file systems for a retention time period (Defined at the topic level). The file system can be network based. In the figure above, the Kafka brokers are allocated on three servers, with data within the topic are replicated two times. In production, it is recommended to use at least five nodes to authorize planned failure and un-planned failure, and when doing replicas, use a replica factor at least equals to three.","title":"Brokers"},{"location":"technology/kafka-overview/#topics","text":"Topics represent end points to publish and consume records. Each record consists of a key, a value (the data payload as byte array), a timestamp and some metadata. Producers publish data records to topic and consumers subscribe to topics. When a record is produced without specifying a partition, a partition will be chosen using a hash of the key. If the record did not provide a timestamp, the producer will stamp the record with its current time (creation time or log append time). Producers hold a pool of buffers to keep records not yet transmitted to the server. Kafka store log data in its log.dir and topic maps to subdirectories in this log directory. Kafka uses topics with a pub/sub combined with queue model: it uses the concept of consumer group to divide the processing over a collection of consumer processes, running in parallel, and messages can be broadcasted to multiple groups. Consumer performs asynchronous pull to the connected brokers via the subscription to a topic. The figure below illustrates one topic having multiple partitions, replicated within the broker cluster:","title":"Topics"},{"location":"technology/kafka-overview/#partitions","text":"Partitions are basically used to parallelize the event processing when a single server would not be able to process all events, using the broker clustering. So to manage increase in the load of messages, Kafka uses partitions. Each broker may have zero or more partitions per topic. When creating topic we specify the number of partition to use. Kafka tolerates up to N-1 server failures without losing any messages. N is the replication factor for a given partition. Each partition is a time ordered immutable sequence of records, that are persisted for a long time period. It is a log. Topic is a labelled log. Consumers see messages in the order they are stored in the log. Each partition is replicated across a configurable number of servers for fault tolerance. The number of partition will depend on characteristics like the number of consumers, the traffic pattern, etc... You can have 2000 partitions per broker. Each partitioned message has a unique sequence id called offset (\"abcde, ab, a ...\" in the figure above are offsets). Those offset ids are defined when events arrived at the broker level, and are local to the partition. They are immutable. When a consumer reads a topic, it actually reads data from all the partitions. As a consumer reads data from a partition, it advances its offset. To read an event the consumer needs to use the topic name, the partition number and the last offset to read from. Brokers keep offset information in an hidden topic. Partitions guarantee that data with the same keys will be sent to the same consumer and in order. Partitions are saved to disk as append log. The older records are deleted after a given time period or if the size of log goes over a limit. It is possible to compact the log. The log compaction means, the last known value for each message key is kept. Compacted Topics are used in Streams processing for stateful operator to keep aggregate or grouping by key. You can read more about log compaction from the Kafka doc .","title":"Partitions"},{"location":"technology/kafka-overview/#replication","text":"Each partition can be replicated across a number of servers. The replication factor is captured by the number of brokers to be used for replication. To ensure high availability it should be set to at least a value of three. Partitions have one leader and zero or more followers. The leader manages all the read and write requests for the partition. The followers replicate the leader content. We are addressing data replication in the high availability section below.","title":"Replication"},{"location":"technology/kafka-overview/#zookeeper","text":"Zookeeper is used to persist the component and platform states and it runs in cluster to ensure high availability. One zookeeper server is the leader and other are used in backup. Kafka does not keep state regarding consumers and producers. Depends on Kafka version, offsets are maintained in Zookeeper or in Kafka : newer versions use an internal Kafka topic called __consumer_offsets. In any case consumers can read next message (or from a specific offset) correctly even during broker server outrages. Access Controls are saved in Zookeeper","title":"Zookeeper"},{"location":"technology/kafka-overview/#consumer-group","text":"This is the way to group consumers so the processing of event is parallelized. The number of consumers in a group is the same as the number of partition defined in a topic. We are detailing consumer group implementation in this note","title":"Consumer group"},{"location":"technology/kafka-overview/#high-availability","text":"As a distributed cluster, Kafka brokers ensure high availability to process new events. Topic has replication factor to support not loosing data in case of broker failure. You need at least 3 brokers to ensure availability and a replication factor set to 3 for each topic, so no data should be lost. In production it is recommended to use 5 brokers cluster to ensure the quorum is always set, but replica factor can still be set to 3. The brokers need to run on separate physical machines, and when cluster extends over multiple availability zone, a rack awareness configuration can be defined. Partition enables data locality, elasticity, scalability, high performance, parallelism, and fault tolerance. Each partition is replicated at least 3 times and allocated in different brokers. One replicas is the leader . In the case of broker failure (broker 1 in figure below), one of the existing partition in the remaining running brokers will take the leader role (e.g. red partition in broker 3):","title":"High Availability"},{"location":"technology/kafka-overview/#replication-and-partition-leadership","text":"The keys in the data record determine the partitioning of data in Kafka . The records with the same key will be in the same partition. As Kafka is keeping its cluster states in Apache Zookeeper , you also need to have at least a three node cluster for zookeeper. Writes to Zookeeper are only be performed on changes to the membership of consumer groups or on changes to the Kafka cluster itself. Assuming you are using the most recent Kafka version (after 0.9), it is possible to have a unique zookeeper cluster for multiple Kafka clusters. But the latency between Kafka and zookeeper needs to be under few milliseconds (< 15ms) anyway. Zookeepers and Brokers should have high availability communication via dual network, and each broker and node allocated on different racks and blades. Consumers and producers are using a list of bootstrap server names (also named advertiser.listeners) to contact the cluster. The list is used for cluster discovery, it does not need to keep the full set of server names or ip addresses. A Kafka cluster has exactly one broker that acts as the controller. Per design Kafka aims to run within a single data center. But it is still recommended to use multiple racks connected with low latency dual networks. With multiple racks you will have better fault tolerance, as one rack failure will impact only one broker. There is a configuration property to assign Kafka broker using rack awareness. (See this configuration from the product documentation). As introduced on the topic section above, data are replicated between brokers. The following diagram illustrates the best case scenario where followers fetch data from the partition leader, acknowledge the replications: Usually replicas is done in-sync, and the configuration settings specify the number of replicas in-sync needed: for example, a replicas 3 can have a minimum in-sync of 2, to tolerate 1 out of sync replica (1 broker outage). The leader maintains a set of in-sync-replicas (ISR) brokers: all the nodes which are up-to-date with the leader\u2019s log, and actively acknowledging new writes. Every write goes through the leader and is propagated to every node in the In Sync Replica set, or ISR. Followers consume messages from the leader just as a normal Kafka consumer would and apply them to their own log. Having the followers pull from the leader has the nice property of allowing the follower to naturally batch together log entries they are applying to their log. Once all nodes in the ISR have acknowledged the request, the leader considers it committed, and can acknowledge to the client. A message is considered committed when all in-sync replicas for that partition have applied it to their log. If a leader fails, followers elect a new one. The leadership of partitions is dynamic and changes as servers come and go. Applications do not need to take specific actions to handle the change in the leadership of a partition. The Kafka client library automatically reconnects to the new leader, although you will see increased latency while the cluster settles. Any replica in the ISR is eligible to be elected leader. When a leader waits to get acknowledge before committing a message there will be more potential leaders. With (#failure + 1) replicas there is no data lost. But there is a risk of having the single broker separated from the zookeeper cluster when network partition occurs. To tolerate f failures, both the majority vote and the ISR approach will wait for the same number of replicas to acknowledge before committing a message. Having higher replicas number like 5, will duplicate 5 times the data (more disk used) and impact throughput as data is sent 1+4 times over the network. Another important design distinction is that Kafka does not require that crashed nodes recover with all their data intact. Kafka protocol for allowing a replica to rejoin the ISR ensures that before rejoining, it must fully re-sync again even if it lost unflushed data in its crash. When a producer sends message, it can control how to get the response from the committed message: wait for all replicas to succeed, wait for one acknowledge, fire and forget. Consumers receive only committed messages. Always assess the latency requirements and consumers needs. Throughput is linked to the number of partitions within a topic and having more consumers running in parallel. Consumers and producers should better run on separate servers than the brokers nodes. Running in parallel, also means the order of event arrivals will be lost. Most of the time, consumers are processing events from a unique partition and Kafka record to partition assignment will guarantee that records with the same key hashcode will be in the same partition. So orders are preserved within a partition. But if consumer needs to read from multiple partitions then if ordered records is needed, the consumer needs to rebuild the order with some complex logic. For high availability assess any potential single point of failure, such as server, rack, network, power supply... We recommend reading this event stream article for planning your Kafka on Kubernetes installation. For the consumers code update, the recreation of the consumer instance within the consumer group will trigger the partition rebalancing. This includes all the state of the aggregated data calculations that were persisted on disk. Until this process is finished real-time events are not processed. It is possible to limit this impact by setting the group.initial.rebalance.delay.ms to delay the rebalancing process one one instance of the consumer dies. Nevertheless the rebalancing will still occur when the updated consumer will rejoin the consumer group. When consumers are stream processing using Kafka streams, it is important to note that during the rollover the downstream processing will see a lag in event arrival: the time for the consumer to reread from the last committed offset. So if end to end timing is becoming important, we need to setup a standby consumer cluster (cluster B). This consumer group has different name, but does the same processing logic, and is consuming the same events from the same topic as the active consumer group cluster (cluster A). The difference is that they do not send events to the downstream topic until they are set up active. So to process the release cluster B is set active while cluster A is set inactive. The downstream will not be that much impacted. Finally to be exhaustive, the control of the segment size for the change log topic, may be considered to avoid having the stream processing doing a lot of computation to reload its state when it restarts. To add new broker, you can deploy the runtime to a new server / rack / blade, and give it a unique ID. Broker will process new topic, but it is possible to use tool to migrate some existing topic/ partitions to the new server. The tool is used to reassign partitions across brokers. An ideal partition distribution would ensure even data load and partition sizes across all brokers.","title":"Replication and partition leadership"},{"location":"technology/kafka-overview/#high-availability-in-the-context-of-kubernetes-deployment","text":"The combination of Kafka with Kubernetes seems to be a sound approach, but it is not that easy to achieve. Kubernetes workloads prefer to be stateless, Kafka is a stateful platform and manages its own brokers, and replications across known servers. It knows the underlying infrastructure. In Kubernetes, nodes and pods may change dynamically. Clients need to be able to access each of the broker directly once they get the connection metadata. Having a service which will round robin across all brokers in the cluster will not work with Kafka. The figure below illustrates a Kubernetes deployment, where zookeeper and Kafka brokers are allocated to 3 worker nodes, with some event driven microservices deployed in separate worker nodes. Those microservices are consumers and producers of events from one to many topics. The advantages of deploying Kafka on Kubernetes cluster is to facilitate the management of stateful sets, by scheduling both the persistence volume and broker pods in a clean rolling rehydration. Services add a logical name to access brokers for any deployed workload within the cluster. The virtual network also enables transparent TLS communication between components. For any Kubernetes deployment real high availability is constrained by the application / workload deployed on it. The Kubernetes platform supports high availability by having at least the following configuration: At least three master nodes (always an odd number of nodes). One is active at master, the others are in standby. The election of the master is using the quorum algorithm. Three proxy nodes. At least three worker nodes, but with zookeeper and Kafka clusters, we may need to have at least three more nodes as we do not want to have zookeeper and Kafka brokers sharing the same host as other pods if the Kafka traffic is supposed to grow. Externalize the management stack to three manager nodes Shared storage outside of the cluster to support private image registry, audit logs, and statefulset data persistence (like the Kafka broker file systems). Use etcd cluster: See recommendations from this article . The virtual IP manager assigns virtual IP addresses to master and proxy nodes and monitors the health of the cluster. It leverages etcd for storing information, so it is important that etcd is high available too and connected to low latency network below 10ms. Traditionally disaster recovery and high availability were always consider separated subjects. Now active/active deployment where workloads are deployed in different data centers, is becoming a common request. For sure, you need multiple Kafka Brokers, which will connect to the same ZooKeeper Ensemble running at least five nodes (you can tolerate the loss of one server during the planned maintenance of another server). One Zookeeper server acts as a lead and the two others as stand-by. The diagram above illustrates a simple deployment where zookeeper servers and Kafka brokers are running in pods, in different worker nodes. It is a viable solution to start deploying solution on top of Kafka. When you have bigger cluster, it may be interesting to separate Zookeeper from Kafka nodes to limit the risk of failover, as zookeeper keeps state of the Kafka cluster topology and metadata. You will limit to have both the zookeeper leader and one Kafka broker dying at the same time. We use Kubernetes anti-affinity to ensure they are scheduled onto separate worker nodes that the ones used by zookeeper. It uses the labels on pods with a rule like: **Kafka** pod should not run on same node as zookeeper pods . Here is an example of such spec: apiVersion : v1 kind : Pod metadata : name : with-pod-affinity spec : affinity : podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : labelSelector : matchExpressions : - key : name operator : In values : - gc-zookeeper topologyKey : kubernetes.io/hostname We recommend reading the \"running zookeeper in k8s tutorial\" for understanding such configuration. For optimum performance, provision a fast storage class for persistence volume. Kafka uses the log.dirs property to configure the driver to persist logs. So you need to define multiple volumes/ drives to support log.dirs . Zookeeper should not be used by other applications deployed in k8s cluster, it has to be dedicated for one Kafka cluster only. In a multi-cluster configuration being used for disaster recovery purposes, messages sent between clusters will have different offsets in the two clusters. It is usual to use timestamps for position information when restarting applications for recovery after a disaster. For Kafka streaming with stateful processing like joins, event aggregation and correlation coming from multiple partitions, it is not easy to achieve high availability cross clusters: in the strictest case every event must be processed by the streaming service exactly once. Which means: producer emits data to different sites and be able to re-emit in case of failure. Brokers are known by producer via a list of hostnames and port numbers. communications between zookeepers and cluster nodes are redundant and safe for data losses consumers ensure idempotence... They have to tolerate data duplication and manage data integrity in their persistence layer. Within Kafka's boundary, data will not be lost, when doing proper configuration, also to support high availability the complexity moves to the producer and the consumer implementation. Kafka configuration is an art and you need to tune the parameters by use case: Partition replication for at least 3 replicas. Recall that in case of node failure, coordination of partition re-assignments is provided with ZooKeeper. End to end latency needs to be measured from producer (when a message is sent) to consumer (when it is read). A consumer is able to get a message when the brokers finish replicating to all in-synch replicas. Use the producer buffering capability to pace the message to the broker. Can use memory or time based threshold via producer properties. Define the number of partitions to drive consumer parallelism. More consumers running in parallel the higher is the throughput. When using multiple partitions the global ordering of message is lost. Assess the retention hours to control when old messages in topic can be deleted. It is possible to keep messages forever, and for some application it makes fully sense. Control the maximum message size the server can receive. Zookeeper is not CPU intensive and each server should have a least 2 GB of heap space and 4GB reserved. Two CPUs per server should be sufficient. Servers keep their entire state machine in memory, and write every mutation to a durable WAL (Write Ahead Log) on persistent storage. To prevent the WAL from growing without bound, ZooKeeper servers periodically snapshot their in memory state to storage. Use fast and dynamically provisioned persistence storage for both WAL and snapshot.","title":"High Availability in the context of Kubernetes deployment"},{"location":"technology/kafka-overview/#performance-considerations","text":"Performance will vary depending of the current Kafka broker nodes load: in Kubernetes deployment, with small production topology, nodes may shared with other pods. It is recommended to control the environment with dedicated nodes for Kafka to achieve higher throughput. Performance will always depend on numerous factors including message throughput, message size, hardware, configuration settings, ... Performance may be linked to different focuses: Resilience: ensuring replication and not loosing data Throughput: ensuring message processing performance Payload size: support larger message","title":"Performance Considerations"},{"location":"technology/kafka-overview/#resilience","text":"When defining a topic, we need to specify the replicas factor to match the be at least 3 and then set the minimum number of in-sync replicas that specifies how may replicas must acknowledge a write to satisfy a producer that requests acknowledgments from all replicas. ( min.insync.replicas ). The replication of message data between brokers can consume a lot of network bandwidth so isolating replication traffic from application traffic can benefit performance. To achieve this, all replication traffic is configured to flow on a dedicated internal network.","title":"Resilience"},{"location":"technology/kafka-overview/#throughput","text":"To achieve higher throughput the messages are not replicated across brokers and the acknowledgement can be set to only one broker. Expose resiliency to failures. The number of producers and consumers are aligned, and the number of partitions matches the number of consumers. All consumers are in the same consumer group. Measurement has to be done from the producer code. With 12 producers on a 3 brokers cluster and small payload (128 bytes), with 24 consumers the measured throughput is around 2.3 M messages / second.","title":"Throughput"},{"location":"technology/kafka-overview/#payload-size","text":"From measurement tests done using Kafka producer performance tool, there is a 1/log(s) curve, where below 10k bytes the performances are correct and then slowly degrade from 3000 msg /s (10k bytes msg) to 65 msg/s (515kb msg). To do performance test the event-streams-sample-producer github provides producer tool in Java, using a group of threads to run in multi cores machine. This project can be dockerized, and deployed in k8s. It uses the Kafka tool named: ProducerPerformance.java in the jar: <dependency> <groupId> org.apache.Kafka </groupId> <artifactId> Kafka-tools </artifactId> </dependency>","title":"Payload size"},{"location":"technology/kafka-overview/#parameter-considerations","text":"There are a lot of factors and parameters that needs to be tuned to improve performance at the brokers threading level ( num.replica.fetchers, num.io.threads, num.network.threads, log.cleaner.threads ) and the pod resources constraints. See configuration documentation .","title":"Parameter considerations"},{"location":"technology/kafka-overview/#openshift-specifics","text":"When exposing the Kafka broker via Routes, the traffic is encrypted with TLS, so client needs to deal with TLS certificates and encryption. Routes are exposed via DNS and HAProxy router. The router will act as middleman between Kafka clients and brokers, adding latency, and it can become bottleneck. The traffic generated by client needs to be sized and in case of the router needs to be scaled up, and even isolate the routing by adding a separate router for the Kafka routes.","title":"Openshift specifics"},{"location":"technology/kafka-overview/#disaster-recovery","text":"With the current implementation it is recommended to have one cluster per data center / availability zone. Consumers and producers are co-located to the brokers cluster. When there are needs to keep some part of the data replicated in both data center, you need to assess what kind of data can be aggregated, and if Kafka mirroring tool can be used. The tool consumes from a source cluster, from a given topic, and produces to a destination cluster with the same named topic. It keeps the message key for partitioning, so order is preserved. The above diagram is using Kafka MirrorMaker with a master to slave deployment. Within the data center 2, the brokers are here to manage the topics and events. When there is no consumer running, nothing happen. Consumers and producers can be started when DC1 fails. This is the active/passive model. In fact, we could have consumers within the DC2 processing topics to manage a read-only model, keeping in memory their projection view, as presented in the CQRS pattern . The second solution is to use one mirror maker in each site, for each topic. This is an active - active topology: consumers and producers are on both sites. But to avoid infinite loop, we need to use naming convention for the topic, or only produce in the cluster of the main topic. Consumers consume from the replicated topic. When you want to deploy solution that spreads over multiple regions to support global streaming, you need to address the following challenges: How do you make data available to applications across multiple data centers? How to serve data closer to the geography? How to be compliant on regulations, like GDPR? How to address no duplication of records? Kafka 2.4 introduces the capability for a consumer to read messages from the closest replica using some rack-id and specific algorithm. This capability will help to extend the cluster to multiple data center and avoid having consumers going over WAN communication.","title":"Disaster Recovery"},{"location":"technology/kafka-overview/#solution-considerations","text":"There are a set of design considerations to assess for each Kafka solution:","title":"Solution Considerations"},{"location":"technology/kafka-overview/#topics_1","text":"Performance is more a function of number of partitions than topics. Expect that each topic has at least one partition. When considering latency you should aim for limiting to hundreds of topic-partition per broker node. What of the most important question is what topics to use?. What is an event type? Should we use one topic to support multiple event types? Let define that an event type is linked to a main business entity like an Order, a ship, a FridgeredContainer. OrderCreated, OrderCancelled, OrderUpdated, OrderClosed are events linked to the states of the Order. The order of those events matter. So the natural approach is to use one topic per data type or schema, specially when using the topic as Event Sourcing where event order is important to build the audit log. You will use a unique partition to support that. The orderID is the partition key and all events related to the order are in the same topic. The important requirement to consider is the sequencing or event order. When event order is very important then use a unique partition, and use the entity unique identifier as key. Ordering is not preserved across partitions. When dealing with entity, independent entities may be in separate topics, when strongly related one may stay together. Other best practices: When event order is important use the same topic and use the entity unique identifier as partition key. When two entities are related together by containment relationship then they can be in the same topic. Different entities are separated to different topics. It is possible to group topics in coarse grained one when we discover that several consumers are listening to the same topics. Clearly define the partition key as it could be an compound key based on multiple entities. With Kafka stream, state store or KTable, you should separate the changelog topic from the others.","title":"Topics"},{"location":"technology/kafka-overview/#producers","text":"When developing a record producer you need to assess the following: What is the expected throughput to send events? Event size * average throughput combined with the expected latency help to compute buffer size. Can the producer batch events together to send them in batch over one send operation? Is there a risk for loosing communication? Tune the RETRIES_CONFIG and buffer size Assess once to exactly once delivery requirement. Look at idempotent producer. See implementation considerations discussion","title":"Producers"},{"location":"technology/kafka-overview/#consumers","text":"From the consumer point of view a set of items need to be addressed during design phase: Do you need to group consumers for parallel consumption of events? What is the processing done once the record is processed out of the topic? And how a record is supposed to be consumed?. How to persist consumer committed position? (the last offset that has been stored securely) Assess if offsets need to be persisted outside of Kafka?. From version 0.9 offset management is more efficient, and synchronous or asynchronous operations can be done from the consumer code. Does record time sensitive, and it is possible that consumers fall behind, so when a consumer restarts he can bypass missed records? Do the consumer needs to perform joins, aggregations between multiple partitions? See implementation considerations discussion","title":"Consumers"},{"location":"technology/kafka-overview/#deployment","text":"In this section we provide the instructions for getting Kafka deployed in your vanilla kubernetes environment through the Strimzi kubernetes operator or getting the IBM Event Streams product (based on Kafka) deployed on your IBM Cloud Private/OpenShift cluster or in your IBM Cloud account as a managed service.","title":"Deployment"},{"location":"technology/kafka-overview/#kubernetes-operator","text":"It is important to note that the deployment and management of stateful application in Kubernetes should, now, use the proposed Operator Framework introduced by Red Hat and Google. One important contribution is the Strimzi Kafka operator that simplifies the deployment of Kafka within k8s by adding a set of operators to deploy and manage Kafka clusters, topics, users and more.","title":"Kubernetes Operator"},{"location":"technology/kafka-overview/#ibm-event-streams","text":"IBM Event Streams is an event-streaming platform based on the open-source Apache Kafka\u00ae project. It can be installed on IBM Cloud Private (ICP) cluster, OpenShift cluster or as a hosted service in IBM Cloud. Instructions for installing IBM Event Streams on your cluster as well as getting an instance as a hosted service in IBM Cloud can be found here","title":"IBM Event Streams"},{"location":"technology/kafka-producers-consumers/","text":"Kafka Producers & Consumers Kafka Producers A producer is a thread safe Kafka client API that publishes records to the cluster. It uses buffers, thread pool, and serializers to send data. They are stateless: the consumers is responsible to manage the offsets of the message they read. When the producer connects via the initial bootstrap connection, it gets the metadata about the topic - partition and the leader broker to connect to. The assignment of messages to partition is done following different algorithms: round-robin if there is no key specified, using the hash code of the key, or custom defined. We recommend reading IBM Event streams producer guidelines to understand how producers work with its configuration parameters. Design considerations When developing a record producer you need to assess the followings: What is the event payload to send? Is is a root aggregate, as defined in domain driven design, with value objects? Does it need to be kept in sequence to be used as event sourcing? or order does not matter? Remember that when order is important, messages need to go to the same topic. When multiple partitions are used, the messages with the same key will go to the same partition to guaranty the order. See related discussions from Martin Kleppmann on confluent web site . Also to be exhaustive, it is possible to get a producer doing retries that could generate duplicate records as acknowledges may take time to come: within a batch of n records, if the producer did not get all the n acknowledges on time, it may resend the batch. This is where 'idempotence' becomes important (see later section). Is there a strong requirement to manage the schema definition? If using one topic to manage all events about a business entity, then be sure to support a flexible avro schema . What is the expected throughput to send events? Event size * average throughput combined with the expected latency help to compute buffer size. By default, the buffer size is set at 32Mb, but can be configured with buffer.memory property. (See producer configuration API Can the producer batches events together to send them in batch over one send operation? By design Kafka producers batch events. Is there a risk for loosing communication? Tune the RETRIES_CONFIG and buffer size, and ensure to have at least 3 or even better 5, brokers within the cluster to maintain quorum in case of one failure. The client API is implemented to support reconnection. When deploying Kafka on Kubernetes, it is important to proxy the broker URLs with a proxy server outside of kubernetes. The HAProxy needs to scale, and as the Kafka traffic may be important, it may make sense to have a dedicated HAProxy for clients to brokers traffic. Assess exactly once delivery requirement. Look at idempotent producer: retries will not introduce duplicate records (see section below). Partitions help to scale the consumer processing of messages, but it also helps the producer to be more efficient as it can send message in parallel to different partition. Where the event timestamp comes from? Should the producer send operation set it or is it loaded from external data? Remember that LogAppendTime is considered to be processing time, and CreateTime is considered to be event time. Typical producer code structure The producer code, using java or python API, does the following steps: define producer properties create a producer instance Connect to the bootstrap URL, get a broker leader send event records and get resulting metadata. Producers are thread safe. The send() operation is asynchronous and returns immediately once record has been stored in the buffer of records, and it is possible to add a callback to process the broker acknowledgements. Here is an example of producer code from the our quick start. Kafka useful Producer APIs Here is a list of common API to use in your producer and consumer code. KafkaProducer A Kafka client that publishes records to the Kafka cluster. The send method is asynchronous. A producer is thread safe so we can have per topic to interface. ProducerRecord to be published to a topic RecordMetadata metadata for a record that has been acknowledged by the server. Properties to consider The following properties are helpful to tune at each topic and producer and will vary depending on the requirements: Properties Description BOOTSTRAP_SERVERS_CONFIG A comma-separated list of host:port values for all the brokers deployed. So producer may use any brokers KEY_SERIALIZER_CLASS_CONFIG and VALUE_SERIALIZER_CLASS_CONFIG convert the keys and values into byte arrays. Using default String serializer should be a good solution for Json payload. For streaming app, use customer serializer. ACKS_CONFIG specifies the minimum number of acknowledgments from a broker that the producer will wait for before considering a record send completed. Values = all, 0, and 1. 0 is for fire and forget. RETRIES_CONFIG specifies the number of times to attempt to resend a batch of events. ENABLE_IDEMPOTENCE_CONFIG Set to true, the number of retries will be maximized, and the acks will be set to All . TRANSACTION_ID A unique identifier for a producer. In case of multiple producer instances, a same ID will mean a second producers can commit the transaction. Epoch number, linked to the process ID, avoid having two producers doing this commit. If no transaction ID is specified, the transaction will be valid within a single session. How to support exactly once delivery Knowing that exactly once delivery is one of the hardest problems to solve in distributed systems, how Kafka does it?. Broker can fail or a network may respond slowly while a producer is trying to send events. Producer can set acknowledge level to control the delivery semantic to ensure not loosing data. The following semantic is supported: At least once : means the producer set ACKS_CONFIG=1 and get an acknowledgement message when the message sent, has been written to at least one time in the cluster (assume replicas = 3). If the ack is not received, the producer may retry, which may generate duplicate records in case the broker stops after saving to the topic and before sending back the acknowledgement message. At most semantic : means the producer will not do retry in case of no acknowledge received. It may create log and compensation, but the message may be lost. Exactly once means even if the producer sends the message twice the system will send only one message to the consumer. Once the consumer commits the read offset, it will not receive the message again, even if it restarts. Consumer offset needs to be in sync with produced event. At the best case scenario, with a replica factor set to 3, a broker responding on time to the producer, and with a consumer committing its offset and reading from the last committed offset it is possible to get only one message end to end. Sometime the brokers will not send acknowledge in expected time, and the producer may decide to send the records again, generating duplicate... To avoid duplicate message at the broker level, when acknowledge is set to ALL, the producer can also set idempotence flag: ENABLE_IDEMPOTENCE_CONFIG = true. With the idempotence property, the record sent, has a sequence number and a producer id, so that the broker keeps the last sequence number per producer and per partition. If a message is received with a lower sequence number, it means a producer is doing some retries on record already processed, so the broker will drop it, to avoid having duplicate records per partition. If the id is greater than current id known by the broker, the broker will create an OutOfSequence exception, which may be fatal as records may have been lost. The sequence number is persisted in a log so even in case of broker leader failure, the new leader will have a good view of the states of the system. The replication mechanism guarantees that, when a message is written to the leader replica, it will be replicated to all available replicas. As soon as you want to get acknowledge of all replicates, it is obvious to set idempotence to true. It does not impact performance. To add to this discussion, as topic may have multiple partitions, idempotent producers do not provide guarantees for writes across multiple Topic-Partition. For that Kafka supports atomic writes to all partitions, so that all records are saved or none of them are visible to consumers. This transaction control is done by using the producer transactional API, and a transactional protocol with coordinator and control message. Here is an example of such configuration that can be done in a producer constructor method: producerProps . put ( \"enable.idempotence\" , \"true\" ); producerProps . put ( \"transactional.id\" , \"prod-1\" ); KafkaProducer . initTransactions () initTransactions() registers the producer with the broker as one that can use transaction, identifying it by its transactional.id and a sequence number, or epoch. Epoch is used to avoid an old producer to commit a transaction while a new producer instance was created for that and continues its work. Kafka streams with consume-process-produce loop requires transaction and exactly once. Even committing its read offset is part of the transaction. So Producer API has a sendOffsetsToTransaction method . See the KIP 98 for details. In case of multiple partitions, the broker will store a list of all updated partitions for a given transaction. To support transaction a transaction coordinator keeps its states into an internal topic (TransactionLog). Control messages are added to the main topic but never exposed to the 'user', so that consumers have the knowledge if a transaction is committed or not. See the code in order command microservice. The consumer is also interested to configure the reading of the transactional messages by defining the isolation level. Consumer waits to read transactional messages until the associated transaction has been committed. Here is an example of consumer code and configuration consumerProps . put ( \"enable.auto.commit\" , \"false\" ); consumerProps . put ( \"isolation.level\" , \"read_committed\" ); With read_committed , no message that was written to the input topic in the same transaction will be read by this consumer until message replicas are all written. In consume-process-produce loop, producer commits its offset with code, and specifies the last offset to read. offsetsToCommit . put ( partition , new OffsetAndMetadata ( offset + 1 )) producer . sendOffsetsToTransaction ( offsetsToCommit , \"consumer-group-id\" ); The producer then commits the transaction. try { KafkaProducer . beginTransaction (); ProducerRecord < String , String > record = new ProducerRecord <>( ApplicationConfig . ORDER_COMMAND_TOPIC , key , value ); Future < RecordMetadata > send = KafkaProducer . send ( record , callBackFunction ); KafkaProducer . commitTransaction (); } catch ( KafkaException e ){ KafkaProducer . abortTransaction (); } There is an interesting article from the Baeldung team about exactly once processing in Kafka with code example which we have re-used to implement the order processing in our Reefer Container Shipment reference application and explained here More readings Creating advanced Kafka producer in java - Cloudurable Confluent blog: Exactly-once Semantics are Possible: Here\u2019s How Kafka Does it Kafka Consumers This note includes some quick summary of different practices we discovered and studied over time. It may be useful for beginner or seasoned developers who want a refresh after some time far away from Kafka... Important concepts Consumers belong to consumer groups . You specify the group name as part of the consumer connection parameters using the group.id configuration: properties . put ( ConsumerConfig . GROUP_ID_CONFIG , groupid ); Consumer groups are grouping consumers to cooperate to consume messages from one or more topics. Consumers can run in separate hosts and separate processes. The figure below represents 2 consumer apps belonging to one consumer group. Consumer 1 is getting data from 2 partitions, while consumer 2 is getting from one partition. When a consumer is unique in a group, it will get data from all partitions. There is always at least one consumer per partition. One broker is responsible to be the consumer group coordinator which is responsible for assigning partitions to the consumers in the group. The first consumer to join the group will be the group leader. It will get the list of consumers and it is responsible for assigning a subset of partitions to each consumer Membership in a consumer group is maintained dynamically. Consumers send hearbeats to the group coordinator broker (see configuration like heartbeat.interval.ms ) and session.timeout.ms . Partition assignement is done by different strategies from range, round robin, sticky and cooperative sticky (See partition assignement strategy ). When a consumer fails, the partitions assigned to it will be reassigned to another consumer in the same group. When a new consumer joins the group, partitions will be moved from existing consumers to the new one. Group rebalancing is also used when new partitions are added to one of the subscribed topics. The group will automatically detect the new partitions through periodic metadata refreshes and assign them to members of the group. During a rebalance, depending of the strategy, consumers may not consume messages (Need Kafka 2.4+ to get cooperative balancing feature). Implementing a Topic consumer is using the Kafka KafkaConsumer class which the API documentation is a must read. It is interesting to note that: To support the same semantic of a queue processing like other integration messaging systems, you need to have all the consumers assigned to a single consumer group, so that each record delivery would be balanced over the group like with a queue. To support pub/sub like other messaging systems, each consumer would have its own consumer group, and subscribes to all the records published to the topic. With client.rack setting a consumer can consume from a local replica, which will have better latency when using a stretched cluster. The implementation is simple for a single thread consumer, and the code structure looks like: prepare the consumer properties create an instance of KafkaConsumer to subscribe to at least one topic loop on polling events: the consumer ensures its liveness with the broker via the poll API. It will get n records per poll. process the ConsumerRecords and commit the offset by code or use the autocommit attribute of the consumer As long as the consumer continues to call poll(), it will stay in the group and continue to receive messages from the partitions it was assigned. When the consumer does not send heartbeats for a duration of session.timeout.ms , then it is considered dead and its partitions will be reassigned. Examples of Java consumers can be found in the order management microservice project under the order-command-ms folder. We are proposing a deep dive study on this manual offset commit in this consumer code that persists events to cassandra. Example of Javascript implementation is in this repository/folder But the complexity comes from the offset management and multithreading needs. So the following important considerations need to be addressed while implementing a consumer. Assess number of consumers needed The KafkaConsumer is not thread safe so it is recommended to run in a unique thread. But if needed you can implement a multi-threads solution, but as each thread will open a TCP connection to the Kafka broker, be sure to close the connection to avoid memory leak. The alternate is to start n processes (JVM process). If you need multiple consumers running in parallel to scale horizontally, you have to define multiple partitions while configuring the topic and use fine-grained control over offset persistence. You\u2019ll use one consumer per partition of a topic. This consumer-per-partition pattern maximizes throughput. When consumers run in parallel and you use multiple threads per consumer you need to be sure the total number of threads across all instances do not exceed the total number of partitions in the topic. Also, a consumer can subscribe to multiple topics. The brokers are doing rebalancing of the assignment of topic-partition to a consumer that belong to a group. When creating a new consumer you can specify the group id in the options. Offset management Recall that offset is just a numeric identifier of a consumer position of the last record read within a partition. Consumers periodically need to commit the offsets they have received, to present a recovery point in case of failure. To commit offset (via API or automatically) the consumer sends a message to Kafka broker to the special topic named __consumer_offsets to keep the committed offset for each partition. Consumers do a read commit for the last processed record: When a consumer starts and is assigned a partition to consume, it will start at its group's committed offset or latest or earliest as auto.offset.reset (When there is a committed offset, the auto.offset.reset property is not used). As shown in the figure below, it is possible to get duplicates if the last message processed by the consumer before crashing and committing its offset, is bigger than the last commited offset. Source: Kafka definitive guide book from Todd Palino, Gwen Shapira In the opposite, if the last committed offset is after the last processed messages and there were multiple messages returned in the poll, then those messages may be lost. It is possible to commit by calling API or by setting some properties at the consumer creation level to enable autocommit offset enable.auto.commit . When doing manual offset commit, there are two types of approaches: offsets\u2014synchronous asynchronous As soon as you are coding manual commit, it is strongly recommended to implement the ConsumerRebalanceListener interface to be able to do the state modifications when the topic is rebalanced. If a consumer fails after processing a message but before committing its offset, the committed offset information will not reflect the processing of the message. This means that the message will be processed again by the next consumer in that group to be assigned the partition. Assess if it is acceptable to loose messages from topic. If so, when a consumer restarts it will start consuming the topic from the latest committed offset within the partition allocated to itself. As storing a message to an external system and storing the offsets are two separate operations, and in case of failure between them, it is possible to have stale offsets, which will introduce duplicate messages when consumers restart to process from last known committed offset. In this case, consumer's idempotence is needed to support updating the same row in the table, or use the event timestamp as update timestamp in the database record or use other clever solution. As presented in the producer coding practice, using transaction to support \"exactly-once\", also means the consumers should read committed data only. This can be achieved by setting the isolation.level=read_committed in the consumer's configuration. The last offset will be the first message in the partition belonging to an open not yet committed transaction. This offset is known as the 'Last Stable Offset'(LSO). Finally in the case where consumers are set to auto commit, it means the offset if committed at the poll() level and if the service crashed while processing of this record as: then the record (partition 0 - offset 4) will never be processed. Producer transaction When consuming from a Kafka topic and producing to another topic, like in Kafka Stream, but also in CQRS implementation, we can use the producer's transaction feature to send the committed offset message and the new records in the second topic in the same transaction. This can be seen as a consume-transform-produce loop pattern so that every input event is processed exactly once. An example of such pattern in done in the order management microservice - command part . Consumer lag The consumer lag for a partition is the difference between the offset of the most recently published message and the consumer's committed offset. If the lag starts to grow, it means the consumer is not able to keep up with the producer's pace. The risk, is that slow consumer may fall behind, and when partition management may remove old log segments, leading the consumer to jump forward to continnue on the next log segment. Consumer may have lost messages. You can use the Kafka-consumer-groups tool to see the consumer lag. Kafka useful Consumer APIs KafkaConsumer a topic consumer which support: transparently handles brokers failure transparently adapt to partition migration within the cluster support grouping for load balancing among consumers maintains TCP connections to the necessary brokers to fetch data subscribe to multiple topics and being part of consumer groups each partition is assigned to exactly one consumer in the group if a process fails, the partitions assigned to it will be reassigned to other consumers in the same group ConsumerRecords holds the list ConsumerRecord per partition for a particular topic. ConsumerRecord A key/value pair to be received from Kafka. This also consists of a topic name and a partition number from which the record is being received, an offset that points to the record in a Kafka partition, and a timestamp References IBM Event Streams - Consuming messages KafkaConsumer class","title":"Producer - Consumer"},{"location":"technology/kafka-producers-consumers/#kafka-producers-consumers","text":"","title":"Kafka Producers &amp; Consumers"},{"location":"technology/kafka-producers-consumers/#kafka-producers","text":"A producer is a thread safe Kafka client API that publishes records to the cluster. It uses buffers, thread pool, and serializers to send data. They are stateless: the consumers is responsible to manage the offsets of the message they read. When the producer connects via the initial bootstrap connection, it gets the metadata about the topic - partition and the leader broker to connect to. The assignment of messages to partition is done following different algorithms: round-robin if there is no key specified, using the hash code of the key, or custom defined. We recommend reading IBM Event streams producer guidelines to understand how producers work with its configuration parameters.","title":"Kafka Producers"},{"location":"technology/kafka-producers-consumers/#design-considerations","text":"When developing a record producer you need to assess the followings: What is the event payload to send? Is is a root aggregate, as defined in domain driven design, with value objects? Does it need to be kept in sequence to be used as event sourcing? or order does not matter? Remember that when order is important, messages need to go to the same topic. When multiple partitions are used, the messages with the same key will go to the same partition to guaranty the order. See related discussions from Martin Kleppmann on confluent web site . Also to be exhaustive, it is possible to get a producer doing retries that could generate duplicate records as acknowledges may take time to come: within a batch of n records, if the producer did not get all the n acknowledges on time, it may resend the batch. This is where 'idempotence' becomes important (see later section). Is there a strong requirement to manage the schema definition? If using one topic to manage all events about a business entity, then be sure to support a flexible avro schema . What is the expected throughput to send events? Event size * average throughput combined with the expected latency help to compute buffer size. By default, the buffer size is set at 32Mb, but can be configured with buffer.memory property. (See producer configuration API Can the producer batches events together to send them in batch over one send operation? By design Kafka producers batch events. Is there a risk for loosing communication? Tune the RETRIES_CONFIG and buffer size, and ensure to have at least 3 or even better 5, brokers within the cluster to maintain quorum in case of one failure. The client API is implemented to support reconnection. When deploying Kafka on Kubernetes, it is important to proxy the broker URLs with a proxy server outside of kubernetes. The HAProxy needs to scale, and as the Kafka traffic may be important, it may make sense to have a dedicated HAProxy for clients to brokers traffic. Assess exactly once delivery requirement. Look at idempotent producer: retries will not introduce duplicate records (see section below). Partitions help to scale the consumer processing of messages, but it also helps the producer to be more efficient as it can send message in parallel to different partition. Where the event timestamp comes from? Should the producer send operation set it or is it loaded from external data? Remember that LogAppendTime is considered to be processing time, and CreateTime is considered to be event time.","title":"Design considerations"},{"location":"technology/kafka-producers-consumers/#typical-producer-code-structure","text":"The producer code, using java or python API, does the following steps: define producer properties create a producer instance Connect to the bootstrap URL, get a broker leader send event records and get resulting metadata. Producers are thread safe. The send() operation is asynchronous and returns immediately once record has been stored in the buffer of records, and it is possible to add a callback to process the broker acknowledgements. Here is an example of producer code from the our quick start.","title":"Typical producer code structure"},{"location":"technology/kafka-producers-consumers/#kafka-useful-producer-apis","text":"Here is a list of common API to use in your producer and consumer code. KafkaProducer A Kafka client that publishes records to the Kafka cluster. The send method is asynchronous. A producer is thread safe so we can have per topic to interface. ProducerRecord to be published to a topic RecordMetadata metadata for a record that has been acknowledged by the server.","title":"Kafka useful Producer APIs"},{"location":"technology/kafka-producers-consumers/#properties-to-consider","text":"The following properties are helpful to tune at each topic and producer and will vary depending on the requirements: Properties Description BOOTSTRAP_SERVERS_CONFIG A comma-separated list of host:port values for all the brokers deployed. So producer may use any brokers KEY_SERIALIZER_CLASS_CONFIG and VALUE_SERIALIZER_CLASS_CONFIG convert the keys and values into byte arrays. Using default String serializer should be a good solution for Json payload. For streaming app, use customer serializer. ACKS_CONFIG specifies the minimum number of acknowledgments from a broker that the producer will wait for before considering a record send completed. Values = all, 0, and 1. 0 is for fire and forget. RETRIES_CONFIG specifies the number of times to attempt to resend a batch of events. ENABLE_IDEMPOTENCE_CONFIG Set to true, the number of retries will be maximized, and the acks will be set to All . TRANSACTION_ID A unique identifier for a producer. In case of multiple producer instances, a same ID will mean a second producers can commit the transaction. Epoch number, linked to the process ID, avoid having two producers doing this commit. If no transaction ID is specified, the transaction will be valid within a single session.","title":"Properties to consider"},{"location":"technology/kafka-producers-consumers/#how-to-support-exactly-once-delivery","text":"Knowing that exactly once delivery is one of the hardest problems to solve in distributed systems, how Kafka does it?. Broker can fail or a network may respond slowly while a producer is trying to send events. Producer can set acknowledge level to control the delivery semantic to ensure not loosing data. The following semantic is supported: At least once : means the producer set ACKS_CONFIG=1 and get an acknowledgement message when the message sent, has been written to at least one time in the cluster (assume replicas = 3). If the ack is not received, the producer may retry, which may generate duplicate records in case the broker stops after saving to the topic and before sending back the acknowledgement message. At most semantic : means the producer will not do retry in case of no acknowledge received. It may create log and compensation, but the message may be lost. Exactly once means even if the producer sends the message twice the system will send only one message to the consumer. Once the consumer commits the read offset, it will not receive the message again, even if it restarts. Consumer offset needs to be in sync with produced event. At the best case scenario, with a replica factor set to 3, a broker responding on time to the producer, and with a consumer committing its offset and reading from the last committed offset it is possible to get only one message end to end. Sometime the brokers will not send acknowledge in expected time, and the producer may decide to send the records again, generating duplicate... To avoid duplicate message at the broker level, when acknowledge is set to ALL, the producer can also set idempotence flag: ENABLE_IDEMPOTENCE_CONFIG = true. With the idempotence property, the record sent, has a sequence number and a producer id, so that the broker keeps the last sequence number per producer and per partition. If a message is received with a lower sequence number, it means a producer is doing some retries on record already processed, so the broker will drop it, to avoid having duplicate records per partition. If the id is greater than current id known by the broker, the broker will create an OutOfSequence exception, which may be fatal as records may have been lost. The sequence number is persisted in a log so even in case of broker leader failure, the new leader will have a good view of the states of the system. The replication mechanism guarantees that, when a message is written to the leader replica, it will be replicated to all available replicas. As soon as you want to get acknowledge of all replicates, it is obvious to set idempotence to true. It does not impact performance. To add to this discussion, as topic may have multiple partitions, idempotent producers do not provide guarantees for writes across multiple Topic-Partition. For that Kafka supports atomic writes to all partitions, so that all records are saved or none of them are visible to consumers. This transaction control is done by using the producer transactional API, and a transactional protocol with coordinator and control message. Here is an example of such configuration that can be done in a producer constructor method: producerProps . put ( \"enable.idempotence\" , \"true\" ); producerProps . put ( \"transactional.id\" , \"prod-1\" ); KafkaProducer . initTransactions () initTransactions() registers the producer with the broker as one that can use transaction, identifying it by its transactional.id and a sequence number, or epoch. Epoch is used to avoid an old producer to commit a transaction while a new producer instance was created for that and continues its work. Kafka streams with consume-process-produce loop requires transaction and exactly once. Even committing its read offset is part of the transaction. So Producer API has a sendOffsetsToTransaction method . See the KIP 98 for details. In case of multiple partitions, the broker will store a list of all updated partitions for a given transaction. To support transaction a transaction coordinator keeps its states into an internal topic (TransactionLog). Control messages are added to the main topic but never exposed to the 'user', so that consumers have the knowledge if a transaction is committed or not. See the code in order command microservice. The consumer is also interested to configure the reading of the transactional messages by defining the isolation level. Consumer waits to read transactional messages until the associated transaction has been committed. Here is an example of consumer code and configuration consumerProps . put ( \"enable.auto.commit\" , \"false\" ); consumerProps . put ( \"isolation.level\" , \"read_committed\" ); With read_committed , no message that was written to the input topic in the same transaction will be read by this consumer until message replicas are all written. In consume-process-produce loop, producer commits its offset with code, and specifies the last offset to read. offsetsToCommit . put ( partition , new OffsetAndMetadata ( offset + 1 )) producer . sendOffsetsToTransaction ( offsetsToCommit , \"consumer-group-id\" ); The producer then commits the transaction. try { KafkaProducer . beginTransaction (); ProducerRecord < String , String > record = new ProducerRecord <>( ApplicationConfig . ORDER_COMMAND_TOPIC , key , value ); Future < RecordMetadata > send = KafkaProducer . send ( record , callBackFunction ); KafkaProducer . commitTransaction (); } catch ( KafkaException e ){ KafkaProducer . abortTransaction (); } There is an interesting article from the Baeldung team about exactly once processing in Kafka with code example which we have re-used to implement the order processing in our Reefer Container Shipment reference application and explained here","title":"How to support exactly once delivery"},{"location":"technology/kafka-producers-consumers/#more-readings","text":"Creating advanced Kafka producer in java - Cloudurable Confluent blog: Exactly-once Semantics are Possible: Here\u2019s How Kafka Does it","title":"More readings"},{"location":"technology/kafka-producers-consumers/#kafka-consumers","text":"This note includes some quick summary of different practices we discovered and studied over time. It may be useful for beginner or seasoned developers who want a refresh after some time far away from Kafka...","title":"Kafka Consumers"},{"location":"technology/kafka-producers-consumers/#important-concepts","text":"Consumers belong to consumer groups . You specify the group name as part of the consumer connection parameters using the group.id configuration: properties . put ( ConsumerConfig . GROUP_ID_CONFIG , groupid ); Consumer groups are grouping consumers to cooperate to consume messages from one or more topics. Consumers can run in separate hosts and separate processes. The figure below represents 2 consumer apps belonging to one consumer group. Consumer 1 is getting data from 2 partitions, while consumer 2 is getting from one partition. When a consumer is unique in a group, it will get data from all partitions. There is always at least one consumer per partition. One broker is responsible to be the consumer group coordinator which is responsible for assigning partitions to the consumers in the group. The first consumer to join the group will be the group leader. It will get the list of consumers and it is responsible for assigning a subset of partitions to each consumer Membership in a consumer group is maintained dynamically. Consumers send hearbeats to the group coordinator broker (see configuration like heartbeat.interval.ms ) and session.timeout.ms . Partition assignement is done by different strategies from range, round robin, sticky and cooperative sticky (See partition assignement strategy ). When a consumer fails, the partitions assigned to it will be reassigned to another consumer in the same group. When a new consumer joins the group, partitions will be moved from existing consumers to the new one. Group rebalancing is also used when new partitions are added to one of the subscribed topics. The group will automatically detect the new partitions through periodic metadata refreshes and assign them to members of the group. During a rebalance, depending of the strategy, consumers may not consume messages (Need Kafka 2.4+ to get cooperative balancing feature). Implementing a Topic consumer is using the Kafka KafkaConsumer class which the API documentation is a must read. It is interesting to note that: To support the same semantic of a queue processing like other integration messaging systems, you need to have all the consumers assigned to a single consumer group, so that each record delivery would be balanced over the group like with a queue. To support pub/sub like other messaging systems, each consumer would have its own consumer group, and subscribes to all the records published to the topic. With client.rack setting a consumer can consume from a local replica, which will have better latency when using a stretched cluster. The implementation is simple for a single thread consumer, and the code structure looks like: prepare the consumer properties create an instance of KafkaConsumer to subscribe to at least one topic loop on polling events: the consumer ensures its liveness with the broker via the poll API. It will get n records per poll. process the ConsumerRecords and commit the offset by code or use the autocommit attribute of the consumer As long as the consumer continues to call poll(), it will stay in the group and continue to receive messages from the partitions it was assigned. When the consumer does not send heartbeats for a duration of session.timeout.ms , then it is considered dead and its partitions will be reassigned. Examples of Java consumers can be found in the order management microservice project under the order-command-ms folder. We are proposing a deep dive study on this manual offset commit in this consumer code that persists events to cassandra. Example of Javascript implementation is in this repository/folder But the complexity comes from the offset management and multithreading needs. So the following important considerations need to be addressed while implementing a consumer.","title":"Important concepts"},{"location":"technology/kafka-producers-consumers/#assess-number-of-consumers-needed","text":"The KafkaConsumer is not thread safe so it is recommended to run in a unique thread. But if needed you can implement a multi-threads solution, but as each thread will open a TCP connection to the Kafka broker, be sure to close the connection to avoid memory leak. The alternate is to start n processes (JVM process). If you need multiple consumers running in parallel to scale horizontally, you have to define multiple partitions while configuring the topic and use fine-grained control over offset persistence. You\u2019ll use one consumer per partition of a topic. This consumer-per-partition pattern maximizes throughput. When consumers run in parallel and you use multiple threads per consumer you need to be sure the total number of threads across all instances do not exceed the total number of partitions in the topic. Also, a consumer can subscribe to multiple topics. The brokers are doing rebalancing of the assignment of topic-partition to a consumer that belong to a group. When creating a new consumer you can specify the group id in the options.","title":"Assess number of consumers needed"},{"location":"technology/kafka-producers-consumers/#offset-management","text":"Recall that offset is just a numeric identifier of a consumer position of the last record read within a partition. Consumers periodically need to commit the offsets they have received, to present a recovery point in case of failure. To commit offset (via API or automatically) the consumer sends a message to Kafka broker to the special topic named __consumer_offsets to keep the committed offset for each partition. Consumers do a read commit for the last processed record: When a consumer starts and is assigned a partition to consume, it will start at its group's committed offset or latest or earliest as auto.offset.reset (When there is a committed offset, the auto.offset.reset property is not used). As shown in the figure below, it is possible to get duplicates if the last message processed by the consumer before crashing and committing its offset, is bigger than the last commited offset. Source: Kafka definitive guide book from Todd Palino, Gwen Shapira In the opposite, if the last committed offset is after the last processed messages and there were multiple messages returned in the poll, then those messages may be lost. It is possible to commit by calling API or by setting some properties at the consumer creation level to enable autocommit offset enable.auto.commit . When doing manual offset commit, there are two types of approaches: offsets\u2014synchronous asynchronous As soon as you are coding manual commit, it is strongly recommended to implement the ConsumerRebalanceListener interface to be able to do the state modifications when the topic is rebalanced. If a consumer fails after processing a message but before committing its offset, the committed offset information will not reflect the processing of the message. This means that the message will be processed again by the next consumer in that group to be assigned the partition. Assess if it is acceptable to loose messages from topic. If so, when a consumer restarts it will start consuming the topic from the latest committed offset within the partition allocated to itself. As storing a message to an external system and storing the offsets are two separate operations, and in case of failure between them, it is possible to have stale offsets, which will introduce duplicate messages when consumers restart to process from last known committed offset. In this case, consumer's idempotence is needed to support updating the same row in the table, or use the event timestamp as update timestamp in the database record or use other clever solution. As presented in the producer coding practice, using transaction to support \"exactly-once\", also means the consumers should read committed data only. This can be achieved by setting the isolation.level=read_committed in the consumer's configuration. The last offset will be the first message in the partition belonging to an open not yet committed transaction. This offset is known as the 'Last Stable Offset'(LSO). Finally in the case where consumers are set to auto commit, it means the offset if committed at the poll() level and if the service crashed while processing of this record as: then the record (partition 0 - offset 4) will never be processed.","title":"Offset management"},{"location":"technology/kafka-producers-consumers/#producer-transaction","text":"When consuming from a Kafka topic and producing to another topic, like in Kafka Stream, but also in CQRS implementation, we can use the producer's transaction feature to send the committed offset message and the new records in the second topic in the same transaction. This can be seen as a consume-transform-produce loop pattern so that every input event is processed exactly once. An example of such pattern in done in the order management microservice - command part .","title":"Producer transaction"},{"location":"technology/kafka-producers-consumers/#consumer-lag","text":"The consumer lag for a partition is the difference between the offset of the most recently published message and the consumer's committed offset. If the lag starts to grow, it means the consumer is not able to keep up with the producer's pace. The risk, is that slow consumer may fall behind, and when partition management may remove old log segments, leading the consumer to jump forward to continnue on the next log segment. Consumer may have lost messages. You can use the Kafka-consumer-groups tool to see the consumer lag.","title":"Consumer lag"},{"location":"technology/kafka-producers-consumers/#kafka-useful-consumer-apis","text":"KafkaConsumer a topic consumer which support: transparently handles brokers failure transparently adapt to partition migration within the cluster support grouping for load balancing among consumers maintains TCP connections to the necessary brokers to fetch data subscribe to multiple topics and being part of consumer groups each partition is assigned to exactly one consumer in the group if a process fails, the partitions assigned to it will be reassigned to other consumers in the same group ConsumerRecords holds the list ConsumerRecord per partition for a particular topic. ConsumerRecord A key/value pair to be received from Kafka. This also consists of a topic name and a partition number from which the record is being received, an offset that points to the record in a Kafka partition, and a timestamp","title":"Kafka useful Consumer APIs"},{"location":"technology/kafka-producers-consumers/#references","text":"IBM Event Streams - Consuming messages KafkaConsumer class","title":"References"}]}