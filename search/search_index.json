{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Travelport Travelport Worldwide Ltd, headquartered in Langley, Slough, UK, provides distribution, technology, and payment solutions for the travel and tourism industry. It is one of the top three global distribution systems after Amadeus IT Group and Sabre Corporation. Travelport Event-Driven Microservice practices Welcome to this site! This site shares some best practices for adopting a cloud native, event-driven microservice implementation using Kafka, CI/CD pipeline, Spring Boot Cloud Stream, and API Management. Travelport MVP The Travelport MVP is a simple proof of technology running on the IBM Cloud that presents the following components: In essence, Travelport wanted to develop the basic skeleton of an API that will allow for shopping trips through two or more different vendors, comparing features/prices through normalized content, and creating a train booking. Through this API, their Rail developers should receive normalized content from the train consolidators, so that they don\u2019t have to handle vendors\u2019 content differences. The API needs to support incremental change and be backward compatible. The main purpose of this initial MVP was to serve as a template for best practices around applied Domain Driven Design (DDD), and the use of Kafka. The MVP will allow Travelport to utilize patterns, observe best practices and understand the capabilities of the future toolstack. Key Dates 14 th May (API Management Demo #2) 7 th May (DevOps pipeline/Argo CD. Non-breaking changes) 30 th April (Addition of SNCF alongside live Trainline API content. Mongo DB storage of reservation) 27 th April (API Management Demo #1) 15 th April (Demo of Event Driven Architecture with Domain and Atomic Services for Trainline API) 6 th April (Initial Event Driven Architecture with Domain and Atomic Services demonstrated through OpenShift) 12 th March (Inception meeting, User Story definition begins, Infrastructure build begins). MVP Toolstack & Principles For the MVP toolstack, Travelport dictated the following principles: * The selection will utilize open source where possible. * Where a licensed product is used it should be for quantified reasons considering the full lifecycle / Scale costs. * Any product selected must be industry best practice and market dominant to ensure portability / future proof / and skills accessibility. * Products must consider the entire lifecycle including deployment, support, monitoring, visualization, infrastructure etc. * Leading edge not bleeding edge. * The entire framework solution must have the least possible integration and customization possible. Repositories for the MVP We created the following repositories in GitHub for this MVP: Rail Query Shop atomic Trainline atomic transforms the Rail catalog request to Trainline request and vice versa Code you can access too EDA Reference Implementation with polyglot microservices, CI/CD, SAGA, SQRS IBM active - open source EDA content with labs, and technology practices A repository to get started with more than hello world on EDA implementation Implementation of the Outbox pattern with Debezium in Quarkus MQ source to Kafka Lab with Strimzi or Confluent Kafka Connector Sink lab for Mongodb Most of those repositories are work in progress. You can ask for improvement via git issue, by opening a PR. For additional details, you can navigate through the different topics in this site or search for a particular topic of interest. Enjoy!","title":"Home"},{"location":"index.html#travelport","text":"Travelport Worldwide Ltd, headquartered in Langley, Slough, UK, provides distribution, technology, and payment solutions for the travel and tourism industry. It is one of the top three global distribution systems after Amadeus IT Group and Sabre Corporation.","title":"Travelport"},{"location":"index.html#travelport-event-driven-microservice-practices","text":"Welcome to this site! This site shares some best practices for adopting a cloud native, event-driven microservice implementation using Kafka, CI/CD pipeline, Spring Boot Cloud Stream, and API Management.","title":"Travelport Event-Driven Microservice practices"},{"location":"index.html#travelport-mvp","text":"The Travelport MVP is a simple proof of technology running on the IBM Cloud that presents the following components: In essence, Travelport wanted to develop the basic skeleton of an API that will allow for shopping trips through two or more different vendors, comparing features/prices through normalized content, and creating a train booking. Through this API, their Rail developers should receive normalized content from the train consolidators, so that they don\u2019t have to handle vendors\u2019 content differences. The API needs to support incremental change and be backward compatible. The main purpose of this initial MVP was to serve as a template for best practices around applied Domain Driven Design (DDD), and the use of Kafka. The MVP will allow Travelport to utilize patterns, observe best practices and understand the capabilities of the future toolstack.","title":"Travelport MVP"},{"location":"index.html#key-dates","text":"14 th May (API Management Demo #2) 7 th May (DevOps pipeline/Argo CD. Non-breaking changes) 30 th April (Addition of SNCF alongside live Trainline API content. Mongo DB storage of reservation) 27 th April (API Management Demo #1) 15 th April (Demo of Event Driven Architecture with Domain and Atomic Services for Trainline API) 6 th April (Initial Event Driven Architecture with Domain and Atomic Services demonstrated through OpenShift) 12 th March (Inception meeting, User Story definition begins, Infrastructure build begins).","title":"Key Dates"},{"location":"index.html#mvp-toolstack-principles","text":"For the MVP toolstack, Travelport dictated the following principles: * The selection will utilize open source where possible. * Where a licensed product is used it should be for quantified reasons considering the full lifecycle / Scale costs. * Any product selected must be industry best practice and market dominant to ensure portability / future proof / and skills accessibility. * Products must consider the entire lifecycle including deployment, support, monitoring, visualization, infrastructure etc. * Leading edge not bleeding edge. * The entire framework solution must have the least possible integration and customization possible.","title":"MVP Toolstack &amp; Principles"},{"location":"index.html#repositories-for-the-mvp","text":"We created the following repositories in GitHub for this MVP: Rail Query Shop atomic Trainline atomic transforms the Rail catalog request to Trainline request and vice versa","title":"Repositories for the MVP"},{"location":"index.html#code-you-can-access-too","text":"EDA Reference Implementation with polyglot microservices, CI/CD, SAGA, SQRS IBM active - open source EDA content with labs, and technology practices A repository to get started with more than hello world on EDA implementation Implementation of the Outbox pattern with Debezium in Quarkus MQ source to Kafka Lab with Strimzi or Confluent Kafka Connector Sink lab for Mongodb Most of those repositories are work in progress. You can ask for improvement via git issue, by opening a PR. For additional details, you can navigate through the different topics in this site or search for a particular topic of interest. Enjoy!","title":"Code you can access too"},{"location":"concepts/fit-for-purpose/index.html","text":"Fit for purpose In this note we want to list some of the criteria to consider and assess during an event-driven architecture establishment work and as part of a continuous application governance. This is not fully exhaustive, but provides a good foundation for analysis and study. Fit for purpose practices should be done under a bigger program about application development governance and data governance. We can look at least to the following major subjects: Cloud native applications With the adoption of cloud native and microservice applications (12 factors app), the following needs to be addressed: Responsiveness with elastic scaling and resilience to failure, which leads to adopting the ' reactive manifesto' and consider messaging as a way to communicate between apps. Elastic may also lead to multi cloud deployment practices. Address data sharing using a push model to improve decoupling, and performance. Instead of having each service use a REST endpoint to pull data from other services, each service can push the change through their main business entity to an event backbone, and future services needing that data can then pull it from the messaging system. Adopting common patterns like Command Query Responsibility Segregation (CQRS) to help implement complex queries, joining different business entities owned by different microservices, event sourcing , transactional outbox and SAGA . Addressing eventual data consistency to propagate change to other components versus ACID transactions. Support always-on approach with the deployment to multiple data centers (at least three) being active/active and being able to propagate data in all data centers. So the net: do we need to implement event-driven microservices because of those needs? Modern data pipeline As new business applications need to react to events in real time, the adoption of an event backbone is really part of the IT toolbox. Some existing deployments consider this to be their new data hub, where all the data about the 'customer' is accessible. Therefore, it is natural to assess the data movement strategy and offload some of those ETL jobs running at night, as most of those works are done already inside of the applications generating those data, but not those data are visible inside the backbone. We detailed the new architecture in this modern data lake discussion, so from a fit for purpose point of view, we need to assess what those ETL jobs were doing and how much of those data is now visible to other to consume. With Event Backbone like Kafka, any consumer can join the consumption at any point of time, within the retention period. So if new data is kept like 10 days, within those 10 days a consumer can continuously get the data, no more wait for the next morning, just connected to the topic you need to. MQ Versus Kafka Consider queue system. like IBM MQ, for: Exactly once delivery, and to participate into two phase commit transaction Asynchronous request / reply communication: the semantic of the communication is for one component to ask a second command to do something on its data. This is a command pattern with delay on the response. Recall messages in queue are kept until consumer(s) got them. Consider Kafka as pub/sub and persistence system for: Publish events as immutable facts of what happen in an application Get continuous visibility of the data Streams Keep data once consumed, for future consumers, for replay-ability Scale horizontally the message consumption Direct product feature comparison Kafka IBM MQ Kafka is a pub/sub engine with streams and connectors MQ is a queue, pub/sub engine with file transfer, MQTT, AMQP and other capabilities All topics are persistent Queues and topics can be persistent or non persistent All subscribers are durable Subscribers can be durable or non durable Adding brokers to requires little work (changing a configuration file) Adding QMGRs requires some work (Add the QMGRs to the cluster, add cluster channels. Queues and Topics need to be added to the cluster.) Topics can be spread across brokers (partitions) with a command Queues and topics can be spread across a cluster by adding them to clustered QMGRs Producers and Consumers are aware of changes made to the cluster All MQ clients require a CCDT file to know of changes if not using a gateway QMGR Can have n number of replication partitions Can have 2 replicas (RDQM) of a QMGR, Multi Instance QMGRs Simple load balancing Load balancing can be simple or more complex using weights and affinity Can reread messages Cannot reread messages that have been already processed All clients connect using a single connection method MQ has Channels which allow different clients to connect, each having the ability to have different security requirements Data Streams processing built in, using Kafka topic for efficiency Stream processing is not built in, but using third party libraries, like MicroProfile Reactive Messaging, ReactiveX, etc. Has connection security, authentication security, and ACLs (read/write to Topic) Has connection security, channel security, authentication security, message security/encryption, ACLs for each Object, third party plugins (Channel Exits) Built on Java, so can run on any platform that support Java 8+ Latest native on AIX, IBM i, Linux systems, Solaris, Windows, z/OS, run as Container Monitoring by using statistics provided by Kafka CLI, open source tools, Prometheus Monitoring using PCF API, MQ Explorer, MQ CLI (runmqsc), Third Party Tools (Tivoli, CA APM, Help Systems, Open Source, etc) Events and Messages There is a long history of messaging in IT systems. You can easily see an event driven solution and events in the context of messaging systems and messages. However, there are different characteristics that are worth considering: Messaging: Messages transport a payload and messages are persisted until consumed. Message consumers are typically directly targeted and related to the producer who cares that the message has been delivered and processed. Events: Events are persisted as a replay-able stream history. Event consumers are not tied to the producer. An event is a record of something that has happened and so can't be changed. (You can't change history.) Messaging versus event streaming We recommend reading this article and this one , to get insight on messaging (focusing on operations / actions to be performed by a system or service) versus events (focusing on the state / facts of a system with no knowledge of the downstream processing). To summarize messaging (like MQ) are to support: Transient Data: data is only stored until a consumer has processed the message, or it expires. Request / reply most of the time. Targeted reliable delivery: targeted to the entity that will process the request or receive the response. Reliable with transaction support. Highly Coupled producers and consumers For events: Stream History: consumers are interested in historic events, not just the most recent. Scalable Consumption: A single event is consumed by many consumers with limited impact as the number of consumers grow. Immutable Data Loosely coupled / decoupled producers and consumers","title":"Fit for purpose"},{"location":"concepts/fit-for-purpose/index.html#fit-for-purpose","text":"In this note we want to list some of the criteria to consider and assess during an event-driven architecture establishment work and as part of a continuous application governance. This is not fully exhaustive, but provides a good foundation for analysis and study. Fit for purpose practices should be done under a bigger program about application development governance and data governance. We can look at least to the following major subjects:","title":"Fit for purpose"},{"location":"concepts/fit-for-purpose/index.html#cloud-native-applications","text":"With the adoption of cloud native and microservice applications (12 factors app), the following needs to be addressed: Responsiveness with elastic scaling and resilience to failure, which leads to adopting the ' reactive manifesto' and consider messaging as a way to communicate between apps. Elastic may also lead to multi cloud deployment practices. Address data sharing using a push model to improve decoupling, and performance. Instead of having each service use a REST endpoint to pull data from other services, each service can push the change through their main business entity to an event backbone, and future services needing that data can then pull it from the messaging system. Adopting common patterns like Command Query Responsibility Segregation (CQRS) to help implement complex queries, joining different business entities owned by different microservices, event sourcing , transactional outbox and SAGA . Addressing eventual data consistency to propagate change to other components versus ACID transactions. Support always-on approach with the deployment to multiple data centers (at least three) being active/active and being able to propagate data in all data centers. So the net: do we need to implement event-driven microservices because of those needs?","title":"Cloud native applications"},{"location":"concepts/fit-for-purpose/index.html#modern-data-pipeline","text":"As new business applications need to react to events in real time, the adoption of an event backbone is really part of the IT toolbox. Some existing deployments consider this to be their new data hub, where all the data about the 'customer' is accessible. Therefore, it is natural to assess the data movement strategy and offload some of those ETL jobs running at night, as most of those works are done already inside of the applications generating those data, but not those data are visible inside the backbone. We detailed the new architecture in this modern data lake discussion, so from a fit for purpose point of view, we need to assess what those ETL jobs were doing and how much of those data is now visible to other to consume. With Event Backbone like Kafka, any consumer can join the consumption at any point of time, within the retention period. So if new data is kept like 10 days, within those 10 days a consumer can continuously get the data, no more wait for the next morning, just connected to the topic you need to.","title":"Modern data pipeline"},{"location":"concepts/fit-for-purpose/index.html#mq-versus-kafka","text":"Consider queue system. like IBM MQ, for: Exactly once delivery, and to participate into two phase commit transaction Asynchronous request / reply communication: the semantic of the communication is for one component to ask a second command to do something on its data. This is a command pattern with delay on the response. Recall messages in queue are kept until consumer(s) got them. Consider Kafka as pub/sub and persistence system for: Publish events as immutable facts of what happen in an application Get continuous visibility of the data Streams Keep data once consumed, for future consumers, for replay-ability Scale horizontally the message consumption","title":"MQ Versus Kafka"},{"location":"concepts/fit-for-purpose/index.html#direct-product-feature-comparison","text":"Kafka IBM MQ Kafka is a pub/sub engine with streams and connectors MQ is a queue, pub/sub engine with file transfer, MQTT, AMQP and other capabilities All topics are persistent Queues and topics can be persistent or non persistent All subscribers are durable Subscribers can be durable or non durable Adding brokers to requires little work (changing a configuration file) Adding QMGRs requires some work (Add the QMGRs to the cluster, add cluster channels. Queues and Topics need to be added to the cluster.) Topics can be spread across brokers (partitions) with a command Queues and topics can be spread across a cluster by adding them to clustered QMGRs Producers and Consumers are aware of changes made to the cluster All MQ clients require a CCDT file to know of changes if not using a gateway QMGR Can have n number of replication partitions Can have 2 replicas (RDQM) of a QMGR, Multi Instance QMGRs Simple load balancing Load balancing can be simple or more complex using weights and affinity Can reread messages Cannot reread messages that have been already processed All clients connect using a single connection method MQ has Channels which allow different clients to connect, each having the ability to have different security requirements Data Streams processing built in, using Kafka topic for efficiency Stream processing is not built in, but using third party libraries, like MicroProfile Reactive Messaging, ReactiveX, etc. Has connection security, authentication security, and ACLs (read/write to Topic) Has connection security, channel security, authentication security, message security/encryption, ACLs for each Object, third party plugins (Channel Exits) Built on Java, so can run on any platform that support Java 8+ Latest native on AIX, IBM i, Linux systems, Solaris, Windows, z/OS, run as Container Monitoring by using statistics provided by Kafka CLI, open source tools, Prometheus Monitoring using PCF API, MQ Explorer, MQ CLI (runmqsc), Third Party Tools (Tivoli, CA APM, Help Systems, Open Source, etc)","title":"Direct product feature comparison"},{"location":"concepts/fit-for-purpose/index.html#events-and-messages","text":"There is a long history of messaging in IT systems. You can easily see an event driven solution and events in the context of messaging systems and messages. However, there are different characteristics that are worth considering: Messaging: Messages transport a payload and messages are persisted until consumed. Message consumers are typically directly targeted and related to the producer who cares that the message has been delivered and processed. Events: Events are persisted as a replay-able stream history. Event consumers are not tied to the producer. An event is a record of something that has happened and so can't be changed. (You can't change history.)","title":"Events and Messages"},{"location":"concepts/fit-for-purpose/index.html#messaging-versus-event-streaming","text":"We recommend reading this article and this one , to get insight on messaging (focusing on operations / actions to be performed by a system or service) versus events (focusing on the state / facts of a system with no knowledge of the downstream processing). To summarize messaging (like MQ) are to support: Transient Data: data is only stored until a consumer has processed the message, or it expires. Request / reply most of the time. Targeted reliable delivery: targeted to the entity that will process the request or receive the response. Reliable with transaction support. Highly Coupled producers and consumers For events: Stream History: consumers are interested in historic events, not just the most recent. Scalable Consumption: A single event is consumed by many consumers with limited impact as the number of consumers grow. Immutable Data Loosely coupled / decoupled producers and consumers","title":"Messaging versus event streaming"},{"location":"concepts/terms-and-definitions/index.html","text":"Terms & Definitions Events Events are notifications of change of state. Notifications are issued, or published and interested parties can subscribe and take action on the events. Typically, the issuer of the notification has no knowledge of what action is taken and receives no corresponding feedback that the notification has been processed. Events are notifications of change of state. Typically, events represent the change of state of something of interest to the business. Events are records of something that has happened. Events can't be changed, that is, they are immutable. (We can't change something that has happened in the past). Event streams An event stream is a continuous unbounded series of events. The start of the stream may have occurred before we started to process the stream. The end of the stream is at some unknown point in the future. Events are ordered by the point in time at which each event occurred. When developing event driven solutions, you will typically see two types of event streams: Event streams whose events are defined and published into a stream as part of a solution. Event streams that connect to a real-time event stream, for example from an IOT device, a voice stream from a telephone system, a video stream, or ship or plane locations from global positioning systems. Event backbone The event backbone is the communication layer in the event driven architecture. It provides the connection between event driven capabilities and in the Cloud Native , it becomes the Pub/Sub communication layer for event driven microservices. At this high level we would consider two types of relevant technologies for the event backbone, Message Brokers and Event Logs . Both technology types could be used to achieve the event communication style, with the \"Publish and subscribe\" model however, it is also important to consider other capabilities which are frequently used within event driven solutions: Keeping an Event Log as a time sequenced as it happened recording of events (Source of the truth). Enabling direct replay of events. Enabling Event Sourcing as a way of recording state changes in distributed systems. Enabling programmatic access to the continuous event stream . When viewed across these wider event driven capabilities, an event log style technology can provide a central component which can support all of these capabilities, whereas a message broker would have to be extended with other components. Selecting the Event Backbone for the reference architecture For the event-driven architecture, we defined the following characteristics to be essential for the event backbone: Publish-subscribe event communication between event producers and consumers Facilitate many consumers with shared central \u201csource of truth\u201d. Capability to store events for a given period of time (event log). This is the shared source of the truth for events. Ability for consumers to subscribe to events. Provide replay of events from history for evolving application instances. Provide programmatic access to continuous stream of events, with minimum time lag. Must be highly scalable and resilient to cloud deployment levels. While choosing an event backbone for your event-driven application development, the following functional and non-functional requirements should be taken into consideration: Persistence When source systems generate events, the consumers of those are interested in those events may not be online or available at the same time. So you need a way to store these messages for a configurable period of time until they are consumed and acted upon. Event backbone should be able to provide such event persistence. Observability At times, you need an overall view of how events are ingested by source systems and getting processed by consumers. It could be a management console where events can be observed. Event backbone should provide such observability. Fault tolerance Event backbone could be made of several components. If one of them becomes unavailable, there should not be any impact on the event processors dependent on the backbone. Event backbone needs to provide this resiliency. High availability Event backbone provides persistence of messages/events. If one of the components of the backbone becomes unavailable, there should not be any impact on the availability of these messages/events. Event backbone should be highly available. Performance Event backbone should provide means of accelerating the event processing operations (e.g. parallelising event processing) thereby providing enhanced performance. Delivery guarantees Event backbone should support guaranteed delivery both for producer and consumer. It should support the delivery guarantee options of at least once , at most once , and exactly once . Security The data residing in the event backbone should be secured, at rest as well as in transit. Only authenticated and authorized users should be able to publish and consume messages from the backbone. Topic specific authorizations will also help blocking access by unauthorized consumers. Event backbone should provide these security measures. Stateful operations for events streams Sometimes, source systems generate a continuous flow of 'inter-related' events (e.g. IoT sensors sending data every second). In order to process such messages correctly, the event backbone needs to support for stateful operations like windowing, joins, aggregations. and any type of real time analytics. Event routing options In EDA, event consumers may not be online at all times. So, it should be easier for consumers to subscribe to a topic when it comes online. On-failure hooks Event backbone can support pre-configured actions/behaviors for certain messages. E.g. if a consumer fails to process a message more than a certain number of times, that message can be sent to another topic for re-trying the processing action. Looking across these capabilities, the potential technologies, the amount of adoption and community activity around the technologies, and the considerations listed above, we selected Apache Kafka as our event backbone for the event-driven reference architecture. Event sources When you consider an event-driven architecture, think about event producers and event consumers as the interaction points with events. As you develop event-driven applications following a microservices architecture, the microservices you develop play the role of both event producers and event consumers, with the events being passed as the communication payload between them. However, as you look at the wider opportunities that being event driven offers, you need to widen your view and consider event sources that come from beyond the application code you are writing. These are events that may be produced from outside our immediate system but have business relevance or enable us to gain valuable insights into things that are affecting your business. Here is a list of common event sources: IoT devices or sensors showing device status changes Click Stream data from web or mobile applications Mobile applications (HTTP to Back-end for Front-end service and then to topic) Geospatial data Weather alerts Social media feeds Real-time voice feeds Other messaging backbone Data change event streams from databases (change data capture) IoT devices and sensors With IoT devices and sensors you typically have a gateway providing the connectivity for the device, and a level of event enrichment and filtering. In terms of domain driven design you would see the device and gateway as being the technical domain and the event-driven reference architecture as providing the infrastructure for the applications in a business domain. In practice, the IoT gateway or platform provides the connectivity and is the point of filtering and consolidation of events so that only business-relevant events are passed up to the business domain. The gateway can also be the point where the technical event is enhanced to relate to something recognizable at the business level. One example of this is to relate a device number or identifier in the event to something that the business recognizes. Clickstream data Clickstream data is often used to understand the behavior of users as they navigate their way through web or mobile apps. It provides a recording of the actions they take, such as the clicks, the mouse-movements, and the gestures. Analysis of the clickstream data can lead to a deep understanding of how users actually interact with the application. It enables you to detect where users struggle and to look for ways to improve the experience. Processing the clickstream in real time in an event-driven architecture can also give rise to the opportunity to take direct action in response to what a user is currently doing, or more accurately has just done. There are various \"collectors\" that enable collection of standard clickstream events and allow custom actions to be collected as events typically through tags in Javascript. Within the Apache Open Source communities the Divolte collector is an example of one of these collectors that directly publishes events to Kafka topics. Event standards and schemas Where you have control as the producer of an event we should consider having an event schema and following a standard to provide the best opportunity for portability of the solutions across cloud environments. With a lack of formal standards, a working group under the Cloud Native Computing Foundation (CNCF) has recently been formed to define and propose Cloud Events as the standard. Microservices The event-driven reference architecture provides support for event-driven microservices - microservices which are connected through and communicate via the pub/sub communication protocol within an event backbone. With Kafka as the event backbone and pub/sub messaging provider, microservices can use the Kafka API's to publish and listen for events. On cloud-native platforms, microservices are the application architecture of choice. As businesses become event-driven, event driven pattern needs to extend into our microservices application space. This means that your microservices are still doing REST calls to well-known microservice but they must respond to and send out events, or in event-driven terms they need to be both event producers and consumers to enforce strong decoupling. With the adoption of microservices, the focus on synchronous communication between services has increased. Service mesh packages such as Istio help with the management of communication, service discovery, load balancing, and visibility in this synchronous communication environment. With event-driven microservices, the communication point becomes the Pub/Sub layer of the event backbone. By adopting an event-based approach for intercommunication between microservices, the microservices applications are naturally responsive (event-driven). This approach enhances the loose coupling nature of microservices because it decouples producers and consumers. Further, it enables the sharing of data across microservices through the event log. These event style characteristics are increasingly important considerations when you develop microservices style applications. In practical terms microservices applications are a combination of synchronous API-driven, and asynchronous event-driven communication styles. For the implementation point of view a set of established patterns are used, such as Database per Service, Event Sourcing, Command Query Responsibility Segregation, Saga, ... Event-driven apps with containers While the serverless approach with Cloud Functions provides a simplified event-based programming model, the majority of microservices applications today are developed for and deployed to a container-based cloud-native stack. Within the cloud-native landscape, Kubernetes is the standard platform for container orchestration, and therefore becomes the base for the container platform in the event-driven architecture. As before, the event backbone is the Pub/Sub communication provider and event log for shared data for the microservices. In this context microservices are developed as direct consumers and producers of events on the backbone via topics. The extra work in this environment is in managing consumer instances to respond to the demand of the event stream. You must determine how many consumer instances need to be running to keep pace with, or always be immediately available to execute, the microservice in response to an arriving event. Commands A command, is an instruction to do something. Typically, commands are directed to a particular consumer. The consumer runs the required command or process, and passes back a confirmation to the issuer stating that the command has been processed. The Command concept plays a key role in the Command-Query Responsibility Segregation pattern that is more commonly known as CQRS . Loose coupling Loose coupling is one of the main benefits of event-driven processing. It allows event producers to emit events without any knowledge about who is going to consume those events. Likewise, event consumers don't need to be aware of the event emitters. Because of this, event consuming modules and event producer modules can be implemented in different languages or use technologies that are different and appropriate for specific jobs. Loosely coupled modules are better suited to evolve independently and, when implemented correctly, result in a significant decrease in system complexity. Loose coupling, however, does not mean \u201cno coupling\u201d. An event consumer consumes events that are useful in achieving its goals and in doing so establishes what data it needs and the type and format of that data. The event producer emits events that it hopes are understood and useful to consumers thus establishing an implicit contract with potential consumers. For example, an event notification in XML format must conform to a certain schema that must be known by both the consumer and the producer. One of the most important things that you can do to reduce coupling in an event-driven system is to reduce the number of distinct event types that flow between modules. To do this you must pay attention to the cohesiveness of those modules. Cohesion Cohesion is the degree to which related things are encapsulated together in the same software module. For the purposes of this EDA discussion, a module is defined as an independently deployable software unit that has high cohesion. Cohesion is strongly related to coupling in the sense that a highly cohesive module communicates less with other modules, thus reducing the number of events most importantly, the number of event types in the system. The less frequently modules interact with each other, the less coupled they are. Achieving cohesion in software while optimizing module size for flexibility and adaptability is difficult, but something to strive for. Designing for cohesion starts with a holistic understanding of the problem domain and good analysis work. Sometimes it must also take into account the constraints of the supporting software environment. Monolithic implementations and implementations that are excessively fine-grained must be avoided.","title":"EDA Concepts"},{"location":"concepts/terms-and-definitions/index.html#terms-definitions","text":"","title":"Terms &amp; Definitions"},{"location":"concepts/terms-and-definitions/index.html#events","text":"Events are notifications of change of state. Notifications are issued, or published and interested parties can subscribe and take action on the events. Typically, the issuer of the notification has no knowledge of what action is taken and receives no corresponding feedback that the notification has been processed. Events are notifications of change of state. Typically, events represent the change of state of something of interest to the business. Events are records of something that has happened. Events can't be changed, that is, they are immutable. (We can't change something that has happened in the past).","title":"Events"},{"location":"concepts/terms-and-definitions/index.html#event-streams","text":"An event stream is a continuous unbounded series of events. The start of the stream may have occurred before we started to process the stream. The end of the stream is at some unknown point in the future. Events are ordered by the point in time at which each event occurred. When developing event driven solutions, you will typically see two types of event streams: Event streams whose events are defined and published into a stream as part of a solution. Event streams that connect to a real-time event stream, for example from an IOT device, a voice stream from a telephone system, a video stream, or ship or plane locations from global positioning systems.","title":"Event streams"},{"location":"concepts/terms-and-definitions/index.html#event-backbone","text":"The event backbone is the communication layer in the event driven architecture. It provides the connection between event driven capabilities and in the Cloud Native , it becomes the Pub/Sub communication layer for event driven microservices. At this high level we would consider two types of relevant technologies for the event backbone, Message Brokers and Event Logs . Both technology types could be used to achieve the event communication style, with the \"Publish and subscribe\" model however, it is also important to consider other capabilities which are frequently used within event driven solutions: Keeping an Event Log as a time sequenced as it happened recording of events (Source of the truth). Enabling direct replay of events. Enabling Event Sourcing as a way of recording state changes in distributed systems. Enabling programmatic access to the continuous event stream . When viewed across these wider event driven capabilities, an event log style technology can provide a central component which can support all of these capabilities, whereas a message broker would have to be extended with other components.","title":"Event backbone"},{"location":"concepts/terms-and-definitions/index.html#selecting-the-event-backbone-for-the-reference-architecture","text":"For the event-driven architecture, we defined the following characteristics to be essential for the event backbone: Publish-subscribe event communication between event producers and consumers Facilitate many consumers with shared central \u201csource of truth\u201d. Capability to store events for a given period of time (event log). This is the shared source of the truth for events. Ability for consumers to subscribe to events. Provide replay of events from history for evolving application instances. Provide programmatic access to continuous stream of events, with minimum time lag. Must be highly scalable and resilient to cloud deployment levels. While choosing an event backbone for your event-driven application development, the following functional and non-functional requirements should be taken into consideration:","title":"Selecting the Event Backbone for the reference architecture"},{"location":"concepts/terms-and-definitions/index.html#persistence","text":"When source systems generate events, the consumers of those are interested in those events may not be online or available at the same time. So you need a way to store these messages for a configurable period of time until they are consumed and acted upon. Event backbone should be able to provide such event persistence.","title":"Persistence"},{"location":"concepts/terms-and-definitions/index.html#observability","text":"At times, you need an overall view of how events are ingested by source systems and getting processed by consumers. It could be a management console where events can be observed. Event backbone should provide such observability.","title":"Observability"},{"location":"concepts/terms-and-definitions/index.html#fault-tolerance","text":"Event backbone could be made of several components. If one of them becomes unavailable, there should not be any impact on the event processors dependent on the backbone. Event backbone needs to provide this resiliency.","title":"Fault tolerance"},{"location":"concepts/terms-and-definitions/index.html#high-availability","text":"Event backbone provides persistence of messages/events. If one of the components of the backbone becomes unavailable, there should not be any impact on the availability of these messages/events. Event backbone should be highly available.","title":"High availability"},{"location":"concepts/terms-and-definitions/index.html#performance","text":"Event backbone should provide means of accelerating the event processing operations (e.g. parallelising event processing) thereby providing enhanced performance.","title":"Performance"},{"location":"concepts/terms-and-definitions/index.html#delivery-guarantees","text":"Event backbone should support guaranteed delivery both for producer and consumer. It should support the delivery guarantee options of at least once , at most once , and exactly once .","title":"Delivery guarantees"},{"location":"concepts/terms-and-definitions/index.html#security","text":"The data residing in the event backbone should be secured, at rest as well as in transit. Only authenticated and authorized users should be able to publish and consume messages from the backbone. Topic specific authorizations will also help blocking access by unauthorized consumers. Event backbone should provide these security measures.","title":"Security"},{"location":"concepts/terms-and-definitions/index.html#stateful-operations-for-events-streams","text":"Sometimes, source systems generate a continuous flow of 'inter-related' events (e.g. IoT sensors sending data every second). In order to process such messages correctly, the event backbone needs to support for stateful operations like windowing, joins, aggregations. and any type of real time analytics.","title":"Stateful operations for events streams"},{"location":"concepts/terms-and-definitions/index.html#event-routing-options","text":"In EDA, event consumers may not be online at all times. So, it should be easier for consumers to subscribe to a topic when it comes online.","title":"Event routing options"},{"location":"concepts/terms-and-definitions/index.html#on-failure-hooks","text":"Event backbone can support pre-configured actions/behaviors for certain messages. E.g. if a consumer fails to process a message more than a certain number of times, that message can be sent to another topic for re-trying the processing action. Looking across these capabilities, the potential technologies, the amount of adoption and community activity around the technologies, and the considerations listed above, we selected Apache Kafka as our event backbone for the event-driven reference architecture.","title":"On-failure hooks"},{"location":"concepts/terms-and-definitions/index.html#event-sources","text":"When you consider an event-driven architecture, think about event producers and event consumers as the interaction points with events. As you develop event-driven applications following a microservices architecture, the microservices you develop play the role of both event producers and event consumers, with the events being passed as the communication payload between them. However, as you look at the wider opportunities that being event driven offers, you need to widen your view and consider event sources that come from beyond the application code you are writing. These are events that may be produced from outside our immediate system but have business relevance or enable us to gain valuable insights into things that are affecting your business.","title":"Event sources"},{"location":"concepts/terms-and-definitions/index.html#here-is-a-list-of-common-event-sources","text":"IoT devices or sensors showing device status changes Click Stream data from web or mobile applications Mobile applications (HTTP to Back-end for Front-end service and then to topic) Geospatial data Weather alerts Social media feeds Real-time voice feeds Other messaging backbone Data change event streams from databases (change data capture)","title":"Here is a list of common event sources:"},{"location":"concepts/terms-and-definitions/index.html#iot-devices-and-sensors","text":"With IoT devices and sensors you typically have a gateway providing the connectivity for the device, and a level of event enrichment and filtering. In terms of domain driven design you would see the device and gateway as being the technical domain and the event-driven reference architecture as providing the infrastructure for the applications in a business domain. In practice, the IoT gateway or platform provides the connectivity and is the point of filtering and consolidation of events so that only business-relevant events are passed up to the business domain. The gateway can also be the point where the technical event is enhanced to relate to something recognizable at the business level. One example of this is to relate a device number or identifier in the event to something that the business recognizes.","title":"IoT devices and sensors"},{"location":"concepts/terms-and-definitions/index.html#clickstream-data","text":"Clickstream data is often used to understand the behavior of users as they navigate their way through web or mobile apps. It provides a recording of the actions they take, such as the clicks, the mouse-movements, and the gestures. Analysis of the clickstream data can lead to a deep understanding of how users actually interact with the application. It enables you to detect where users struggle and to look for ways to improve the experience. Processing the clickstream in real time in an event-driven architecture can also give rise to the opportunity to take direct action in response to what a user is currently doing, or more accurately has just done. There are various \"collectors\" that enable collection of standard clickstream events and allow custom actions to be collected as events typically through tags in Javascript. Within the Apache Open Source communities the Divolte collector is an example of one of these collectors that directly publishes events to Kafka topics.","title":"Clickstream data"},{"location":"concepts/terms-and-definitions/index.html#event-standards-and-schemas","text":"Where you have control as the producer of an event we should consider having an event schema and following a standard to provide the best opportunity for portability of the solutions across cloud environments. With a lack of formal standards, a working group under the Cloud Native Computing Foundation (CNCF) has recently been formed to define and propose Cloud Events as the standard.","title":"Event standards and schemas"},{"location":"concepts/terms-and-definitions/index.html#microservices","text":"The event-driven reference architecture provides support for event-driven microservices - microservices which are connected through and communicate via the pub/sub communication protocol within an event backbone. With Kafka as the event backbone and pub/sub messaging provider, microservices can use the Kafka API's to publish and listen for events. On cloud-native platforms, microservices are the application architecture of choice. As businesses become event-driven, event driven pattern needs to extend into our microservices application space. This means that your microservices are still doing REST calls to well-known microservice but they must respond to and send out events, or in event-driven terms they need to be both event producers and consumers to enforce strong decoupling. With the adoption of microservices, the focus on synchronous communication between services has increased. Service mesh packages such as Istio help with the management of communication, service discovery, load balancing, and visibility in this synchronous communication environment. With event-driven microservices, the communication point becomes the Pub/Sub layer of the event backbone. By adopting an event-based approach for intercommunication between microservices, the microservices applications are naturally responsive (event-driven). This approach enhances the loose coupling nature of microservices because it decouples producers and consumers. Further, it enables the sharing of data across microservices through the event log. These event style characteristics are increasingly important considerations when you develop microservices style applications. In practical terms microservices applications are a combination of synchronous API-driven, and asynchronous event-driven communication styles. For the implementation point of view a set of established patterns are used, such as Database per Service, Event Sourcing, Command Query Responsibility Segregation, Saga, ...","title":"Microservices"},{"location":"concepts/terms-and-definitions/index.html#event-driven-apps-with-containers","text":"While the serverless approach with Cloud Functions provides a simplified event-based programming model, the majority of microservices applications today are developed for and deployed to a container-based cloud-native stack. Within the cloud-native landscape, Kubernetes is the standard platform for container orchestration, and therefore becomes the base for the container platform in the event-driven architecture. As before, the event backbone is the Pub/Sub communication provider and event log for shared data for the microservices. In this context microservices are developed as direct consumers and producers of events on the backbone via topics. The extra work in this environment is in managing consumer instances to respond to the demand of the event stream. You must determine how many consumer instances need to be running to keep pace with, or always be immediately available to execute, the microservice in response to an arriving event.","title":"Event-driven apps with containers"},{"location":"concepts/terms-and-definitions/index.html#commands","text":"A command, is an instruction to do something. Typically, commands are directed to a particular consumer. The consumer runs the required command or process, and passes back a confirmation to the issuer stating that the command has been processed. The Command concept plays a key role in the Command-Query Responsibility Segregation pattern that is more commonly known as CQRS .","title":"Commands"},{"location":"concepts/terms-and-definitions/index.html#loose-coupling","text":"Loose coupling is one of the main benefits of event-driven processing. It allows event producers to emit events without any knowledge about who is going to consume those events. Likewise, event consumers don't need to be aware of the event emitters. Because of this, event consuming modules and event producer modules can be implemented in different languages or use technologies that are different and appropriate for specific jobs. Loosely coupled modules are better suited to evolve independently and, when implemented correctly, result in a significant decrease in system complexity. Loose coupling, however, does not mean \u201cno coupling\u201d. An event consumer consumes events that are useful in achieving its goals and in doing so establishes what data it needs and the type and format of that data. The event producer emits events that it hopes are understood and useful to consumers thus establishing an implicit contract with potential consumers. For example, an event notification in XML format must conform to a certain schema that must be known by both the consumer and the producer. One of the most important things that you can do to reduce coupling in an event-driven system is to reduce the number of distinct event types that flow between modules. To do this you must pay attention to the cohesiveness of those modules.","title":"Loose coupling"},{"location":"concepts/terms-and-definitions/index.html#cohesion","text":"Cohesion is the degree to which related things are encapsulated together in the same software module. For the purposes of this EDA discussion, a module is defined as an independently deployable software unit that has high cohesion. Cohesion is strongly related to coupling in the sense that a highly cohesive module communicates less with other modules, thus reducing the number of events most importantly, the number of event types in the system. The less frequently modules interact with each other, the less coupled they are. Achieving cohesion in software while optimizing module size for flexibility and adaptability is difficult, but something to strive for. Designing for cohesion starts with a holistic understanding of the problem domain and good analysis work. Sometimes it must also take into account the constraints of the supporting software environment. Monolithic implementations and implementations that are excessively fine-grained must be avoided.","title":"Cohesion"},{"location":"methodology/index.html","text":"Methodology In this project we have covered some event storming and domain driven design practices. You will find our working content on event storming in our public github with also domain driven design and some documented examples like the vaccine solution design thinking and event storming workshop report add the DDD applied to this use case. We also quickly reviewed the developer experience and link to the methodology. In the next section we describe in more detail the potential practices, you could adopt in the future. Getting developer on board The first question is how to relate the artifacts discovered and analyzed during the domain driven design workshops to API, event, schema... So the OpenAPI document will most likely integrate verbs supporting the 'blue stickers'. With OpenAPI 3.0 specification the schema can be separated, so schema definition can be done in separate documentation. As an implementation approach we can use OpenAPI editor to design the beginning of the API specifications from the discovered commands, and entities, aggregates and value objects. For the events discovered, we can develop JSON schema to at least cover all the event definitions and then incrementally add attributes to them. It is important to use an iterative approach to define such schema. Once a first, good enough, version is defined we can start to deloy to a schema registry. The asyncAPI could be developed top-down (See api management section ) or discovered once Kafka Topic is defined. If we consider developer's artifacts, we can see that most of the bounded contexts will lead to at least one microservice. Then within each microservice git repository, developer can start developing: OpenAPI definitions started by an architect or lead developer during the analyzis of the DDD commands and aggregates Event schemas and asyncAPI. Code Application deployment manifests for Kubernetes Topic definition for which this repository has ownership. (For example the entity managed by this microservice could define topic(s) to produce facts about this entity) Adopting a git action pipeline and a gitops repository, the topic definition and application manifests will evolve to be part of this gitops repository. Once APIs reach a suffisant level of maturity they can be uploaded to an API management tool to support subscription and deployment to gateway. Schema definitions are also published to schema registry which are used by Kafka based solution. CI/CD pipelines can automate this deployment process. This figure above presents the concepts of the elements to consider as part of the methodology. But if we think in term of production and development environments, things may differ. For development or testing environment, schema registry is mandatory so the Gitops needs to provision it and upload any schemas. For production environment the API management becomes mandatory and APIs need to be uploaded to start managing their subscriptions.","title":"Methodology"},{"location":"methodology/index.html#methodology","text":"In this project we have covered some event storming and domain driven design practices. You will find our working content on event storming in our public github with also domain driven design and some documented examples like the vaccine solution design thinking and event storming workshop report add the DDD applied to this use case. We also quickly reviewed the developer experience and link to the methodology. In the next section we describe in more detail the potential practices, you could adopt in the future.","title":"Methodology"},{"location":"methodology/index.html#getting-developer-on-board","text":"The first question is how to relate the artifacts discovered and analyzed during the domain driven design workshops to API, event, schema... So the OpenAPI document will most likely integrate verbs supporting the 'blue stickers'. With OpenAPI 3.0 specification the schema can be separated, so schema definition can be done in separate documentation. As an implementation approach we can use OpenAPI editor to design the beginning of the API specifications from the discovered commands, and entities, aggregates and value objects. For the events discovered, we can develop JSON schema to at least cover all the event definitions and then incrementally add attributes to them. It is important to use an iterative approach to define such schema. Once a first, good enough, version is defined we can start to deloy to a schema registry. The asyncAPI could be developed top-down (See api management section ) or discovered once Kafka Topic is defined. If we consider developer's artifacts, we can see that most of the bounded contexts will lead to at least one microservice. Then within each microservice git repository, developer can start developing: OpenAPI definitions started by an architect or lead developer during the analyzis of the DDD commands and aggregates Event schemas and asyncAPI. Code Application deployment manifests for Kubernetes Topic definition for which this repository has ownership. (For example the entity managed by this microservice could define topic(s) to produce facts about this entity) Adopting a git action pipeline and a gitops repository, the topic definition and application manifests will evolve to be part of this gitops repository. Once APIs reach a suffisant level of maturity they can be uploaded to an API management tool to support subscription and deployment to gateway. Schema definitions are also published to schema registry which are used by Kafka based solution. CI/CD pipelines can automate this deployment process. This figure above presents the concepts of the elements to consider as part of the methodology. But if we think in term of production and development environments, things may differ. For development or testing environment, schema registry is mandatory so the Gitops needs to provision it and upload any schemas. For production environment the API management becomes mandatory and APIs need to be uploaded to start managing their subscriptions.","title":"Getting developer on board"},{"location":"patterns/api_mgt/index.html","text":"Modernization of the API Lifecycle This page summarizes some of the best practices for introducing API Management in development practices and architecture patterns within an enterprise setting. Moving from a Pure API Gateway to an API Management system An API Gateway helps provide security, control, integration, and optimized access to a full range of mobile, web, application programming interface (API), service-oriented architecture (SOA), B2B and cloud workloads. A gateway is used in the following patterns: As a Security Gateway , placed between the consumer facing firewall and the system of records facing firewall (DMZ). It is used for both policy enforcement and consistent security policies across business channels. As an API Gateway , both as an internal and external gateway, with centralized service governance and policy enforcement, and with traffic monitoring. To provide connectivity (HTTP) and mediation (XML, JSON) services in the internal network, close to the system of record. API Management gives enterprises greater flexibility when reusing the functionality of API integrations and helps save time and money without trading off security. An API Management system supports a broader scope of features for API lifecycle management, including: API lifecycle management to activate, retire, or stage an API product. API governance with security, access, and versioning. Analytics, dashboards, and third party data offload for usage analysis. API socialization based on a portal for the developer community that allows self-service and discovery. An API developer toolkit to facilitate the creation and testing of APIs. Classical Pain Points Some of the familiar pain points that indicate the need for a broader API Management product include: Current API details like endpoints, request/response message format, error conditions, test messages, and SLAs are not easily available or not well documented. Difficult to tell which subscribers are really using the API and how often, without building a custom solution. Difficult to differentiate between business-critical subscribers versus low value subscribers. Managing different lines of business and organizations is complex. No dynamic scaling built into the solution, which often means making hardware investments for maximum load or worst availability scenarios. Difficult to evolve the APIs, such as moving from SOAP based web services to RESTful services to GraphQL No support for AsyncAPI to automate and formalize the documentation or code generation of any event-driven APIs. The need to ensure consistent security rules Integrating CI/CD pipelines with the API lifecycle API Management for Travelport To determine whether IBM's API Management product, API Connect , can meet Travelport's requirements, Travelport presented several use cases to IBM regarding the product's support for API design and governance. The following table addresses those use cases, and highlights in green the ones we had time to demonstrate during our API Management demo: Enterprise APIs across boundaries If you consider a typical API Management product, it includes a set of components as presented in the figure below that could be deployed on an on-premise Kubernetes-based platform or in several Cloud provider regions. APIs served by different applications or microservices can be deployed in multiple regions but still be managed by one central API Management server. Here is how it works: An API developer signs on to the API Management cloud services account and accesses the API developer User interface or CLI toolkit. The developer creates the sync API and implements business logic. He maps and integrates the API data model to the back-end schema through the transformation and connectivity service. He tests and deploys the API to the runtime and publishes to the API Management system. He can also create Async APIs from a messaging system by binding channels to topics or queues and define the message payload. The API Product Manager or API owner signs on to the API Management cloud services account and accesses the API Management component. She includes the sync API endpoint to existing API products, and plans and specifies access controls. She publishes the API to the developer portal for external discovery by application developers. An application developer accesses the developer portal and uses search to discover any available APIs. The application developer uses the API in an application and deploys that application to the device. The device user opens the application that issues the API request. The request is handled by the API gateway, which performs load balancing and security validation for all API requests. The API gateway also validates access policies with API Management and invokes the API. The API polyglot runtime executes the API and obtains the data payload from the backend. The API response is sent back to the API gateway. Alternatively, APIs exposed by enterprise applications can be executed on that enterprise application runtime. The API gateway forwards the response to the calling application. The API gateway reports usage metrics and analytics to the API Management system. API developers and API owners can log on to the API analytics visualization component to view dashboards on API usage metrics and other analytics. For Travelport, a deployment across cloud providers could look like the diagram below, using API Connect in CP4I: On the left side (green boxes), the consumers of the API register to a Developer portal to get the metadata about the API they want to consume. The Developer portal also allows them to test the APIs to better understand how they work. The consumers register their applications as API subscribers. These applications can run on the cloud or on-premise. For Travelport's Rail Services, the API Gateway services would be colocated with the target Rail Services to reduce latency. These would be deployed as StatefulSet on an OpenShift cluster, which means as a set of pods with consistent identities . Identities are defined as: Network : A single stable DNS and hostname. Storage : As many VolumeClaims as requested. The StatefulSet guarantees that a given network identity will always map to the same storage identity. An API Gateway acts as a reverse proxy, and, in this case, exposes the Booking APIs , enforcing user authentication and security policies, and handling traffic monitoring, rate limiting, and statistics. The API Gateway can also perform transformations and aggregate various services to fulfill a request. The Developer Portals can be separated, or centralized depending on API characteristics exposed from different clouds (e.g. different Developer Portals for internal and external APIs). In the example above, the portal is deployed onto the Cloud Provider as a container inside an OpenShift cluster. The Analytic service is also a StatefulSet and gets metrics from the gateway. Rail services for Travelport are accessing remote consolidators and this traffic can also go to the API gateway. Those services will also integrate with existing backend services running on-premise, whether they are deployed or not on OpenShift. In the diagram above, the management service for the API Management product is depicted as running on-premise to illustrate that it is a central deployment to manage multiple gateways. Gateway services, Developer Portal services, and Analytics services are scoped to a single region, unlike the Management System, which can communicate across availability zones. The different API Management services run on OpenShift which can help ensure high availability of each of the components. Open API Within the API Management system, the OpenAPI document can be created top-down using a Swagger-based UI or bottom up using Annotation in the Java JAXRS resource classes. Either way, the API can be uploaded to the API Management product. The important parts are to define the operations exposed and the request / response structure of the data model. Support for Async API Cloud Pak for Integration (CP4I) 2021.1, which includes APIConnect V10, also provides some support for the AsyncAPI specification. AsyncAPI is an open source initiative that focuses on making Event-Driven Architectures (EDAs) as easy to work with as REST APIs. The AsyncAPI specification (currently at 2.0.0) establishes standards for events and EDAs, covering everything \"from documentation to code generation, and from discovery to event management\" asyncapi.com/docs . The goal is to enable the creation of better tooling in the message-driven space, better governance of asynchronous APIs, and standardization in documentation for asynchronous APIs. In short, Async API is to Message-driven architectures what OpenAPI is to REST APIs. While OpenAPI is the recommended practice for RESTful APIs, adopting AsyncAPI is the recommended practice for event-driven APIs. AsyncAPI Documents An AsyncAPI document is a file in either YAML or JSON format that defines and annotates the different components of an event-driven API. For example, AsyncAPI can formally describe how to connect to a Kafka cluster, the details of the Kafka topics (channels in AsyncAPI), and the type of data in messages. AsyncAPI includes both formal schema definitions and space for free-text descriptions (https://dalelane.co.uk/blog/?p=4219). Here is what it looks like: You may have noticed the similarities with OpenAPI. AsyncAPI was initially an adaptation of OpenAPI, which does not include support for the Message Queuing Telemetry Transport (MQTT) and the Advanced Message Queuing Protocol (AMQP). The creator of the AsyncAPI specification, Fran M\u00e9ndez, describes what he did at first with just OpenAPI to make up for the lack of MQTT and AMQP support: \"paths were AMQP topics, GET was SUBSCRIBE, and POST was PUBLISH--and ugly hack at best...\" This forced him to write additonal code to support the necessary EDA-based documentation and code generation. Many companies use OpenAPI, but in real-world situations, including that of Travelport, systems need formalized documentation and code generation support for both REST APIs and events. Here are the structural differences (and similarities) between OpenAPI and AsyncAPI: Source: https://www.asyncapi.com/docs/getting-started/coming-from-openapi Note a few things: * AsyncAPI is compatible with OpenAPI schemas, which is quite useful since many times the information flowing in the events is very similar to the one the REST APIs have to handle in requests and responses. * The message payload in AsyncAPI does not have to be an AsyncAPI/OpenAPI schema; it can be any value such as an Apache Avro schema, which is considered to be one of the better choices for stream data, where data is modeled as streams (see the section titled Why Use Avro for Kafka below). * The AsyncAPI server object is almost identical to its OpenAPI counterpart with the exception that scheme has been renamed to protocol and AsyncAPI introduces a new property called protocolVersion . * AsyncAPI channel parameters are the equivalent of OpenAPI path parameters, except that AsyncAPI does not have the notion of query and cookie , and header parameters can be defined in the message object. Describing Kafka with AsyncAPI It is also important to understand how to use AsyncAPI from the perspective of a Kafka user. The following section summarizes what is described in more detail in this article written by Dale Lane. First, there are some minor differences in terminology between Kafka and AsyncAPI that you should note: Comparing Kafka and AsyncAPI Terminology The AsyncAPI Document Considering the structure in the diagram above, let's look at some of the parts of an AsyncAPI document: Info The Info section has three parts which represent the minimum required information about the application: title , version , and description (optional), used as follows: asyncapi : 2.0.0 ... info : title : Account Service version : 1.0.0 description : This service is in charge of processing user signups In the description field you can use markdown language for rich-text formatting. Id The Id is the application identifier. It can be a URN (recommended) or URL. The important thing is that it must be a unique ID for your document. Here is an example: asyncapi : '2.0.0' ... id : 'urn:uk:co:andrewdoransmith:apimgmtdemo:railevents' ... Servers The servers section allows you to add and define which servers client applications can connect to for sending and receiving messages (for example, this could be a list of server objects, each uniquely identifying a Kafka broker). Here is an example, where you have three Kafka brokers in the same cluster: servers : broker1 : url : andy-broker-0:9092 protocol : kafka protocolVersion : '1.0.0' description : This is the first broker broker2 : url : andy-broker-1:9092 protocol : kafka protocolVersion : '1.0.0' description : This is the second broker broker3 : url : andy-broker-2:9092 protocol : kafka protocolVersion : '1.0.0' description : This is the third broker The example above uses the kafka protocol for the different brokers, which is a popular protocol for streaming solutions, but the protocol can be any. For example, the most common protocols include: mqtt , which is widely adopted by the Internet of Things and mobile apps, amqp , which is popular for its reliable queueing, ws for WebSockets, frequently used in browsers, and http , which is used in HTTP streaming APIs. Difference Between the AMQP and MQTT Protocols AMQP was mainly popularized by RabbitMQ. It provides reliable queuing, topic-based publish-and-subscribe messaging, flexible routing, transactions, and security. The main reasons to use AMQP are reliability and interoperability. AMQP exchanges route messages directly\u2014in fanout form, by topic, and also based on headers. MQTT The design principles and aims of MQTT are much more simple and focused than those of AMQP\u2014it provides publish-and-subscribe messaging (no queues, in spite of the name) and was specifically designed for resource-constrained devices and low bandwidth, high latency networks such as dial up lines and satellite links. Basically, it can be used effectively in embedded systems. When using a broker-centric architecture such as Kafka or RabbitMQ, you normally specify the URL of the broker. For more classic client-server models, such as REST APIs, your server should be the URL of the server. One limitation in AsyncAPI documents is that you cannot include multiple Kafka clusters (such as a production cluster and clusters for dev, test, and staging environments) in a single document. One workaround is to list all brokers for all clusters in the same document, and then rely on the description or extension fields to explain which ones are in which cluster. This is, however, not recommended because it could interfere with code generators or other parts of the AsyncAPI ecosystem which may consider them as all being members of one large cluster. So, as a best practice, avoid this workaround and stick to one cluster per AsyncAPI document. NOTE : As with OpenAPI, you can add additional attributes to the spec using the x- prefix, which identifies an entry as your own extension to the AsyncAPI specs. Security If the Kafka cluster doesn\u2019t have auth enabled, the protocol used should be kafka . Otherwise, if client applications are required to provide credentials, the protocol should be kafka-secure . To identify the type of credentials, add a security section to the server object. The value you put there is the name of a securityScheme object you define in the components section. The types of security schemes that you can specify aren\u2019t Kafka-specific. Choose the value that describes your type of approach to security. For example, if you\u2019re using SASL/SCRAM, which is a username/password-based approach to auth, you could describe this as userPassword . Here is an example: asyncapi : 2.0.0 ... servers : broker1 : url : localhost:9092 description : Production server protocol : kafka-secure protocolVersion : '1.0.0' security : - saslScramCreds : [] ... components : securitySchemes : saslScramCreds : type : userPassword description : Info about how/where to get credentials x-mykafka-sasl-mechanism : 'SCRAM-SHA-256' view raw The description field allows you to explain the security options that Kafka clients need to use. You could also use an extension (with the -x prefix). Channels All brokers support communication through multiple channels (known as topics , event types , routing keys , event names or other terms depending on the system). Channels are assigned a name or identifier. The channels section of the specification stores all of the mediums where messages flow through. Here is a simple example: channels : hello : publish : message : payload : type : string pattern : '^hello .+$' In this example, you only have one channel called hello. An app would subscribe to this channel to receive hello {name} messages. Notice that the payload object defines how the message must be structured. In this example, the message must be of type string and match the regular expression '^hello .+$' in the format hello {name} string. Each topic (or channel) identifies the operations that you want to describe in the spec. Here is another example: asyncapi : '2.0.0' ... channels : my.topic.name : description : This is my Kafka topic subscribe : operationId : someUniqueId summary : Interesting messages description : You can get really interesting messages from this topic tags : - name : awesome - name : interesting ... For each operation, you can provide a unique id, a short one-line text summary, and a more detailed description in plain text or markdown formatting. Bindings AsyncAPI puts protocol-specific values in sections called bindings . The bindings sections allows you to specify the values that Kafka clients should use to perform the operation. The values you can describe here include the consumer group id and the client id. If there are expectations about the format of these values, then you can describe those by using regular expressions: asyncapi : '2.0.0' ... channels : my.topic.name : description : This is my Kafka topic subscribe : ... bindings : kafka : groupId : type : string pattern : '^[A-Z]{10}[0-5]$' clientId : type : string pattern : '^[a-z]{22}$' bindingVersion : '0.1.0' You can instead specify a discrete set of values, in the form of enumerations: asyncapi : '2.0.0' ... channels : my.topic.name : description : This is my Kafka topic subscribe : ... bindings : kafka : groupId : type : string enum : - validone - validtwo clientId : type : string enum : - validoption bindingVersion : '0.1.0' view raw Messages A message is how information is exchanged via a channel between servers and applications. According to the AsyncAPI specifications, a message MUST contain a payload and MAY also contain headers. The headers MAY be subdivided into protocol-defined headers and header properties defined by the application which can act as supporting metadata. The payload contains the data, defined by the application, which MUST be serialized into a supported format (JSON, XML, Avro, binary, etc.). Because a message is a generic mechanism, it can support multiple interaction patterns such as event, command, request, or response. As with all the other levels of the spec, you can provide background and narrative in a description field for the message: asyncapi : '2.0.0' ... channels : my.topic.name : ... subscribe : ... message : description : Description of a single message Summary In short, the following diagram summarizes the sections described above: For more information, the official AsyncAPI specifications can be found here . Why Use Avro for Kafka? Apache Avro is \"an open source data serialization system that helps with data exchange between systems, programming languages, and processing frameworks\" (https://www.confluent.io/blog/avro-kafka-data/). Avro is a great fit for stream data because it has the following features: Direct mapping to and from JSON, but typically much faster than JSON, with much smaller encodings Compact format, making it more efficient for high-volume usage Bindings for a wide variety of programming languages A rich, extensible schema language defined in pure JSON The Travelport API Management Demo Besides a new, event-driven approach to its API model, Travelport needs a way to securely provide self-service access to different versions of its APIs, to enable their developers to discover and easily use these APIs, and to be able to redirect API calls based on several criteria. IBM API Connect (APIC) is a complete and scalable API Management platform that allows them to do these things, in addition to exposing, managing, and monetizing APIs across clouds. API Connect is also available with other capabilities as an IBM Cloud Pak\u00ae solution. IBM put together an API Management demo, mainly two demonstrate three main areas of interest to Travelport: * Version Control * API Documentation & Discovery * Redirecting to different APIs based on certain criteria Types of APIs An API is a set of functions that provide some business or technical capability and can be called by applications by using a defined protocol. Applications are typically mobile or web applications, and they use the HTTP protocol. An API definition is composed of paths, and can be one of the following types: REST API A REST API is a defined set of interactions that uses the HTTP protocol, typically by using JSON or XML as the data format that is exchanged. For example, a data request might use an HTTP GET method, and a data record might use an HTTP POST method. The choice of data format depends on the type of application that is calling the API. JSON is commonly used for web pages or mobile applications that present a user interface (by using JavaScript or HTML), whereas XML has been traditionally used for machine-to-machine scenarios, although that is changing. In IBM API Connect, you can create REST APIs by using LoopBack\u00ae functions to create models and data sources (top-down). These are then used by your REST API definition and exposed to your users. API Connect also supports a bottom-up approach where you can import existing APIs into the system and benefit from the API Management capabilities of the product. Alternatively, you can expose and secure your existing APIs by using a Proxy or Invoke policy. In either case, you can configure your API definition either by using the API Manager, or by writing an OpenAPI definition file and publishing it using either API Manager or the command line interface. SOAP API API Connect also allows you to create SOAP API definitions that are based on an existing Web Services Description Language (WSDL) file. You can use this facility to benefit from the capabilities that are provided by API Connect, which include analytics and mapping between variables. You can also expose the API by using the Developer Portal for any existing SOAP services in your organization, including any SOAP services that are part of a service-oriented architecture (SOA) or Enterprise Service Bus (ESB) infrastructure. You can create SOAP API definitions through either the command line interface, or through the API Manager UI. API Manager You can manage your APIs by using API Connect's API Manager UI: The API Manager UI allows you to manage private internal APIs as well as public external APIs. API Manager is an on-premises offering that provides the capabilities required to externalize and manage your services as REST or SOAP APIs. Developer Portal The Developer Portal is a convenient place to share APIs with application developers. After a Developer Portal has been enabled through the API Manager, and one or more API Products have been published, application developers can browse and use the APIs from the Developer Portal dashboard, as shown below: The Developer Portal can be used as is when it is first enabled, or it can be customized to fit the corporate branding and design requirements of a particular organization. You can configure the Developer Portal for test and development purposes, or for internal use only. Create Capability Travelport can leverage API Connect's Create capability, powered by the Open Source Loopback framework, to build APIs (top-down) with a built-in Swagger (OpenAPI) editor or using a simple guided model-driven approach. APIC allows developers to create a new API from scratch or an API based on the schema of an existing data source, such as a database. Creating a REST proxy API from an existing target service As mentioned earlier, it is also possible to create a REST proxy API from an existing target service (bottom-up) that you import into the API Connect system. This is the approach we took for the Travelport demo. Essentially, if you have an existing REST service that you want to expose through an IBM API Connect API definition, you can create a proxy API and specify the target endpoint by using the API Manager. Explore Capability As demonstrated in the Travelport demo, developers can use API Connect's Explore capability to quickly examine their new APIs and try their operations. Developers can add their APIs to the API Connect server and have the choice of running them on the cloud or on-premise, as mentioned above. This can be done through API Connect's UI or with the provided CLI. Developers and Administrators can then connect to the API Connect Server and see their running APIs, including those that were created as well as those that were added based on existing data sources. Subscribing to an API in the Developer Portal To subscribe to an API, from the Developer Portal, the develper clicks API Products to find and subscribe to any available APIs: In the example above, an API called FindBranch Version 2.0, which is contained in the FindBranch Auto Products product is available. Clicking Subscribe enables the developer to use the API: Under the Application heading, the developer can click Select App for the new application: From the next screen below, the developer can click Next: Doing this shows the screen below, which confirms to the developer that his or her applicatin is now subscribed to the selected API under the selected plan. Pressing Done in the next screen completes the subscription. Testing An API in the Developer Portal To test an API, the developer clicks API Products in the Developer Portal dashboard to show all available products: Clicking a product, such as FindBranch auto product, for example, and then clicking the FindBranch API from the provided list shows the available API operations. The developer can click GET/details , for instance, to see the details of the GET operation: Clicking the Try it tab and pressing Send allows the developer to test the API to better understand how it works: As you can see above, this shows the request/response from invoking the API. API Connect shows a returned response of 200 OK and the message body, indicating that the REST API operation call was successful. API Product Managers Administrators or API Product Managers can make changes such as adding corporate security polices or transformations before publishing the APIs. They can also control the visibility and define the rate limit for the APIs. Once the APIs are ready, API Product Managers can expose them for developer consumption and self-service in Travelport's Developer Portal. Developers who have signed up can discover and use any APIs which Travelport has exposed. Because the APIs were documented while they were being created using OpenAPI notation, developers can not only view the APIs but also try them out, as demonstrated in the demo. API Connect provides source code examples in several different languages that show how to use the API operations. Languages supported include Curl, Ruby, Python, Java, Go, and Node.js. Testing Besides the small individual tests that you can run from the Developer Portal to understand how an API works, as explained above and demonstrated in the Travelport demo, the demo also showed how to use API Connect Test and Monitor to effortlessly generate and schedule API test assertions. This free browser based tool also provides dashboards for monitoring API test assertions and for receiving alerts on the health of your APIs. As the demo showed, you can use an HTTP Client within API Connect Test and Monitor to generate a request to an API and get the expected results: You enter any required parameters and authorization token and press Send to send the request. API Connect Test and Monitor then shows you the response payload as a formatted JSON object: To generate a test, you then click Generate Test , and in the dialog that appears name the test and add it to a project. When you click the checkmark in the upper right corner of the dialog, API Connect Test and Monitor automatically generates test assertions based on the response payload. You can add, delete, or modify these assertions, or even add new ones from a broad selection: You can also reorder the assertions by dragging and dropping them: Although coding is not required, if you wish, you change the underlying code by clicking CODE to enter the code view: Once you have your test arranged the way you want it, you can run it to generate a test report, which highlights successes and failures: Test Scheduling IBM API Connect Test and Monitor also lets you automate test scheduling at regular intervals. To do this, you first save any test you may have open and exit out of the composer. Then publish your test to prepare it for scheduling: Once your test is published, you can click Schedule to schedule regular test intervals. Test and Monitor allows you to tweak the test schedule as necessary. You can create a new run, name it, and choose the times you wish to run your tests: Clicking Save Run schedules the tests. Finally, you can access the dashboard to monitor the health of your API: GraphQL APIs GraphQL is a query language for APIs that gives an application client greater control over what data it retrieves in an API request when compared with a REST API request. IBM\u00ae API Connect enables you to create a GraphQL API proxy definition that proxies a backend GraphQL server, and to define rate limiting controls that reflect the amount of data that is returned from the server by a request to the GraphQL API. The Developer Portal also supports testing GraphQL APIs. Advantages of GraphQL over REST APIs GraphQL provides the following particular advantages over REST APIs: The application client can request only the data that it needs. For example, when retrieving bank account records, request only the account number and current balance for each account, but not the customer name and address details. With a REST API request, either the backend REST service must provide separate endpoints for different data subsets, or the application client must retrieve the complete records and then discard the unwanted data. The application client can retrieve multiple related resources in a single request. For example, a customer's bank account record might include an array that references other finance products that the customer holds. If an application client wants to retrieve the bank account details for a specific customer, and details of the other finance products for that customer, then with a REST API the client would first retrieve the bank account details, then make separate requests for each of the other products. A GraphQL API can be designed to allow the client to retrieve all this information in a single request. However, this flexibility presents rate limiting challenges, because two seemingly very similar requests might return vastly different amounts of data, and what might have required multiple REST API requests, each counting towards the rate limit, might require only a single GraphQL API request. It is important therefore that rate limiting controls are imposed that reflect the amount of data that is retrieved. API Connect extends the GraphQL standard by providing, in a GraphQL API definition, the ability to configure a range of settings that are used to calculate the complexity of a GraphQL request and an associated cost that counts towards the rate limit. Creating a GraphQL proxy API The Travelport demo also showed how API Connect supports GraphQL. To create a GraphQL proxy API from an existing GraphQL server, click Add , then API (from REST, GraphQL or SOAP) : Then select, From existing GraphQL service (GraphQL proxy) : Give it a name and complete the rest of the parameters, especially the GraphQL Server URL field which, of course, cannot be left blank. Notice that, once you enter the URL, API Connect will find the service and automatically detect the schema. You can click Next and API Connect will attempt to connect to the GraphQL server, introspect it, and pull in any objects (in this case, queries and operations) that it can find on that server: The gateway will sometimes provide warnings (17 in the example above), which are really just recommendations on how you can optimize the query. Clicking Next allows you to then activate the API for publishing: Finally, clicking Next shows a screen such as the one below with checkmarks: This shows that the gateway was able to connect to the GraphQL server and detect its schema, paths, and operations, validate them, and automatically populate the canvas with different policies, including security policies: Further Readings IBM Redbook on Agile Integration A Demo of Event Endpoint Management - Cloud Pak for Integration","title":"API management"},{"location":"patterns/api_mgt/index.html#modernization-of-the-api-lifecycle","text":"This page summarizes some of the best practices for introducing API Management in development practices and architecture patterns within an enterprise setting.","title":"Modernization of the API Lifecycle"},{"location":"patterns/api_mgt/index.html#moving-from-a-pure-api-gateway-to-an-api-management-system","text":"An API Gateway helps provide security, control, integration, and optimized access to a full range of mobile, web, application programming interface (API), service-oriented architecture (SOA), B2B and cloud workloads. A gateway is used in the following patterns: As a Security Gateway , placed between the consumer facing firewall and the system of records facing firewall (DMZ). It is used for both policy enforcement and consistent security policies across business channels. As an API Gateway , both as an internal and external gateway, with centralized service governance and policy enforcement, and with traffic monitoring. To provide connectivity (HTTP) and mediation (XML, JSON) services in the internal network, close to the system of record. API Management gives enterprises greater flexibility when reusing the functionality of API integrations and helps save time and money without trading off security. An API Management system supports a broader scope of features for API lifecycle management, including: API lifecycle management to activate, retire, or stage an API product. API governance with security, access, and versioning. Analytics, dashboards, and third party data offload for usage analysis. API socialization based on a portal for the developer community that allows self-service and discovery. An API developer toolkit to facilitate the creation and testing of APIs.","title":"Moving from a Pure API Gateway to an API Management system"},{"location":"patterns/api_mgt/index.html#classical-pain-points","text":"Some of the familiar pain points that indicate the need for a broader API Management product include: Current API details like endpoints, request/response message format, error conditions, test messages, and SLAs are not easily available or not well documented. Difficult to tell which subscribers are really using the API and how often, without building a custom solution. Difficult to differentiate between business-critical subscribers versus low value subscribers. Managing different lines of business and organizations is complex. No dynamic scaling built into the solution, which often means making hardware investments for maximum load or worst availability scenarios. Difficult to evolve the APIs, such as moving from SOAP based web services to RESTful services to GraphQL No support for AsyncAPI to automate and formalize the documentation or code generation of any event-driven APIs. The need to ensure consistent security rules Integrating CI/CD pipelines with the API lifecycle","title":"Classical Pain Points"},{"location":"patterns/api_mgt/index.html#api-management-for-travelport","text":"To determine whether IBM's API Management product, API Connect , can meet Travelport's requirements, Travelport presented several use cases to IBM regarding the product's support for API design and governance. The following table addresses those use cases, and highlights in green the ones we had time to demonstrate during our API Management demo:","title":"API Management for Travelport"},{"location":"patterns/api_mgt/index.html#enterprise-apis-across-boundaries","text":"If you consider a typical API Management product, it includes a set of components as presented in the figure below that could be deployed on an on-premise Kubernetes-based platform or in several Cloud provider regions. APIs served by different applications or microservices can be deployed in multiple regions but still be managed by one central API Management server. Here is how it works: An API developer signs on to the API Management cloud services account and accesses the API developer User interface or CLI toolkit. The developer creates the sync API and implements business logic. He maps and integrates the API data model to the back-end schema through the transformation and connectivity service. He tests and deploys the API to the runtime and publishes to the API Management system. He can also create Async APIs from a messaging system by binding channels to topics or queues and define the message payload. The API Product Manager or API owner signs on to the API Management cloud services account and accesses the API Management component. She includes the sync API endpoint to existing API products, and plans and specifies access controls. She publishes the API to the developer portal for external discovery by application developers. An application developer accesses the developer portal and uses search to discover any available APIs. The application developer uses the API in an application and deploys that application to the device. The device user opens the application that issues the API request. The request is handled by the API gateway, which performs load balancing and security validation for all API requests. The API gateway also validates access policies with API Management and invokes the API. The API polyglot runtime executes the API and obtains the data payload from the backend. The API response is sent back to the API gateway. Alternatively, APIs exposed by enterprise applications can be executed on that enterprise application runtime. The API gateway forwards the response to the calling application. The API gateway reports usage metrics and analytics to the API Management system. API developers and API owners can log on to the API analytics visualization component to view dashboards on API usage metrics and other analytics. For Travelport, a deployment across cloud providers could look like the diagram below, using API Connect in CP4I: On the left side (green boxes), the consumers of the API register to a Developer portal to get the metadata about the API they want to consume. The Developer portal also allows them to test the APIs to better understand how they work. The consumers register their applications as API subscribers. These applications can run on the cloud or on-premise. For Travelport's Rail Services, the API Gateway services would be colocated with the target Rail Services to reduce latency. These would be deployed as StatefulSet on an OpenShift cluster, which means as a set of pods with consistent identities . Identities are defined as: Network : A single stable DNS and hostname. Storage : As many VolumeClaims as requested. The StatefulSet guarantees that a given network identity will always map to the same storage identity. An API Gateway acts as a reverse proxy, and, in this case, exposes the Booking APIs , enforcing user authentication and security policies, and handling traffic monitoring, rate limiting, and statistics. The API Gateway can also perform transformations and aggregate various services to fulfill a request. The Developer Portals can be separated, or centralized depending on API characteristics exposed from different clouds (e.g. different Developer Portals for internal and external APIs). In the example above, the portal is deployed onto the Cloud Provider as a container inside an OpenShift cluster. The Analytic service is also a StatefulSet and gets metrics from the gateway. Rail services for Travelport are accessing remote consolidators and this traffic can also go to the API gateway. Those services will also integrate with existing backend services running on-premise, whether they are deployed or not on OpenShift. In the diagram above, the management service for the API Management product is depicted as running on-premise to illustrate that it is a central deployment to manage multiple gateways. Gateway services, Developer Portal services, and Analytics services are scoped to a single region, unlike the Management System, which can communicate across availability zones. The different API Management services run on OpenShift which can help ensure high availability of each of the components.","title":"Enterprise APIs across boundaries"},{"location":"patterns/api_mgt/index.html#open-api","text":"Within the API Management system, the OpenAPI document can be created top-down using a Swagger-based UI or bottom up using Annotation in the Java JAXRS resource classes. Either way, the API can be uploaded to the API Management product. The important parts are to define the operations exposed and the request / response structure of the data model.","title":"Open API"},{"location":"patterns/api_mgt/index.html#support-for-async-api","text":"Cloud Pak for Integration (CP4I) 2021.1, which includes APIConnect V10, also provides some support for the AsyncAPI specification. AsyncAPI is an open source initiative that focuses on making Event-Driven Architectures (EDAs) as easy to work with as REST APIs. The AsyncAPI specification (currently at 2.0.0) establishes standards for events and EDAs, covering everything \"from documentation to code generation, and from discovery to event management\" asyncapi.com/docs . The goal is to enable the creation of better tooling in the message-driven space, better governance of asynchronous APIs, and standardization in documentation for asynchronous APIs. In short, Async API is to Message-driven architectures what OpenAPI is to REST APIs. While OpenAPI is the recommended practice for RESTful APIs, adopting AsyncAPI is the recommended practice for event-driven APIs.","title":"Support for Async API"},{"location":"patterns/api_mgt/index.html#asyncapi-documents","text":"An AsyncAPI document is a file in either YAML or JSON format that defines and annotates the different components of an event-driven API. For example, AsyncAPI can formally describe how to connect to a Kafka cluster, the details of the Kafka topics (channels in AsyncAPI), and the type of data in messages. AsyncAPI includes both formal schema definitions and space for free-text descriptions (https://dalelane.co.uk/blog/?p=4219). Here is what it looks like: You may have noticed the similarities with OpenAPI. AsyncAPI was initially an adaptation of OpenAPI, which does not include support for the Message Queuing Telemetry Transport (MQTT) and the Advanced Message Queuing Protocol (AMQP). The creator of the AsyncAPI specification, Fran M\u00e9ndez, describes what he did at first with just OpenAPI to make up for the lack of MQTT and AMQP support: \"paths were AMQP topics, GET was SUBSCRIBE, and POST was PUBLISH--and ugly hack at best...\" This forced him to write additonal code to support the necessary EDA-based documentation and code generation. Many companies use OpenAPI, but in real-world situations, including that of Travelport, systems need formalized documentation and code generation support for both REST APIs and events. Here are the structural differences (and similarities) between OpenAPI and AsyncAPI: Source: https://www.asyncapi.com/docs/getting-started/coming-from-openapi Note a few things: * AsyncAPI is compatible with OpenAPI schemas, which is quite useful since many times the information flowing in the events is very similar to the one the REST APIs have to handle in requests and responses. * The message payload in AsyncAPI does not have to be an AsyncAPI/OpenAPI schema; it can be any value such as an Apache Avro schema, which is considered to be one of the better choices for stream data, where data is modeled as streams (see the section titled Why Use Avro for Kafka below). * The AsyncAPI server object is almost identical to its OpenAPI counterpart with the exception that scheme has been renamed to protocol and AsyncAPI introduces a new property called protocolVersion . * AsyncAPI channel parameters are the equivalent of OpenAPI path parameters, except that AsyncAPI does not have the notion of query and cookie , and header parameters can be defined in the message object.","title":"AsyncAPI Documents"},{"location":"patterns/api_mgt/index.html#describing-kafka-with-asyncapi","text":"It is also important to understand how to use AsyncAPI from the perspective of a Kafka user. The following section summarizes what is described in more detail in this article written by Dale Lane. First, there are some minor differences in terminology between Kafka and AsyncAPI that you should note:","title":"Describing Kafka with AsyncAPI"},{"location":"patterns/api_mgt/index.html#comparing-kafka-and-asyncapi-terminology","text":"","title":"Comparing Kafka and AsyncAPI Terminology"},{"location":"patterns/api_mgt/index.html#the-asyncapi-document","text":"Considering the structure in the diagram above, let's look at some of the parts of an AsyncAPI document:","title":"The AsyncAPI Document"},{"location":"patterns/api_mgt/index.html#info","text":"The Info section has three parts which represent the minimum required information about the application: title , version , and description (optional), used as follows: asyncapi : 2.0.0 ... info : title : Account Service version : 1.0.0 description : This service is in charge of processing user signups In the description field you can use markdown language for rich-text formatting.","title":"Info"},{"location":"patterns/api_mgt/index.html#id","text":"The Id is the application identifier. It can be a URN (recommended) or URL. The important thing is that it must be a unique ID for your document. Here is an example: asyncapi : '2.0.0' ... id : 'urn:uk:co:andrewdoransmith:apimgmtdemo:railevents' ...","title":"Id"},{"location":"patterns/api_mgt/index.html#servers","text":"The servers section allows you to add and define which servers client applications can connect to for sending and receiving messages (for example, this could be a list of server objects, each uniquely identifying a Kafka broker). Here is an example, where you have three Kafka brokers in the same cluster: servers : broker1 : url : andy-broker-0:9092 protocol : kafka protocolVersion : '1.0.0' description : This is the first broker broker2 : url : andy-broker-1:9092 protocol : kafka protocolVersion : '1.0.0' description : This is the second broker broker3 : url : andy-broker-2:9092 protocol : kafka protocolVersion : '1.0.0' description : This is the third broker The example above uses the kafka protocol for the different brokers, which is a popular protocol for streaming solutions, but the protocol can be any. For example, the most common protocols include: mqtt , which is widely adopted by the Internet of Things and mobile apps, amqp , which is popular for its reliable queueing, ws for WebSockets, frequently used in browsers, and http , which is used in HTTP streaming APIs.","title":"Servers"},{"location":"patterns/api_mgt/index.html#difference-between-the-amqp-and-mqtt-protocols","text":"AMQP was mainly popularized by RabbitMQ. It provides reliable queuing, topic-based publish-and-subscribe messaging, flexible routing, transactions, and security. The main reasons to use AMQP are reliability and interoperability. AMQP exchanges route messages directly\u2014in fanout form, by topic, and also based on headers. MQTT The design principles and aims of MQTT are much more simple and focused than those of AMQP\u2014it provides publish-and-subscribe messaging (no queues, in spite of the name) and was specifically designed for resource-constrained devices and low bandwidth, high latency networks such as dial up lines and satellite links. Basically, it can be used effectively in embedded systems. When using a broker-centric architecture such as Kafka or RabbitMQ, you normally specify the URL of the broker. For more classic client-server models, such as REST APIs, your server should be the URL of the server. One limitation in AsyncAPI documents is that you cannot include multiple Kafka clusters (such as a production cluster and clusters for dev, test, and staging environments) in a single document. One workaround is to list all brokers for all clusters in the same document, and then rely on the description or extension fields to explain which ones are in which cluster. This is, however, not recommended because it could interfere with code generators or other parts of the AsyncAPI ecosystem which may consider them as all being members of one large cluster. So, as a best practice, avoid this workaround and stick to one cluster per AsyncAPI document. NOTE : As with OpenAPI, you can add additional attributes to the spec using the x- prefix, which identifies an entry as your own extension to the AsyncAPI specs.","title":"Difference Between the AMQP and MQTT Protocols"},{"location":"patterns/api_mgt/index.html#security","text":"If the Kafka cluster doesn\u2019t have auth enabled, the protocol used should be kafka . Otherwise, if client applications are required to provide credentials, the protocol should be kafka-secure . To identify the type of credentials, add a security section to the server object. The value you put there is the name of a securityScheme object you define in the components section. The types of security schemes that you can specify aren\u2019t Kafka-specific. Choose the value that describes your type of approach to security. For example, if you\u2019re using SASL/SCRAM, which is a username/password-based approach to auth, you could describe this as userPassword . Here is an example: asyncapi : 2.0.0 ... servers : broker1 : url : localhost:9092 description : Production server protocol : kafka-secure protocolVersion : '1.0.0' security : - saslScramCreds : [] ... components : securitySchemes : saslScramCreds : type : userPassword description : Info about how/where to get credentials x-mykafka-sasl-mechanism : 'SCRAM-SHA-256' view raw The description field allows you to explain the security options that Kafka clients need to use. You could also use an extension (with the -x prefix).","title":"Security"},{"location":"patterns/api_mgt/index.html#channels","text":"All brokers support communication through multiple channels (known as topics , event types , routing keys , event names or other terms depending on the system). Channels are assigned a name or identifier. The channels section of the specification stores all of the mediums where messages flow through. Here is a simple example: channels : hello : publish : message : payload : type : string pattern : '^hello .+$' In this example, you only have one channel called hello. An app would subscribe to this channel to receive hello {name} messages. Notice that the payload object defines how the message must be structured. In this example, the message must be of type string and match the regular expression '^hello .+$' in the format hello {name} string. Each topic (or channel) identifies the operations that you want to describe in the spec. Here is another example: asyncapi : '2.0.0' ... channels : my.topic.name : description : This is my Kafka topic subscribe : operationId : someUniqueId summary : Interesting messages description : You can get really interesting messages from this topic tags : - name : awesome - name : interesting ... For each operation, you can provide a unique id, a short one-line text summary, and a more detailed description in plain text or markdown formatting.","title":"Channels"},{"location":"patterns/api_mgt/index.html#bindings","text":"AsyncAPI puts protocol-specific values in sections called bindings . The bindings sections allows you to specify the values that Kafka clients should use to perform the operation. The values you can describe here include the consumer group id and the client id. If there are expectations about the format of these values, then you can describe those by using regular expressions: asyncapi : '2.0.0' ... channels : my.topic.name : description : This is my Kafka topic subscribe : ... bindings : kafka : groupId : type : string pattern : '^[A-Z]{10}[0-5]$' clientId : type : string pattern : '^[a-z]{22}$' bindingVersion : '0.1.0' You can instead specify a discrete set of values, in the form of enumerations: asyncapi : '2.0.0' ... channels : my.topic.name : description : This is my Kafka topic subscribe : ... bindings : kafka : groupId : type : string enum : - validone - validtwo clientId : type : string enum : - validoption bindingVersion : '0.1.0' view raw","title":"Bindings"},{"location":"patterns/api_mgt/index.html#messages","text":"A message is how information is exchanged via a channel between servers and applications. According to the AsyncAPI specifications, a message MUST contain a payload and MAY also contain headers. The headers MAY be subdivided into protocol-defined headers and header properties defined by the application which can act as supporting metadata. The payload contains the data, defined by the application, which MUST be serialized into a supported format (JSON, XML, Avro, binary, etc.). Because a message is a generic mechanism, it can support multiple interaction patterns such as event, command, request, or response. As with all the other levels of the spec, you can provide background and narrative in a description field for the message: asyncapi : '2.0.0' ... channels : my.topic.name : ... subscribe : ... message : description : Description of a single message","title":"Messages"},{"location":"patterns/api_mgt/index.html#summary","text":"In short, the following diagram summarizes the sections described above: For more information, the official AsyncAPI specifications can be found here .","title":"Summary"},{"location":"patterns/api_mgt/index.html#why-use-avro-for-kafka","text":"Apache Avro is \"an open source data serialization system that helps with data exchange between systems, programming languages, and processing frameworks\" (https://www.confluent.io/blog/avro-kafka-data/). Avro is a great fit for stream data because it has the following features: Direct mapping to and from JSON, but typically much faster than JSON, with much smaller encodings Compact format, making it more efficient for high-volume usage Bindings for a wide variety of programming languages A rich, extensible schema language defined in pure JSON","title":"Why Use Avro for Kafka?"},{"location":"patterns/api_mgt/index.html#the-travelport-api-management-demo","text":"Besides a new, event-driven approach to its API model, Travelport needs a way to securely provide self-service access to different versions of its APIs, to enable their developers to discover and easily use these APIs, and to be able to redirect API calls based on several criteria. IBM API Connect (APIC) is a complete and scalable API Management platform that allows them to do these things, in addition to exposing, managing, and monetizing APIs across clouds. API Connect is also available with other capabilities as an IBM Cloud Pak\u00ae solution. IBM put together an API Management demo, mainly two demonstrate three main areas of interest to Travelport: * Version Control * API Documentation & Discovery * Redirecting to different APIs based on certain criteria","title":"The Travelport API Management Demo"},{"location":"patterns/api_mgt/index.html#types-of-apis","text":"An API is a set of functions that provide some business or technical capability and can be called by applications by using a defined protocol. Applications are typically mobile or web applications, and they use the HTTP protocol. An API definition is composed of paths, and can be one of the following types:","title":"Types of APIs"},{"location":"patterns/api_mgt/index.html#rest-api","text":"A REST API is a defined set of interactions that uses the HTTP protocol, typically by using JSON or XML as the data format that is exchanged. For example, a data request might use an HTTP GET method, and a data record might use an HTTP POST method. The choice of data format depends on the type of application that is calling the API. JSON is commonly used for web pages or mobile applications that present a user interface (by using JavaScript or HTML), whereas XML has been traditionally used for machine-to-machine scenarios, although that is changing. In IBM API Connect, you can create REST APIs by using LoopBack\u00ae functions to create models and data sources (top-down). These are then used by your REST API definition and exposed to your users. API Connect also supports a bottom-up approach where you can import existing APIs into the system and benefit from the API Management capabilities of the product. Alternatively, you can expose and secure your existing APIs by using a Proxy or Invoke policy. In either case, you can configure your API definition either by using the API Manager, or by writing an OpenAPI definition file and publishing it using either API Manager or the command line interface.","title":"REST API"},{"location":"patterns/api_mgt/index.html#soap-api","text":"API Connect also allows you to create SOAP API definitions that are based on an existing Web Services Description Language (WSDL) file. You can use this facility to benefit from the capabilities that are provided by API Connect, which include analytics and mapping between variables. You can also expose the API by using the Developer Portal for any existing SOAP services in your organization, including any SOAP services that are part of a service-oriented architecture (SOA) or Enterprise Service Bus (ESB) infrastructure. You can create SOAP API definitions through either the command line interface, or through the API Manager UI.","title":"SOAP API"},{"location":"patterns/api_mgt/index.html#api-manager","text":"You can manage your APIs by using API Connect's API Manager UI: The API Manager UI allows you to manage private internal APIs as well as public external APIs. API Manager is an on-premises offering that provides the capabilities required to externalize and manage your services as REST or SOAP APIs.","title":"API Manager"},{"location":"patterns/api_mgt/index.html#developer-portal","text":"The Developer Portal is a convenient place to share APIs with application developers. After a Developer Portal has been enabled through the API Manager, and one or more API Products have been published, application developers can browse and use the APIs from the Developer Portal dashboard, as shown below: The Developer Portal can be used as is when it is first enabled, or it can be customized to fit the corporate branding and design requirements of a particular organization. You can configure the Developer Portal for test and development purposes, or for internal use only.","title":"Developer Portal"},{"location":"patterns/api_mgt/index.html#create-capability","text":"Travelport can leverage API Connect's Create capability, powered by the Open Source Loopback framework, to build APIs (top-down) with a built-in Swagger (OpenAPI) editor or using a simple guided model-driven approach. APIC allows developers to create a new API from scratch or an API based on the schema of an existing data source, such as a database.","title":"Create Capability"},{"location":"patterns/api_mgt/index.html#creating-a-rest-proxy-api-from-an-existing-target-service","text":"As mentioned earlier, it is also possible to create a REST proxy API from an existing target service (bottom-up) that you import into the API Connect system. This is the approach we took for the Travelport demo. Essentially, if you have an existing REST service that you want to expose through an IBM API Connect API definition, you can create a proxy API and specify the target endpoint by using the API Manager.","title":"Creating a REST proxy API from an existing target service"},{"location":"patterns/api_mgt/index.html#explore-capability","text":"As demonstrated in the Travelport demo, developers can use API Connect's Explore capability to quickly examine their new APIs and try their operations. Developers can add their APIs to the API Connect server and have the choice of running them on the cloud or on-premise, as mentioned above. This can be done through API Connect's UI or with the provided CLI. Developers and Administrators can then connect to the API Connect Server and see their running APIs, including those that were created as well as those that were added based on existing data sources.","title":"Explore Capability"},{"location":"patterns/api_mgt/index.html#subscribing-to-an-api-in-the-developer-portal","text":"To subscribe to an API, from the Developer Portal, the develper clicks API Products to find and subscribe to any available APIs: In the example above, an API called FindBranch Version 2.0, which is contained in the FindBranch Auto Products product is available. Clicking Subscribe enables the developer to use the API: Under the Application heading, the developer can click Select App for the new application: From the next screen below, the developer can click Next: Doing this shows the screen below, which confirms to the developer that his or her applicatin is now subscribed to the selected API under the selected plan. Pressing Done in the next screen completes the subscription.","title":"Subscribing to an API in the Developer Portal"},{"location":"patterns/api_mgt/index.html#testing-an-api-in-the-developer-portal","text":"To test an API, the developer clicks API Products in the Developer Portal dashboard to show all available products: Clicking a product, such as FindBranch auto product, for example, and then clicking the FindBranch API from the provided list shows the available API operations. The developer can click GET/details , for instance, to see the details of the GET operation: Clicking the Try it tab and pressing Send allows the developer to test the API to better understand how it works: As you can see above, this shows the request/response from invoking the API. API Connect shows a returned response of 200 OK and the message body, indicating that the REST API operation call was successful.","title":"Testing An API in the Developer Portal"},{"location":"patterns/api_mgt/index.html#api-product-managers","text":"Administrators or API Product Managers can make changes such as adding corporate security polices or transformations before publishing the APIs. They can also control the visibility and define the rate limit for the APIs. Once the APIs are ready, API Product Managers can expose them for developer consumption and self-service in Travelport's Developer Portal. Developers who have signed up can discover and use any APIs which Travelport has exposed. Because the APIs were documented while they were being created using OpenAPI notation, developers can not only view the APIs but also try them out, as demonstrated in the demo. API Connect provides source code examples in several different languages that show how to use the API operations. Languages supported include Curl, Ruby, Python, Java, Go, and Node.js.","title":"API Product Managers"},{"location":"patterns/api_mgt/index.html#testing","text":"Besides the small individual tests that you can run from the Developer Portal to understand how an API works, as explained above and demonstrated in the Travelport demo, the demo also showed how to use API Connect Test and Monitor to effortlessly generate and schedule API test assertions. This free browser based tool also provides dashboards for monitoring API test assertions and for receiving alerts on the health of your APIs. As the demo showed, you can use an HTTP Client within API Connect Test and Monitor to generate a request to an API and get the expected results: You enter any required parameters and authorization token and press Send to send the request. API Connect Test and Monitor then shows you the response payload as a formatted JSON object: To generate a test, you then click Generate Test , and in the dialog that appears name the test and add it to a project. When you click the checkmark in the upper right corner of the dialog, API Connect Test and Monitor automatically generates test assertions based on the response payload. You can add, delete, or modify these assertions, or even add new ones from a broad selection: You can also reorder the assertions by dragging and dropping them: Although coding is not required, if you wish, you change the underlying code by clicking CODE to enter the code view: Once you have your test arranged the way you want it, you can run it to generate a test report, which highlights successes and failures:","title":"Testing"},{"location":"patterns/api_mgt/index.html#test-scheduling","text":"IBM API Connect Test and Monitor also lets you automate test scheduling at regular intervals. To do this, you first save any test you may have open and exit out of the composer. Then publish your test to prepare it for scheduling: Once your test is published, you can click Schedule to schedule regular test intervals. Test and Monitor allows you to tweak the test schedule as necessary. You can create a new run, name it, and choose the times you wish to run your tests: Clicking Save Run schedules the tests. Finally, you can access the dashboard to monitor the health of your API:","title":"Test Scheduling"},{"location":"patterns/api_mgt/index.html#graphql-apis","text":"GraphQL is a query language for APIs that gives an application client greater control over what data it retrieves in an API request when compared with a REST API request. IBM\u00ae API Connect enables you to create a GraphQL API proxy definition that proxies a backend GraphQL server, and to define rate limiting controls that reflect the amount of data that is returned from the server by a request to the GraphQL API. The Developer Portal also supports testing GraphQL APIs.","title":"GraphQL APIs"},{"location":"patterns/api_mgt/index.html#advantages-of-graphql-over-rest-apis","text":"GraphQL provides the following particular advantages over REST APIs: The application client can request only the data that it needs. For example, when retrieving bank account records, request only the account number and current balance for each account, but not the customer name and address details. With a REST API request, either the backend REST service must provide separate endpoints for different data subsets, or the application client must retrieve the complete records and then discard the unwanted data. The application client can retrieve multiple related resources in a single request. For example, a customer's bank account record might include an array that references other finance products that the customer holds. If an application client wants to retrieve the bank account details for a specific customer, and details of the other finance products for that customer, then with a REST API the client would first retrieve the bank account details, then make separate requests for each of the other products. A GraphQL API can be designed to allow the client to retrieve all this information in a single request. However, this flexibility presents rate limiting challenges, because two seemingly very similar requests might return vastly different amounts of data, and what might have required multiple REST API requests, each counting towards the rate limit, might require only a single GraphQL API request. It is important therefore that rate limiting controls are imposed that reflect the amount of data that is retrieved. API Connect extends the GraphQL standard by providing, in a GraphQL API definition, the ability to configure a range of settings that are used to calculate the complexity of a GraphQL request and an associated cost that counts towards the rate limit.","title":"Advantages of GraphQL over REST APIs"},{"location":"patterns/api_mgt/index.html#creating-a-graphql-proxy-api","text":"The Travelport demo also showed how API Connect supports GraphQL. To create a GraphQL proxy API from an existing GraphQL server, click Add , then API (from REST, GraphQL or SOAP) : Then select, From existing GraphQL service (GraphQL proxy) : Give it a name and complete the rest of the parameters, especially the GraphQL Server URL field which, of course, cannot be left blank. Notice that, once you enter the URL, API Connect will find the service and automatically detect the schema. You can click Next and API Connect will attempt to connect to the GraphQL server, introspect it, and pull in any objects (in this case, queries and operations) that it can find on that server: The gateway will sometimes provide warnings (17 in the example above), which are really just recommendations on how you can optimize the query. Clicking Next allows you to then activate the API for publishing: Finally, clicking Next shows a screen such as the one below with checkmarks: This shows that the gateway was able to connect to the GraphQL server and detect its schema, paths, and operations, validate them, and automatically populate the canvas with different policies, including security policies:","title":"Creating a GraphQL proxy API"},{"location":"patterns/api_mgt/index.html#further-readings","text":"IBM Redbook on Agile Integration A Demo of Event Endpoint Management - Cloud Pak for Integration","title":"Further Readings"},{"location":"patterns/cicd/index.html","text":"CI/CD Practices Travelport MVP Continuous Delivery Practices With IBM\u00ae Cloud Continuous Delivery, you can build, test, and deliver applications by using DevOps or DevSecOps practices and industry-leading tools. Continuous Delivery supports a wide variety of practices. There is no one-size-fits-all answer. The practices you employ can vary from one software delivery project to the next. The IBM\u00ae Cloud Garage Method is the IBM approach to rapidly deliver engaging applications. It combines continuous delivery with IBM Design Thinking, lean startup methodology, DevOps, and agile practices. Those practices are mainly focused on cloud native, but can benefit any software development effort. OpenShift Pipelines OpenShift Pipelines is a Continuous Integration / Continuous Delivery (CI/CD) solution based on the open source Tekton project. In the OpenShift platform, the Open Source Tekton project is known as OpenShift Pipelines , so both terms are often used interchangeably. The key objective of Tekton is to enable development teams to quickly create pipelines of activity from simple, repeatable steps. A unique characteristic of Tekton that differentiates it from previous CI/CD solutions is that Tekton steps execute within a container that is specifically created just for that task. Users can interact with OpenShift Pipelines using the web user interface, command line interface, and via a Visual Studio Code editor plugin. The command line access is a mixture of the OpenShift \u2018oc\u2019 command line utility and the \u2018tkn\u2019 command line for specific Tekton commands. The tkn and oc command line utilities can be downloaded from the OpenShift console web user interface. To do this, simply press the white circle containing a black question mark near your name on the top right corner and then select Command Line Tools: Cluster For the Rail project demo, we created a cluster in the IBM Cloud called tvpt-rail-dev , as follows: Pipelines Used in This Project We also created the following pipeline (labelled ci-pipeline ) within the namespace tvpt-pipelines : The idea is to build an application from source code using Maven and the OpenShift Source to Image (S2I) process. The steps in the pipeline we created are as follows: git-status-pending which tells Git that we are starting the build. clone-source-repo which clones the repository maven-package which does the Maven build. image-build-and-push which is a Docker image build and push operation git-kustomize-app is a custom step that takes all the changes from the build (e.g. new image, tags, properties, config maps, sealed secrets, updates, etc.) and anything that needs to be pushed to the Argo repo for Argo to deploy. argo-sync-health-check Once the pipeline pushes everything to the Argo repo in the previous step, this step deploys it to (syncs with) the cluster, and reports status back from Argo saying that everyting has been deployed and the health of the application is good. newman-integration-test This step starts the integration tests using the Postman collection. git-status-complete Finally, this completion task sends status back to Git saying that the build is completed. Use of Persistent Storage We also added persistent storage for the pipeline, in our case called pipeline-storage-claim : Having persistent storage allows us to cache and manage state between tasks in the pipeline. For example, for its build, Maven needs all the repositories from the project dependencies. Having persistent storage allows us to maintain a cache of those repositories so that we don't have to download them every time we build. Since each step runs in an isolated container any data that is created by a step for use by another step must be stored appropriately. If the data is accessed by a subsequent step within the same task then it is possible to use the /workspace directory to hold any created files and directories. A further option for steps within the same task is to use an emptyDir storage mechanism which can be useful for separating different data content for ease of use. If file stored data is to be accessed by a subsequent step that is in a different task, then a Kubernetes persistent volume claim is required to be used. The mechanism for adding storage to a step is called a volumeMount , as described further below. In our case, a persistent volume claim called pipeline-storage-claim is mounted into the step at a specifc path. Other steps within the task and within other tasks of the pipeline can also mount this volume and reuse any data placed there by this step. Note that the path used is where the Buildah command expects to find a local image repository. As a result any steps that invoke a Buildah command will mount this volume at this location. Buildah is a tool that facilitates building Open Container Initiative (OCI) container images. The Buildah package provides a command line tool that can be used to create a container from scratch or using an image as a starting point. When to use persistent storage: Your data must still be available, even if the container, the worker node, or the cluster is removed. You should use persistent storage in the following scenarios: Stateful apps Core business data Data that must be available due to legal requirements, such as a defined retention period Auditing Data that must be accessed and shared across app instances. For example: Access across pods : When you use Kubernetes persistent volumes to access your storage, you can determine the number of pods that can mount the volume at the same time. Some storage solutions, such as block storage, can be accessed by one pod at a time only. With other storage solutions, you can share volume across multiple pods. Access across zones and regions : You might require your data to be accessible across zones or regions. Some storage solutions, such as file and block storage, are data center-specific and cannot be shared across zones in a multizone cluster setup. Pipeline Listener Our pipeline listener, el-ci-event-listener , exposes the endpoints for triggering pipeline execution from a webhook: Tasks The fundamental resource of the Tekton process is the task , which contains at least one step to be executed and performs a useful function. Tasks are normally organized into an ordered execution set using a pipeline resource. Pipelines execute tasks in parallel unless a task is directed to run after the completion of another task. This facilitates parallel execution of build / test / deploy activities and is a useful characteristic that guides the user in the grouping of steps within tasks. These are the reusable tasks we have created for the Travelport demo: Structure of a Pipeline A pipelineRun resource invokes the execution of a pipeline. This allows specific properties and resources to be used as inputs to the pipeline process, such that the steps within the tasks are configured for the requirements of the user or environment. Here is the breakdown of the parts that make up a pipeline run: The pipelineRun invokes the pipeline, which contains tasks. Each task consists of a number of steps, each of which can contain elements such as command, script, volumeMounts, workingDir, parameters, resources, workspace, or image. command The command element specifies the command to be executed, which can be a sequence of a command and arguments. script Alternatively, you can use a script which can be useful if a single step is required to perform a number of command line operations. Below is an example from the Travelport demo that uses a script (for the mvn-settings task) and a command (for the mvn-goals task). These two tasks make up the maven-package step, responsible for the maven build: steps : - image : 'registry.access.redhat.com/ubi8/ubi-minimal:latest' name : mvn-settings resources : {} script : > #!/usr/bin/env bash [[ -f $(workspaces.maven-settings.path)/settings.xml ]] && \\ echo 'using existing $(workspaces.maven-settings.path)/settings.xml' && exit 0 cat > $(workspaces.maven-settings.path)/settings.xml <<EOF <settings> <mirrors> <!-- The mirrors added here are generated from environment variables. Don't change. --> <!-- ### mirrors from ENV ### --> </mirrors> <proxies> <!-- The proxies added here are generated from environment variables. Don't change. --> <!-- ### HTTP proxy from ENV ### --> </proxies> </settings> EOF xml=\"\" if [ -n \"$(params.PROXY_HOST)\" -a -n \"$(params.PROXY_PORT)\" ]; then xml=\"<proxy>\\ <id>genproxy</id>\\ <active>true</active>\\ <protocol>$(params.PROXY_PROTOCOL)</protocol>\\ <host>$(params.PROXY_HOST)</host>\\ <port>$(params.PROXY_PORT)</port>\" if [ -n \"$(params.PROXY_USER)\" -a -n \"$(params.PROXY_PASSWORD)\" ]; then xml=\"$xml\\ <username>$(params.PROXY_USER)</username>\\ <password>$(params.PROXY_PASSWORD)</password>\" fi if [ -n \"$(params.PROXY_NON_PROXY_HOSTS)\" ]; then xml=\"$xml\\ <nonProxyHosts>$(params.PROXY_NON_PROXY_HOSTS)</nonProxyHosts>\" fi xml=\"$xml\\ </proxy>\" sed -i \"s|<!-- ### HTTP proxy from ENV ### -->|$xml|\" $(workspaces.maven-settings.path)/settings.xml fi if [ -n \"$(params.MAVEN_MIRROR_URL)\" ]; then xml=\" <mirror>\\ <id>mirror.default</id>\\ <url>$(params.MAVEN_MIRROR_URL)</url>\\ <mirrorOf>central</mirrorOf>\\ </mirror>\" sed -i \"s|<!-- ### mirrors from ENV ### -->|$xml|\" $(workspaces.maven-settings.path)/settings.xml fi securityContext : privileged : true - args : - '-s' - $(workspaces.maven-settings.path)/settings.xml - $(params.GOALS) command : - /usr/bin/mvn image : $(params.MAVEN_IMAGE) name : mvn-goals resources : {} securityContext : privileged : true workingDir : $(workspaces.source.path)/$(params.SUB_PATH) volumeMounts volumeMounts allow you to add storage to a step. Since each step runs in an isolated container, any data that is created by a step for use by another step must be stored appropriately. If the data is accessed by a subsequent step within the same task then it is possible to use the /workspace directory to hold any created files and directories. A further option for steps within the same task is to use an emptyDir storage mechanism which can be useful for separating out different data content for ease of use. If file stored data is to be accessed by a subsequent step that is in a different task then a Kubernetes persistent volume claim is required to be used. As explained below, this is what we do for the Travelport demo. Note that volumes are defined in a section of the task outside the scope of any steps, and then each step that needs the volume will mount it. workingDir The workingDir element refers to the path within the container that should be the current working directory when the command is executed. parameters As with volumeMounts, parameters are defined outside the scope of any step within a task and then they are referenced from within the step. Parameters in this case refers to any information in text form required by a step such as a path, a name of an object, a username etc. The example below shows the parameters used in the Buildah task, which builds source into a container image and then pushes it to a container registry: params : - description : Reference of the image buildah will produce. name : IMAGE type : string - default : 'quay.io/buildah/stable:v1.17.0' description : The location of the buildah builder image. name : BUILDER_IMAGE type : string - default : overlay description : Set buildah storage driver name : STORAGE_DRIVER type : string - default : ./Dockerfile description : Path to the Dockerfile to build. name : DOCKERFILE type : string - default : . description : Path to the directory to use as context. name : CONTEXT type : string - default : 'true' description : >- Verify the TLS on the registry endpoint (for push/pull to a non-TLS registry) name : TLSVERIFY type : string - default : oci description : 'The format of the built container, oci or docker' name : FORMAT type : string - default : '' description : Extra parameters passed for the build command when building images. name : BUILD_EXTRA_ARGS type : string - default : '' description : Extra parameters passed for the push command when pushing images. name : PUSH_EXTRA_ARGS type : string - default : '' description : subpath for build artifacts. name : SUB_PATH type : string resources A reference to the resource is declared within the task and then the steps use the resources in commands. A resource can be used as an output in a step within the task. In Tekton, there is no explicit Git pull command. Simply including a Git resource in a task definition will result in a Git pull action taking place, before any steps execute, which will pull the content of the Git repository to a location of /workspace/<git-resource-name> . In the example below the Git repository content is pulled to /workspace/source . kind : Task resources : inputs : - name : source type : git outputs : - name : intermediate-image type : image steps : - name : build command : - buildah - bud - '-t' - $(resources.outputs.intermediate-image.url) Resources may reference either an image or a Git repository and the resource entity is defined in a separate YAML file. Image resources may be defined as either input or output resources depending on whether an existing image is to be consumed by a step or whether the image is to be created by a step. Workspace A workspace is similar to a volume in that it provides storage that can be shared across multiple tasks. A persistent volume claim is required to be created first and then the intent to use the volume is declared within the pipeline and task before mapping the workspace into an individual step such that it is mounted. Workspaces and volumes are similar in behavior but are defined in slightly different places. Image Since each Tekton step runs within its own image, the image must be referenced as shown in the example below: steps : - name : build command : - buildah - bud - '-t' - $(resources.outputs.intermediate-image.url) image : registry.redhat.io/rhel8/buildah Developer Perspective The OpenShift Console provides an Administrator and a Developer perspective. With the correct user access, the Administrator perspective lets you manage workload storage, networking, cluster settings, and more. The Developer perspective lets you build applications and associated components and services, define how they work together, monitor their health, get application metrics, etc. over time. Argo CD To implement our GitOps workflow, we used Argo CD, the GitOps continuous delivery tool for Kubernetes. Argo CD is found in the OpenShift GitOps project. If you go to the Developer's Perspective, you can see a topology: OpenShift GitOps is an OpenShift add-on which provides Argo CD and other tooling to enable teams to implement GitOps workflows for cluster configuration and application delivery. OpenShift GitOps is available as an operator in the OperatorHub and can be installed with a simple one-click experience. Clicking the Argo Server node that contains the URL takes you to the Argo login page: Since OpenShift OAuth is enabled, if you are already logged in to OpenShift, you do not need to provide authentication. Clicking the LOG IN VIA OPENSHIFT button directly takes you to the ArgoCD main screen: As you can see in the figure above, we have a few running applications for the Travelport MVP, including: tvpt-argocd - the root application which manages all the other apps tvpt-preprod-infra , to manage post configuration for the cluster, such as logDNA, sealed secrets, etc.; in other words, a central place to manage the infrastructure configurations for an environment, in this case pre-prod. tvpt-preprod-apps tvpt-prod-apps For demonstration purposes, we are assuming a pre-prod and prod environment. Both environments are in the same cluster, with only the applications, pre-prod and prod, isolated by project names (namespaces). If you click on an environment, for example, pre-prod, you will get by default a tree view of the applications within that environment, as shown below: So, for example, as you can see above, we have rail-shop-atomic and rail-shop-domain , etc. If you click the network view, as shown below, you will see only the the running applications (the services and pods). Finally, you also have a list view available: Similarly, if you go to the production environment, you will see the production apps: In Tree view mode: Network view mode: where you can see three instances of the application running, for handling additional load. ...and List view: This shows that you can have different configurations of the same application running in different environments. Repositories for Travelport Demo We created two repositiories for the Argo GitOps in the Travelport Demo: * tvpt-app-config * tvpt-infra-config This, of course, can be expanded further as necessary. As mentioned earlier, tvpt-infra... holds infrastructure configuration information. If you peek into the code, you will see the following: The bootstrap folder is about deploying Argo itself. Once you have Argo up and running, this folder will contain all of the applications, projects, etc. and all of the stuff you need to configure with argo. You also have the infrastructure folder: within the base folder you have kubeseal , for example, to enter secrets and credential information, logDNA for logDNA configuration, and tekton for managing the Tekton pipelines. The overlays folder looks like this: Command Line Tools to Download OpenShift Command Line Interface (CLI) The OpenShift CLI allows you to create applications and manage OpenShift projects from a terminal. The oc binary offers the same capabilities as the kubectl binary, but it is further extended to natively support OpenShift Container Platform features. More to be added here Source-to-Image (S2I) Capability The Red Hat OpenShift \u2018Source to Image\u2019 (S2I) build process allows you to point OpenShift at a Git source repository and OpenShift will perform the following tasks: Examine the source code in the repository and identify the language used Select a builder image for the identified language from the OpenShift image repository Create an instance of the builder image from the image repository (green arrow on figure 1) Clone the source code in the builder image and build the application (the grey box in figure 1). The entire build process including pulling in any dependencies takes place within the builder image. When the build is complete push the builder image to the OpenShift image repository (the blue arrow on figure 1). Create an instance of the builder image, with the built application, and execute the container in a pod (the purple arrow in figure 1). OpenShift Clients The OpenShift client oc simplifies working with Kubernetes and OpenShift clusters, offering a number of advantages over kubectl such as easy login, kube config file management, and access to developer tools. The kubectl binary is included alongside for when strict Kubernetes compliance is necessary. To learn more about OpenShift, visit docs.openshift.com and select the version of OpenShift you are using. Installing the tools After extracting this archive, move the oc and kubectl binaries to a location on your PATH such as /usr/local/bin . Then run: oc login [ API_URL ] to start a session against an OpenShift cluster. After login, run oc and oc help to learn more about how to get started with OpenShift. License OpenShift is licensed under the Apache Public License 2.0. The source code for this program is located on github . \u2192 Reference Information OpenShift Blog OpenShift Documentation","title":"CICD for Travelport"},{"location":"patterns/cicd/index.html#cicd-practices","text":"","title":"CI/CD Practices"},{"location":"patterns/cicd/index.html#travelport-mvp","text":"","title":"Travelport MVP"},{"location":"patterns/cicd/index.html#continuous-delivery-practices","text":"With IBM\u00ae Cloud Continuous Delivery, you can build, test, and deliver applications by using DevOps or DevSecOps practices and industry-leading tools. Continuous Delivery supports a wide variety of practices. There is no one-size-fits-all answer. The practices you employ can vary from one software delivery project to the next. The IBM\u00ae Cloud Garage Method is the IBM approach to rapidly deliver engaging applications. It combines continuous delivery with IBM Design Thinking, lean startup methodology, DevOps, and agile practices. Those practices are mainly focused on cloud native, but can benefit any software development effort.","title":"Continuous Delivery Practices"},{"location":"patterns/cicd/index.html#openshift-pipelines","text":"OpenShift Pipelines is a Continuous Integration / Continuous Delivery (CI/CD) solution based on the open source Tekton project. In the OpenShift platform, the Open Source Tekton project is known as OpenShift Pipelines , so both terms are often used interchangeably. The key objective of Tekton is to enable development teams to quickly create pipelines of activity from simple, repeatable steps. A unique characteristic of Tekton that differentiates it from previous CI/CD solutions is that Tekton steps execute within a container that is specifically created just for that task. Users can interact with OpenShift Pipelines using the web user interface, command line interface, and via a Visual Studio Code editor plugin. The command line access is a mixture of the OpenShift \u2018oc\u2019 command line utility and the \u2018tkn\u2019 command line for specific Tekton commands. The tkn and oc command line utilities can be downloaded from the OpenShift console web user interface. To do this, simply press the white circle containing a black question mark near your name on the top right corner and then select Command Line Tools:","title":"OpenShift Pipelines"},{"location":"patterns/cicd/index.html#cluster","text":"For the Rail project demo, we created a cluster in the IBM Cloud called tvpt-rail-dev , as follows:","title":"Cluster"},{"location":"patterns/cicd/index.html#pipelines-used-in-this-project","text":"We also created the following pipeline (labelled ci-pipeline ) within the namespace tvpt-pipelines : The idea is to build an application from source code using Maven and the OpenShift Source to Image (S2I) process. The steps in the pipeline we created are as follows: git-status-pending which tells Git that we are starting the build. clone-source-repo which clones the repository maven-package which does the Maven build. image-build-and-push which is a Docker image build and push operation git-kustomize-app is a custom step that takes all the changes from the build (e.g. new image, tags, properties, config maps, sealed secrets, updates, etc.) and anything that needs to be pushed to the Argo repo for Argo to deploy. argo-sync-health-check Once the pipeline pushes everything to the Argo repo in the previous step, this step deploys it to (syncs with) the cluster, and reports status back from Argo saying that everyting has been deployed and the health of the application is good. newman-integration-test This step starts the integration tests using the Postman collection. git-status-complete Finally, this completion task sends status back to Git saying that the build is completed.","title":"Pipelines Used in This Project"},{"location":"patterns/cicd/index.html#use-of-persistent-storage","text":"We also added persistent storage for the pipeline, in our case called pipeline-storage-claim : Having persistent storage allows us to cache and manage state between tasks in the pipeline. For example, for its build, Maven needs all the repositories from the project dependencies. Having persistent storage allows us to maintain a cache of those repositories so that we don't have to download them every time we build. Since each step runs in an isolated container any data that is created by a step for use by another step must be stored appropriately. If the data is accessed by a subsequent step within the same task then it is possible to use the /workspace directory to hold any created files and directories. A further option for steps within the same task is to use an emptyDir storage mechanism which can be useful for separating different data content for ease of use. If file stored data is to be accessed by a subsequent step that is in a different task, then a Kubernetes persistent volume claim is required to be used. The mechanism for adding storage to a step is called a volumeMount , as described further below. In our case, a persistent volume claim called pipeline-storage-claim is mounted into the step at a specifc path. Other steps within the task and within other tasks of the pipeline can also mount this volume and reuse any data placed there by this step. Note that the path used is where the Buildah command expects to find a local image repository. As a result any steps that invoke a Buildah command will mount this volume at this location. Buildah is a tool that facilitates building Open Container Initiative (OCI) container images. The Buildah package provides a command line tool that can be used to create a container from scratch or using an image as a starting point.","title":"Use of Persistent Storage"},{"location":"patterns/cicd/index.html#when-to-use-persistent-storage","text":"Your data must still be available, even if the container, the worker node, or the cluster is removed. You should use persistent storage in the following scenarios: Stateful apps Core business data Data that must be available due to legal requirements, such as a defined retention period Auditing Data that must be accessed and shared across app instances. For example: Access across pods : When you use Kubernetes persistent volumes to access your storage, you can determine the number of pods that can mount the volume at the same time. Some storage solutions, such as block storage, can be accessed by one pod at a time only. With other storage solutions, you can share volume across multiple pods. Access across zones and regions : You might require your data to be accessible across zones or regions. Some storage solutions, such as file and block storage, are data center-specific and cannot be shared across zones in a multizone cluster setup.","title":"When to use persistent storage:"},{"location":"patterns/cicd/index.html#pipeline-listener","text":"Our pipeline listener, el-ci-event-listener , exposes the endpoints for triggering pipeline execution from a webhook:","title":"Pipeline Listener"},{"location":"patterns/cicd/index.html#tasks","text":"The fundamental resource of the Tekton process is the task , which contains at least one step to be executed and performs a useful function. Tasks are normally organized into an ordered execution set using a pipeline resource. Pipelines execute tasks in parallel unless a task is directed to run after the completion of another task. This facilitates parallel execution of build / test / deploy activities and is a useful characteristic that guides the user in the grouping of steps within tasks. These are the reusable tasks we have created for the Travelport demo:","title":"Tasks"},{"location":"patterns/cicd/index.html#structure-of-a-pipeline","text":"A pipelineRun resource invokes the execution of a pipeline. This allows specific properties and resources to be used as inputs to the pipeline process, such that the steps within the tasks are configured for the requirements of the user or environment. Here is the breakdown of the parts that make up a pipeline run: The pipelineRun invokes the pipeline, which contains tasks. Each task consists of a number of steps, each of which can contain elements such as command, script, volumeMounts, workingDir, parameters, resources, workspace, or image.","title":"Structure of a Pipeline"},{"location":"patterns/cicd/index.html#command","text":"The command element specifies the command to be executed, which can be a sequence of a command and arguments.","title":"command"},{"location":"patterns/cicd/index.html#script","text":"Alternatively, you can use a script which can be useful if a single step is required to perform a number of command line operations. Below is an example from the Travelport demo that uses a script (for the mvn-settings task) and a command (for the mvn-goals task). These two tasks make up the maven-package step, responsible for the maven build: steps : - image : 'registry.access.redhat.com/ubi8/ubi-minimal:latest' name : mvn-settings resources : {} script : > #!/usr/bin/env bash [[ -f $(workspaces.maven-settings.path)/settings.xml ]] && \\ echo 'using existing $(workspaces.maven-settings.path)/settings.xml' && exit 0 cat > $(workspaces.maven-settings.path)/settings.xml <<EOF <settings> <mirrors> <!-- The mirrors added here are generated from environment variables. Don't change. --> <!-- ### mirrors from ENV ### --> </mirrors> <proxies> <!-- The proxies added here are generated from environment variables. Don't change. --> <!-- ### HTTP proxy from ENV ### --> </proxies> </settings> EOF xml=\"\" if [ -n \"$(params.PROXY_HOST)\" -a -n \"$(params.PROXY_PORT)\" ]; then xml=\"<proxy>\\ <id>genproxy</id>\\ <active>true</active>\\ <protocol>$(params.PROXY_PROTOCOL)</protocol>\\ <host>$(params.PROXY_HOST)</host>\\ <port>$(params.PROXY_PORT)</port>\" if [ -n \"$(params.PROXY_USER)\" -a -n \"$(params.PROXY_PASSWORD)\" ]; then xml=\"$xml\\ <username>$(params.PROXY_USER)</username>\\ <password>$(params.PROXY_PASSWORD)</password>\" fi if [ -n \"$(params.PROXY_NON_PROXY_HOSTS)\" ]; then xml=\"$xml\\ <nonProxyHosts>$(params.PROXY_NON_PROXY_HOSTS)</nonProxyHosts>\" fi xml=\"$xml\\ </proxy>\" sed -i \"s|<!-- ### HTTP proxy from ENV ### -->|$xml|\" $(workspaces.maven-settings.path)/settings.xml fi if [ -n \"$(params.MAVEN_MIRROR_URL)\" ]; then xml=\" <mirror>\\ <id>mirror.default</id>\\ <url>$(params.MAVEN_MIRROR_URL)</url>\\ <mirrorOf>central</mirrorOf>\\ </mirror>\" sed -i \"s|<!-- ### mirrors from ENV ### -->|$xml|\" $(workspaces.maven-settings.path)/settings.xml fi securityContext : privileged : true - args : - '-s' - $(workspaces.maven-settings.path)/settings.xml - $(params.GOALS) command : - /usr/bin/mvn image : $(params.MAVEN_IMAGE) name : mvn-goals resources : {} securityContext : privileged : true workingDir : $(workspaces.source.path)/$(params.SUB_PATH)","title":"script"},{"location":"patterns/cicd/index.html#volumemounts","text":"volumeMounts allow you to add storage to a step. Since each step runs in an isolated container, any data that is created by a step for use by another step must be stored appropriately. If the data is accessed by a subsequent step within the same task then it is possible to use the /workspace directory to hold any created files and directories. A further option for steps within the same task is to use an emptyDir storage mechanism which can be useful for separating out different data content for ease of use. If file stored data is to be accessed by a subsequent step that is in a different task then a Kubernetes persistent volume claim is required to be used. As explained below, this is what we do for the Travelport demo. Note that volumes are defined in a section of the task outside the scope of any steps, and then each step that needs the volume will mount it.","title":"volumeMounts"},{"location":"patterns/cicd/index.html#workingdir","text":"The workingDir element refers to the path within the container that should be the current working directory when the command is executed.","title":"workingDir"},{"location":"patterns/cicd/index.html#parameters","text":"As with volumeMounts, parameters are defined outside the scope of any step within a task and then they are referenced from within the step. Parameters in this case refers to any information in text form required by a step such as a path, a name of an object, a username etc. The example below shows the parameters used in the Buildah task, which builds source into a container image and then pushes it to a container registry: params : - description : Reference of the image buildah will produce. name : IMAGE type : string - default : 'quay.io/buildah/stable:v1.17.0' description : The location of the buildah builder image. name : BUILDER_IMAGE type : string - default : overlay description : Set buildah storage driver name : STORAGE_DRIVER type : string - default : ./Dockerfile description : Path to the Dockerfile to build. name : DOCKERFILE type : string - default : . description : Path to the directory to use as context. name : CONTEXT type : string - default : 'true' description : >- Verify the TLS on the registry endpoint (for push/pull to a non-TLS registry) name : TLSVERIFY type : string - default : oci description : 'The format of the built container, oci or docker' name : FORMAT type : string - default : '' description : Extra parameters passed for the build command when building images. name : BUILD_EXTRA_ARGS type : string - default : '' description : Extra parameters passed for the push command when pushing images. name : PUSH_EXTRA_ARGS type : string - default : '' description : subpath for build artifacts. name : SUB_PATH type : string","title":"parameters"},{"location":"patterns/cicd/index.html#resources","text":"A reference to the resource is declared within the task and then the steps use the resources in commands. A resource can be used as an output in a step within the task. In Tekton, there is no explicit Git pull command. Simply including a Git resource in a task definition will result in a Git pull action taking place, before any steps execute, which will pull the content of the Git repository to a location of /workspace/<git-resource-name> . In the example below the Git repository content is pulled to /workspace/source . kind : Task resources : inputs : - name : source type : git outputs : - name : intermediate-image type : image steps : - name : build command : - buildah - bud - '-t' - $(resources.outputs.intermediate-image.url) Resources may reference either an image or a Git repository and the resource entity is defined in a separate YAML file. Image resources may be defined as either input or output resources depending on whether an existing image is to be consumed by a step or whether the image is to be created by a step.","title":"resources"},{"location":"patterns/cicd/index.html#workspace","text":"A workspace is similar to a volume in that it provides storage that can be shared across multiple tasks. A persistent volume claim is required to be created first and then the intent to use the volume is declared within the pipeline and task before mapping the workspace into an individual step such that it is mounted. Workspaces and volumes are similar in behavior but are defined in slightly different places.","title":"Workspace"},{"location":"patterns/cicd/index.html#image","text":"Since each Tekton step runs within its own image, the image must be referenced as shown in the example below: steps : - name : build command : - buildah - bud - '-t' - $(resources.outputs.intermediate-image.url) image : registry.redhat.io/rhel8/buildah","title":"Image"},{"location":"patterns/cicd/index.html#developer-perspective","text":"The OpenShift Console provides an Administrator and a Developer perspective. With the correct user access, the Administrator perspective lets you manage workload storage, networking, cluster settings, and more. The Developer perspective lets you build applications and associated components and services, define how they work together, monitor their health, get application metrics, etc. over time.","title":"Developer Perspective"},{"location":"patterns/cicd/index.html#argo-cd","text":"To implement our GitOps workflow, we used Argo CD, the GitOps continuous delivery tool for Kubernetes. Argo CD is found in the OpenShift GitOps project. If you go to the Developer's Perspective, you can see a topology: OpenShift GitOps is an OpenShift add-on which provides Argo CD and other tooling to enable teams to implement GitOps workflows for cluster configuration and application delivery. OpenShift GitOps is available as an operator in the OperatorHub and can be installed with a simple one-click experience. Clicking the Argo Server node that contains the URL takes you to the Argo login page: Since OpenShift OAuth is enabled, if you are already logged in to OpenShift, you do not need to provide authentication. Clicking the LOG IN VIA OPENSHIFT button directly takes you to the ArgoCD main screen: As you can see in the figure above, we have a few running applications for the Travelport MVP, including: tvpt-argocd - the root application which manages all the other apps tvpt-preprod-infra , to manage post configuration for the cluster, such as logDNA, sealed secrets, etc.; in other words, a central place to manage the infrastructure configurations for an environment, in this case pre-prod. tvpt-preprod-apps tvpt-prod-apps For demonstration purposes, we are assuming a pre-prod and prod environment. Both environments are in the same cluster, with only the applications, pre-prod and prod, isolated by project names (namespaces). If you click on an environment, for example, pre-prod, you will get by default a tree view of the applications within that environment, as shown below: So, for example, as you can see above, we have rail-shop-atomic and rail-shop-domain , etc. If you click the network view, as shown below, you will see only the the running applications (the services and pods). Finally, you also have a list view available: Similarly, if you go to the production environment, you will see the production apps: In Tree view mode: Network view mode: where you can see three instances of the application running, for handling additional load. ...and List view: This shows that you can have different configurations of the same application running in different environments.","title":"Argo CD"},{"location":"patterns/cicd/index.html#repositories-for-travelport-demo","text":"We created two repositiories for the Argo GitOps in the Travelport Demo: * tvpt-app-config * tvpt-infra-config This, of course, can be expanded further as necessary. As mentioned earlier, tvpt-infra... holds infrastructure configuration information. If you peek into the code, you will see the following: The bootstrap folder is about deploying Argo itself. Once you have Argo up and running, this folder will contain all of the applications, projects, etc. and all of the stuff you need to configure with argo. You also have the infrastructure folder: within the base folder you have kubeseal , for example, to enter secrets and credential information, logDNA for logDNA configuration, and tekton for managing the Tekton pipelines. The overlays folder looks like this:","title":"Repositories for Travelport Demo"},{"location":"patterns/cicd/index.html#command-line-tools-to-download","text":"","title":"Command Line Tools to Download"},{"location":"patterns/cicd/index.html#openshift-command-line-interface-cli","text":"The OpenShift CLI allows you to create applications and manage OpenShift projects from a terminal. The oc binary offers the same capabilities as the kubectl binary, but it is further extended to natively support OpenShift Container Platform features. More to be added here","title":"OpenShift Command Line Interface (CLI)"},{"location":"patterns/cicd/index.html#source-to-image-s2i-capability","text":"The Red Hat OpenShift \u2018Source to Image\u2019 (S2I) build process allows you to point OpenShift at a Git source repository and OpenShift will perform the following tasks: Examine the source code in the repository and identify the language used Select a builder image for the identified language from the OpenShift image repository Create an instance of the builder image from the image repository (green arrow on figure 1) Clone the source code in the builder image and build the application (the grey box in figure 1). The entire build process including pulling in any dependencies takes place within the builder image. When the build is complete push the builder image to the OpenShift image repository (the blue arrow on figure 1). Create an instance of the builder image, with the built application, and execute the container in a pod (the purple arrow in figure 1).","title":"Source-to-Image (S2I) Capability"},{"location":"patterns/cicd/index.html#openshift-clients","text":"The OpenShift client oc simplifies working with Kubernetes and OpenShift clusters, offering a number of advantages over kubectl such as easy login, kube config file management, and access to developer tools. The kubectl binary is included alongside for when strict Kubernetes compliance is necessary. To learn more about OpenShift, visit docs.openshift.com and select the version of OpenShift you are using.","title":"OpenShift Clients"},{"location":"patterns/cicd/index.html#installing-the-tools","text":"After extracting this archive, move the oc and kubectl binaries to a location on your PATH such as /usr/local/bin . Then run: oc login [ API_URL ] to start a session against an OpenShift cluster. After login, run oc and oc help to learn more about how to get started with OpenShift.","title":"Installing the tools"},{"location":"patterns/cicd/index.html#license","text":"OpenShift is licensed under the Apache Public License 2.0. The source code for this program is located on github . \u2192","title":"License"},{"location":"patterns/cicd/index.html#reference-information","text":"OpenShift Blog OpenShift Documentation","title":"Reference Information"},{"location":"patterns/cqrs/index.html","text":"Command-Query Responsibility Segregation (CQRS) Problems and Constraints A domain model encapsulates domain data with the behavior for maintaining the correctness of that data as it is modified, structuring the data based on how it is stored in the database and to facilitate managing the data. Multiple clients can independently update the data concurrently. Different clients may not use the data the way the domain model structures it, and may not agree with each other on how it should be structured. When a domain model becomes overburdened managing complex aggregate objects, concurrent updates, and numerous cross-cutting views, how can it be refactored to separate different aspects of how the data is used? An application accesses data both to read and to modify it. The primitive data tasks are often expressed as create, read, update, and delete (CRUD); using them is known as CRUDing the data. Application code often does not make much distinction between the tasks; individual operations may mix reading the data with changing the data as needed. This simple approach works well when all clients of the data can use the same structure and contention is low. A single domain model can manage the data, make it accessible as domain objects, and ensure updates maintain its consistency. However, this approach becomes inadequate when different clients want different views across multiple sets of data, when the data is too widely used, and/or when multiple clients updating the data my unknowingly conflict with each other. For example, in a microservices architecture, each microservice should store and manage its own data, but a user interface may need to display data from several microservices. A query that gathers bits of data from multiple sources can be inefficient (time and bandwidth consumed accessing multiple data sources, CPU consumed transforming data, memory consumed by intermediate objects) and must be repeated each time the data is accessed. Another example is an enterprise database of record managing data required by multiple applications. It can become overloaded with too many clients needing too many connections to run too many threads performing too many transactions--such that the database becomes a performance bottleneck and can even crash. Another example is maintaining consistency of the data while clients concurrently make independent updates to the data. While each update may be consistent, they may conflict with each other. Database locking ensures that the updates don't change the same data concurrently, but doesn't ensure multiple independent changes result in a consistent data model. When data usage is more complex than a single domain model can facilitate, a more sophisticated approach is needed. Solution and Pattern Refactor a domain model to separate operations for querying data and operations for updating data so that they may be handled independently. The CQRS pattern strictly segregates operations that read data from operations that update data. An operation can read data (the R in CRUD) or can write data (the CUD in CRUD), but not both. This separation can make using data much more manageable in several respects. The read operations and the write operations are simpler to implement because their functionality is more finely focused. The operations can be developed independently, potentially by separate teams. The operations can be optimized independently and can evolve independently, following changing user requirements more easily. These optimized operations can scale better, perform better, and security can be applied more precisely. The full CQRS pattern uses separate read and write databases. In doing so, the pattern segregates not just the APIs for accessing data or the models for managing data, but even segregates the database itself into two, a read/write database that is effectively write-only and one or more read-only databases. The adoption of the pattern can be applied in phases, incrementally from existing code. To illustrate this, we will use four stages that could be used incrementally or a developer can go directly from stage 0 to 3 without considering the others: Stage 0: Typical application data access Stage 1: Separate read and write APIs Stage 2: Separate read and write models Stage 3: Separate read and write databases Typical application data access Before even beginning to apply the pattern, let\u2019s consider the typical app design for accessing data. This diagram shows an app with a domain model for accessing data persisted in a database of record, i.e. a single source of truth for that data. The domain model has an API that at a minimum enables clients to perform CRUD tasks on domain objects within the model. The domain model is an object representation of the database documents or records. It is comprised of domain objects that represent individual documents or records and the business logic for managing and using them. Domain-Driven Design (DDD) models these domain objects as entities \u2014\u201cobjects that have a distinct identity that runs through time and different representations\u201d\u2014and aggregates \u2014\u201ca cluster of domain objects that can be treated as a single unit\u201d; the aggregate root maintains the integrity of the aggregate as a whole. Ideally, the domain model\u2019s API should be more domain-specific than simply CRUDing of data. Instead, it should expose higher-level operations that represent business functionality like findCustomer() , placeOrder() , transferFunds() , and so on. These operations read and update data as needed, sometimes doing both in a single operation. They are correct as long as they fit the way the business works. Separate read and write APIs The first and most visible step in applying the CQRS pattern is splitting the CRUD API into separate read and write APIs. This diagram shows the same domain model as before, but its single CRUD API is split into retrieve and modify APIs. The two APIs share the existing domain model but split the behavior: Read : The retrieve API is used to read the existing state of the objects in the domain model without changing that state. The API treats the domain state as read only. Write : The modify API is used to change the objects in the domain model. Changes are made using CUD tasks: create new domain objects, update the state in existing ones, and delete ones that are no longer needed. The operations in this API do not return result values, they return success (ack or void) or failure (nak or throw an exception). The create operation might return the primary of key of the entity, which can be generated either by the domain model or in the data source. This separation of APIs is an application of the Command Query Separation (CQS) pattern, which says to clearly separate methods that change state from those that don\u2019t. To do so, each of an object\u2019s methods can be in one of two categories (but not both): Query : Returns a result. Does not change the system\u2019s state nor cause any side effects that change the state. Command (a.k.a. modifiers or mutators): Changes the state of a system. Does not return a value, just an indication of success or failure. With this approach, the domain model works the same and provides access to the data the same as before. What has changed is the API for using the domain model. Whereas a higher-level operation might previously have both changed the state of the application and returned a part of that state, now each such operation is redesigned to only do one or the other. When the domain model splits its API into read and write operations, clients using the API must likewise split their functionality into querying and updating functionality. Most new web based applications are based in the single page application, with components and services that use and encapsulate remote API. So this separation of backend API fits well with modern web applications. This stage depends on the domain model being able to implement both the retrieve and modify APIs. A single domain model requires the retrieve and modify behavior to have similar, corresponding implementations. For them to evolve independently, the two APIs will need to be implemented with separate read and write models. Separate read and write models The second step in applying the CQRS pattern is to split the domain model into separate read and write models. This doesn\u2019t just change the API for accessing domain functionality, it also changes the design of how that functionality is structured and implemented. This diagram shows that the domain model becomes the basis for a write model that handles changes to the domain objects, along with a separate read model used to access the state of the app. Naturally, the read model implements the retrieve API and the write model implements the modify API. Now, the application consists not only of separate APIs for querying and updating the domain objects, there\u2019s also separate business functionality for doing so. Both the read business functionality and the write business functionality share the same database. The write model is implemented by specializing the domain model to focus solely on maintaining the valid structure of domain objects when changing their state and by applying any business rules. Meanwhile, responsibility for returning domain objects is shifted to a separate read model. The read model defines data transfer objects (DTOs) designed specifically for the model to return just the data the client wants in a structure the client finds convenient. The read model knows how to gather the data used to populate the DTOs. DTOs encapsulate little if any domain functionality, they just bundle data into a convenient package that can easily be transmitted using a single method call, especially between processes. The read model should be able to implement the retrieve API by implementing the necessary queries and executing them. If the retrieve API is already built to return domain objects as results, the read model can continue to do so, or better yet, implements DTO types that are compatible with the domain objects and returns those. Likewise, the modify API was already implemented using the domain model, so the write model should preserve that. The write model may enhance the implementation to more explicitly implement a command interface or use command objects. This phase assumes that the read and write models can both be implemented using the same database of record the domain model has been using. To the extent this is true, the implementations of reading and writing can evolve independently and be optimized independently. This independence may become increasingly limited since they are both bound to the same database with a single schema or data model. To enable the read and write models to evolve independently, they may each need their own database. Separate read and write databases The third step in applying the CQRS pattern\u2014-which implements the complete CQRS pattern solution\u2014-is splitting the database of record into separate read and write databases. This diagram shows the write model and read model, each supported by its own database. The overall solution consists of two main parts: the write solution that supports updating the data and the read solution that supports querying the data. The two parts are connected by the event bus. The write model has its own read/write database and the read model has its own read-only database. The read/write database still serves as the database of record (the single source of truth for the data) but is mostly used write-only: mostly written to and rarely read. Reading is offloaded onto a separate read database that contains the same data but is used read-only. The query database is effectively a cache of the database of record, with all of the inherit benefits and complexity of the Caching pattern. The query database contains a copy of the data in the database of record, with the copy structured and staged for easier access by the clients using the retrieve API. As a copy, overhead is needed to keep the copy synchronized with changes in the original. Latency in this synchronization process creates eventual consistency, during which the data copy is stale. The separate databases enable the separate read and write models and their respective retrieve and modify APIs to truly evolve independently. Not only can the read model or write model\u2019s implementation change without changing the other, but how each stores its data can be changed independently. This solution offers the following advantages: Scaling : The query load is moved from the write database to the read database. If the database of record is a scalability bottleneck and a lot of the load on it is caused by queries, unloading those query responsibilities can significantly improve the scalability of the combined data access. Performance : The schemas of the two databases can be different, enabling them to be designed and optimized independently for better performance. The write database can be optimized for data consistency and correctness, with capabilities such as stored procedures that fit the write model and assist with data updates. The read database can store the data in units that better fit the read model and are better optimized for querying, with larger rows requiring fewer joins. Notice that the design for this stage is significantly more complex than the design for the previous stage. Separate databases with copies of the same data may make data modeling and using data easier, but they require significant overhead to synchronize the data and keep the copies consistent. CQRS employs a couple of design features that support keeping the databases synchronized: Command Bus for queuing commands (optional): A more subtle and optional design decision is to queue the commands produced by the modify API, shown in the diagram as the command bus. This can significantly increase the throughput of multiple apps updating the database, as well as serialize updates to help avoid--or at least detect--merge conflicts. With the bus, a client making an update does not block synchronously while the change is written to the database. Rather, the request to change the database is captured as a command ( Design Patterns ) and put on a message queue, after which the client can proceed with other work. Asynchronously in the background, the write model processes the commands at the maximum sustainable rate that the database can handle, without the database ever becoming overloaded. If the database becomes temporarily unavailable, the commands queue and will be processed when the database becomes available once more. Event Bus for publishing update events (required): Whenever the write database is updated, a change notification is published as an event on the event bus. Interested parties can subscribe to the event bus to be notified when the database is updated. One such party is an event processor for the query database, which receives update events and processes them by updating the query database accordingly. In this way, every time the write database is updated, a corresponding update is made to the read database to keep it in sync. The connection between the command bus and the event bus is facilitated by an application of the Event Sourcing pattern , which keeps a change log that is suitable for publishing. Event sourcing maintains not only the current state of the data but also the history of how that current state was reached. For each command on the command bus, the write model performs these tasks to process the command: Logs the change Updates the database with the change Creates an update event describing the change and publishes it to the event bus The changes that are logged can be the commands from the command bus or the update events published to the event bus Considerations Keep these decisions in mind while applying this pattern: Client impact : Applying CQRS not only changes how data is stored and accessed, but also changes the APIs that clients use to access data. This means that each client must be redesigned to use the new APIs. Riskiness : A lot of the complexity of the pattern solution involves duplicating the data in two databases and keeping them synchronized. Risk comes from querying data that is stale or downright wrong because of problems with the synchronization. Eventual consistency : Clients querying data must expect that updates will have latency. In a microservices architecture, eventual data consistency is a given and acceptable in many of cases. Command queuing : Using a command bus as part of the write solution to queue the commands is optional but powerful. In addition to the benefits of queuing, the command objects can easily be stored in the change log and easily be converted into notification events. (In the next section, we illustrate a way to use event bus to queue commands as well.) Change log : The log of changes to the database of record can be either the list of commands from the command bus or the list of event notifications published on the event bus. The Event Sourcing pattern assumes it\u2019s a log of events, but that pattern doesn\u2019t include the command bus. An event list may be easier to scan as a history, whereas a command list is easier to replay. Create keys : Strict interpretation of the Command Query Separation (CQS) pattern says that command operations do not have return types. A possible exception is commands that create data: An operation that creates a new record or document typically returns the key for accessing the new data, which is convenient for the client. However, if the create operation is invoked asynchronously by a command on a command bus, the write model will need to perform a callback on the client to return the key. Messaging queues and topics : While messaging is used to implement both the command bus and event bus, the two busses use messaging differently. The command bus guarantees exactly once delivery. The event bus broadcasts each event to all interested event processors. Query database persistence : The database of record is always persistent. The query database is a cache that can be a persistent cache or an in-memory cache. If the cache is in-memory and is lost, it must be rebuilt completely from the database of record. Security : Controls on reading data and updating data can be applied separately using the two parts of the solution. Combining event sourcing and CQRS The CQRS application pattern is frequently associated with event sourcing: when doing event sourcing and domain driven design, we event source the aggregates or root entities. Aggregate creates events that are persisted. On top of the simple create, update and read by ID operations, the business requirements want to perform complex queries that can't be answered by a single aggregate. By just using event sourcing to be able to respond to a query like \"what are the orders of a customer\", then we have to rebuild the history of all orders and filter per customer. It is a lot of computation. This is linked to the problem of having conflicting domain models between query and persistence. As introduced in previous section, creations and updates are done as state notification events (change of state), and are persisted in the event log/store. The following figure, presents two separate microservices, one supporting the write model, and multiple other supporting the queries: The query part is separate processes that consume change log events and build a projection for future queries. The \"write\" part may persist in SQL while the read may use document oriented database with strong indexing and query capabilities. Or use in-memory database, or distributed cache... They do not need to be in the same language. With CQRS and ES the projections are retroactives. New query equals implementing new projection and read the events from the beginning of time or the recent committed state and snapshot. Read and write models are strongly decoupled and can evolve independently. It is important to note that the 'Command' part can still handle simple queries, primary-key based, like get order by id, or queries that do not involve joins. The event backbone, use a pub/sub model, and Kafka is a good candidate as an implementation technology. With this structure, the Read model microservice will most likely consume events from multiple topics to build the data projection based on joining those data streams. A query, to assess if the cold-chain was respected on the fresh food order shipment, will go to the voyage, container metrics, and order to be able to answer this question. This is where CQRS shines. We can note that, we can separate the API definition and management in a API gateway. The shipment order microservice is implementing this pattern. Some implementation items to consider: Consistency (ensure the data constraints are respected for each data transaction): CQRS without event sourcing has the same consistency guarantees as the database used to persist data and events. With Event Sourcing the consistency could be different, one for the write model and one for the read model. On write model, strong consistency is important to ensure the current state of the system is correct, so it leverages transaction, lock and sharding. On read side, we need less consistency, as they mostly work on stale data. Locking data on the read operation is not reasonable. Scalability : Separating read and write as two different microservices allows for high availability. Caching at the read level can be used to increase performance response time, and can be deployed as multiple standalone instances (Pods in Kubernetes). It is also possible to separate the query implementations between different services. Functions as service / serverless are good technology choices to implement complex queries. Availability : The write model sacrifices consistency for availability. This is a fact. The read model is eventually consistent so high availability is possible. In case of failure the system disables the writing of data but still is able to read them as they are served by different databases and services. With CQRS, the write model can evolve over time without impacting the read model, as long as the event model doesn't change. The read model requires additional tables, but they are often simpler and easier to understand. CQRS results in an increased number of objects, with commands, operations, events,... and packaging in deployable components or containers. It adds potentially different type of data sources. It is more complex. Some challenges to always consider: How to support event structure version management? How much data to keep in the event store (history)? How to adopt data duplication which results to eventual data consistency?. The CQRS pattern was introduced by Greg Young , and described in Martin Fowler's work on microservices. As you can see in previous figure, as soon as we see two arrows from the same component, we have to ask ourselves how does it work: the write model has to persist Order in its own database and then sends OrderCreated event to the topic... Should those operations be atomic and controlled with transaction? We detail this in next section. Keeping the write model on Mainframe It is very important to note that the system of records and transaction processing is still easier to run on mainframe to support strong consistency. But with the move to cloud native development, it does not mean we have to move all the system of records to the cloud. Data pipelines can be put in place, but CQRS should help by keeping the write model on the current system of records and without impacting the current MIPS utilization move data in the eventual consistency world of the cloud native, distributed computing world. In the figure above, the write model follows the current transaction processing on the mainframe, change data capture push data to Event backbone for getting real time visibility into the distributed world. The read is the costly operation, dues to the joins to be done to build the projection views needed to expose data depending of the business use cases. This is more true with distributed microservices. All the write operations for the business entities kept in the mainframe's system of records are still done via the transaction. Reference data can be injected in one load job to topic and persisted in the event store so streaming applications can leverage them by joining with transactional data. The consistency challenges As introduced in the previous section, there is a potential problem of data inconsistency: once a command saves changes into the database, the consumers do not see the new or updated data until event notification completes processing. With traditional Java service, using JPA and JMS, the save and send operations can be part of the same XA transaction and both succeed or fail. With event sourcing pattern, the source of trust is the event source, which acts as a version control system, as shown in the diagram below. The steps for synchronizing changes to the data are: The write model creates the event and publishes it The consumer receives the event and extracts its payload The consumer updates its local datasource with the payload data If the consumer fails to process the update, it can persist the event to an error log Each error in the log can be replayed A command line interface replays an event via an admin API, which searches in the topic using this order id to replay the save operation This implementation causes a problem for the createOrder(order): string operation: The Order Service is supposed to return the new order complete with the order id that is a unique key, a key most likely created by the database. If updating the database fails, there is no new order yet and so no database key to use as the order ID. To avoid this problem, if the underlying technology supports assigning the new order's key, the service can generate the order ID and use that as the order's key in the database. It is important to clearly study the Kafka consumer API and the different parameters on how to support the read offset. We are addressing those implementation best practices in our consumer note. CQRS and Change Data Capture There are other ways to support this dual operations level: When using Kafka, Kafka Connect has the capability to subscribe to databases via JDBC, allowing to poll tables for updates and then produce events to Kafka. There is an open-source change data capture solution based on extracting change events from database transaction logs, Debezium that helps to respond to insert, update and delete operations on databases and generate events accordingly. It supports databases like MySQL, Postgres, MongoDB and others. Write the order to the database and in the same transaction write to an event table ( \"outbox pattern\" ). Then use a polling to get the events to send to Kafka from this event table and delete the row in the table once the event is sent. Use the Change Data Capture from the database transaction log and generate events from this log. The IBM Infosphere CDC product helps to implement this pattern. For more detail about this solution see this product tour . The CQRS implementation using CDC will look like in the following diagram: What is important to note is that the event needs to be flexible on the data payload. On the view side, updates to the view part need to be idempotent. Delay in the view There is a delay between the data persistence and the availability of the data in the Read model. For most business applications, it is perfectly acceptable. In web based data access most of the data are at stale. When there is a need for the client, calling the query operation, to know if the data is up-to-date, the service can define a versioning strategy. When the order data was entered in a form within a single page application, the \"create order\" operation should return the order with its unique key freshly created and the Single Page Application will have the last data. Here is an example of such operation: @POST public Response create ( OrderCreate dto ) { Order order = new Order ( UUID . randomUUID (). toString (), dto . getProductID (),...); // ... return Response . ok (). entity ( order ). build () } Schema change What to do when we need to add attribute to event?. So we need to create a versioninig schema for event structure. You need to use flexible schema like json schema, Apache Avro or protocol buffer and may be, add an event adapter (as a function?) to translate between the different event structures. Code reference The following project includes two sub modules, each deployable as a microservice to illustrate the command and query part: https://github.com/ibm-cloud-architecture/refarch-kc-order-ms Further readings https://www.codeproject.com/Articles/555855/Introduction-to-CQRS http://udidahan.com/2009/12/09/clarified-cqrs https://martinfowler.com/bliki/CQRS.html https://microservices.io/patterns/data/cqrs.html https://community.risingstack.com/when-to-use-cqrs https://dzone.com/articles/concepts-of-cqrs https://martinfowler.com/bliki/CommandQuerySeparation.html https://www.martinfowler.com/eaaCatalog/domainModel.html https://dddcommunity.org/learning-ddd/what_is_ddd/ https://martinfowler.com/bliki/EvansClassification.html https://martinfowler.com/bliki/DDD_Aggregate.html https://martinfowler.com/eaaCatalog/dataTransferObject.html https://en.wikipedia.org/wiki/Command_pattern https://www.pearson.com/us/higher-education/program/Gamma-Design-Patterns-Elements-of-Reusable-Object-Oriented-Software/PGM14333.html","title":"CQRS"},{"location":"patterns/cqrs/index.html#command-query-responsibility-segregation-cqrs","text":"","title":"Command-Query Responsibility Segregation (CQRS)"},{"location":"patterns/cqrs/index.html#problems-and-constraints","text":"A domain model encapsulates domain data with the behavior for maintaining the correctness of that data as it is modified, structuring the data based on how it is stored in the database and to facilitate managing the data. Multiple clients can independently update the data concurrently. Different clients may not use the data the way the domain model structures it, and may not agree with each other on how it should be structured. When a domain model becomes overburdened managing complex aggregate objects, concurrent updates, and numerous cross-cutting views, how can it be refactored to separate different aspects of how the data is used? An application accesses data both to read and to modify it. The primitive data tasks are often expressed as create, read, update, and delete (CRUD); using them is known as CRUDing the data. Application code often does not make much distinction between the tasks; individual operations may mix reading the data with changing the data as needed. This simple approach works well when all clients of the data can use the same structure and contention is low. A single domain model can manage the data, make it accessible as domain objects, and ensure updates maintain its consistency. However, this approach becomes inadequate when different clients want different views across multiple sets of data, when the data is too widely used, and/or when multiple clients updating the data my unknowingly conflict with each other. For example, in a microservices architecture, each microservice should store and manage its own data, but a user interface may need to display data from several microservices. A query that gathers bits of data from multiple sources can be inefficient (time and bandwidth consumed accessing multiple data sources, CPU consumed transforming data, memory consumed by intermediate objects) and must be repeated each time the data is accessed. Another example is an enterprise database of record managing data required by multiple applications. It can become overloaded with too many clients needing too many connections to run too many threads performing too many transactions--such that the database becomes a performance bottleneck and can even crash. Another example is maintaining consistency of the data while clients concurrently make independent updates to the data. While each update may be consistent, they may conflict with each other. Database locking ensures that the updates don't change the same data concurrently, but doesn't ensure multiple independent changes result in a consistent data model. When data usage is more complex than a single domain model can facilitate, a more sophisticated approach is needed.","title":"Problems and Constraints"},{"location":"patterns/cqrs/index.html#solution-and-pattern","text":"Refactor a domain model to separate operations for querying data and operations for updating data so that they may be handled independently. The CQRS pattern strictly segregates operations that read data from operations that update data. An operation can read data (the R in CRUD) or can write data (the CUD in CRUD), but not both. This separation can make using data much more manageable in several respects. The read operations and the write operations are simpler to implement because their functionality is more finely focused. The operations can be developed independently, potentially by separate teams. The operations can be optimized independently and can evolve independently, following changing user requirements more easily. These optimized operations can scale better, perform better, and security can be applied more precisely. The full CQRS pattern uses separate read and write databases. In doing so, the pattern segregates not just the APIs for accessing data or the models for managing data, but even segregates the database itself into two, a read/write database that is effectively write-only and one or more read-only databases. The adoption of the pattern can be applied in phases, incrementally from existing code. To illustrate this, we will use four stages that could be used incrementally or a developer can go directly from stage 0 to 3 without considering the others: Stage 0: Typical application data access Stage 1: Separate read and write APIs Stage 2: Separate read and write models Stage 3: Separate read and write databases","title":"Solution and Pattern"},{"location":"patterns/cqrs/index.html#typical-application-data-access","text":"Before even beginning to apply the pattern, let\u2019s consider the typical app design for accessing data. This diagram shows an app with a domain model for accessing data persisted in a database of record, i.e. a single source of truth for that data. The domain model has an API that at a minimum enables clients to perform CRUD tasks on domain objects within the model. The domain model is an object representation of the database documents or records. It is comprised of domain objects that represent individual documents or records and the business logic for managing and using them. Domain-Driven Design (DDD) models these domain objects as entities \u2014\u201cobjects that have a distinct identity that runs through time and different representations\u201d\u2014and aggregates \u2014\u201ca cluster of domain objects that can be treated as a single unit\u201d; the aggregate root maintains the integrity of the aggregate as a whole. Ideally, the domain model\u2019s API should be more domain-specific than simply CRUDing of data. Instead, it should expose higher-level operations that represent business functionality like findCustomer() , placeOrder() , transferFunds() , and so on. These operations read and update data as needed, sometimes doing both in a single operation. They are correct as long as they fit the way the business works.","title":"Typical application data access"},{"location":"patterns/cqrs/index.html#separate-read-and-write-apis","text":"The first and most visible step in applying the CQRS pattern is splitting the CRUD API into separate read and write APIs. This diagram shows the same domain model as before, but its single CRUD API is split into retrieve and modify APIs. The two APIs share the existing domain model but split the behavior: Read : The retrieve API is used to read the existing state of the objects in the domain model without changing that state. The API treats the domain state as read only. Write : The modify API is used to change the objects in the domain model. Changes are made using CUD tasks: create new domain objects, update the state in existing ones, and delete ones that are no longer needed. The operations in this API do not return result values, they return success (ack or void) or failure (nak or throw an exception). The create operation might return the primary of key of the entity, which can be generated either by the domain model or in the data source. This separation of APIs is an application of the Command Query Separation (CQS) pattern, which says to clearly separate methods that change state from those that don\u2019t. To do so, each of an object\u2019s methods can be in one of two categories (but not both): Query : Returns a result. Does not change the system\u2019s state nor cause any side effects that change the state. Command (a.k.a. modifiers or mutators): Changes the state of a system. Does not return a value, just an indication of success or failure. With this approach, the domain model works the same and provides access to the data the same as before. What has changed is the API for using the domain model. Whereas a higher-level operation might previously have both changed the state of the application and returned a part of that state, now each such operation is redesigned to only do one or the other. When the domain model splits its API into read and write operations, clients using the API must likewise split their functionality into querying and updating functionality. Most new web based applications are based in the single page application, with components and services that use and encapsulate remote API. So this separation of backend API fits well with modern web applications. This stage depends on the domain model being able to implement both the retrieve and modify APIs. A single domain model requires the retrieve and modify behavior to have similar, corresponding implementations. For them to evolve independently, the two APIs will need to be implemented with separate read and write models.","title":"Separate read and write APIs"},{"location":"patterns/cqrs/index.html#separate-read-and-write-models","text":"The second step in applying the CQRS pattern is to split the domain model into separate read and write models. This doesn\u2019t just change the API for accessing domain functionality, it also changes the design of how that functionality is structured and implemented. This diagram shows that the domain model becomes the basis for a write model that handles changes to the domain objects, along with a separate read model used to access the state of the app. Naturally, the read model implements the retrieve API and the write model implements the modify API. Now, the application consists not only of separate APIs for querying and updating the domain objects, there\u2019s also separate business functionality for doing so. Both the read business functionality and the write business functionality share the same database. The write model is implemented by specializing the domain model to focus solely on maintaining the valid structure of domain objects when changing their state and by applying any business rules. Meanwhile, responsibility for returning domain objects is shifted to a separate read model. The read model defines data transfer objects (DTOs) designed specifically for the model to return just the data the client wants in a structure the client finds convenient. The read model knows how to gather the data used to populate the DTOs. DTOs encapsulate little if any domain functionality, they just bundle data into a convenient package that can easily be transmitted using a single method call, especially between processes. The read model should be able to implement the retrieve API by implementing the necessary queries and executing them. If the retrieve API is already built to return domain objects as results, the read model can continue to do so, or better yet, implements DTO types that are compatible with the domain objects and returns those. Likewise, the modify API was already implemented using the domain model, so the write model should preserve that. The write model may enhance the implementation to more explicitly implement a command interface or use command objects. This phase assumes that the read and write models can both be implemented using the same database of record the domain model has been using. To the extent this is true, the implementations of reading and writing can evolve independently and be optimized independently. This independence may become increasingly limited since they are both bound to the same database with a single schema or data model. To enable the read and write models to evolve independently, they may each need their own database.","title":"Separate read and write models"},{"location":"patterns/cqrs/index.html#separate-read-and-write-databases","text":"The third step in applying the CQRS pattern\u2014-which implements the complete CQRS pattern solution\u2014-is splitting the database of record into separate read and write databases. This diagram shows the write model and read model, each supported by its own database. The overall solution consists of two main parts: the write solution that supports updating the data and the read solution that supports querying the data. The two parts are connected by the event bus. The write model has its own read/write database and the read model has its own read-only database. The read/write database still serves as the database of record (the single source of truth for the data) but is mostly used write-only: mostly written to and rarely read. Reading is offloaded onto a separate read database that contains the same data but is used read-only. The query database is effectively a cache of the database of record, with all of the inherit benefits and complexity of the Caching pattern. The query database contains a copy of the data in the database of record, with the copy structured and staged for easier access by the clients using the retrieve API. As a copy, overhead is needed to keep the copy synchronized with changes in the original. Latency in this synchronization process creates eventual consistency, during which the data copy is stale. The separate databases enable the separate read and write models and their respective retrieve and modify APIs to truly evolve independently. Not only can the read model or write model\u2019s implementation change without changing the other, but how each stores its data can be changed independently. This solution offers the following advantages: Scaling : The query load is moved from the write database to the read database. If the database of record is a scalability bottleneck and a lot of the load on it is caused by queries, unloading those query responsibilities can significantly improve the scalability of the combined data access. Performance : The schemas of the two databases can be different, enabling them to be designed and optimized independently for better performance. The write database can be optimized for data consistency and correctness, with capabilities such as stored procedures that fit the write model and assist with data updates. The read database can store the data in units that better fit the read model and are better optimized for querying, with larger rows requiring fewer joins. Notice that the design for this stage is significantly more complex than the design for the previous stage. Separate databases with copies of the same data may make data modeling and using data easier, but they require significant overhead to synchronize the data and keep the copies consistent. CQRS employs a couple of design features that support keeping the databases synchronized: Command Bus for queuing commands (optional): A more subtle and optional design decision is to queue the commands produced by the modify API, shown in the diagram as the command bus. This can significantly increase the throughput of multiple apps updating the database, as well as serialize updates to help avoid--or at least detect--merge conflicts. With the bus, a client making an update does not block synchronously while the change is written to the database. Rather, the request to change the database is captured as a command ( Design Patterns ) and put on a message queue, after which the client can proceed with other work. Asynchronously in the background, the write model processes the commands at the maximum sustainable rate that the database can handle, without the database ever becoming overloaded. If the database becomes temporarily unavailable, the commands queue and will be processed when the database becomes available once more. Event Bus for publishing update events (required): Whenever the write database is updated, a change notification is published as an event on the event bus. Interested parties can subscribe to the event bus to be notified when the database is updated. One such party is an event processor for the query database, which receives update events and processes them by updating the query database accordingly. In this way, every time the write database is updated, a corresponding update is made to the read database to keep it in sync. The connection between the command bus and the event bus is facilitated by an application of the Event Sourcing pattern , which keeps a change log that is suitable for publishing. Event sourcing maintains not only the current state of the data but also the history of how that current state was reached. For each command on the command bus, the write model performs these tasks to process the command: Logs the change Updates the database with the change Creates an update event describing the change and publishes it to the event bus The changes that are logged can be the commands from the command bus or the update events published to the event bus","title":"Separate read and write databases"},{"location":"patterns/cqrs/index.html#considerations","text":"Keep these decisions in mind while applying this pattern: Client impact : Applying CQRS not only changes how data is stored and accessed, but also changes the APIs that clients use to access data. This means that each client must be redesigned to use the new APIs. Riskiness : A lot of the complexity of the pattern solution involves duplicating the data in two databases and keeping them synchronized. Risk comes from querying data that is stale or downright wrong because of problems with the synchronization. Eventual consistency : Clients querying data must expect that updates will have latency. In a microservices architecture, eventual data consistency is a given and acceptable in many of cases. Command queuing : Using a command bus as part of the write solution to queue the commands is optional but powerful. In addition to the benefits of queuing, the command objects can easily be stored in the change log and easily be converted into notification events. (In the next section, we illustrate a way to use event bus to queue commands as well.) Change log : The log of changes to the database of record can be either the list of commands from the command bus or the list of event notifications published on the event bus. The Event Sourcing pattern assumes it\u2019s a log of events, but that pattern doesn\u2019t include the command bus. An event list may be easier to scan as a history, whereas a command list is easier to replay. Create keys : Strict interpretation of the Command Query Separation (CQS) pattern says that command operations do not have return types. A possible exception is commands that create data: An operation that creates a new record or document typically returns the key for accessing the new data, which is convenient for the client. However, if the create operation is invoked asynchronously by a command on a command bus, the write model will need to perform a callback on the client to return the key. Messaging queues and topics : While messaging is used to implement both the command bus and event bus, the two busses use messaging differently. The command bus guarantees exactly once delivery. The event bus broadcasts each event to all interested event processors. Query database persistence : The database of record is always persistent. The query database is a cache that can be a persistent cache or an in-memory cache. If the cache is in-memory and is lost, it must be rebuilt completely from the database of record. Security : Controls on reading data and updating data can be applied separately using the two parts of the solution.","title":"Considerations"},{"location":"patterns/cqrs/index.html#combining-event-sourcing-and-cqrs","text":"The CQRS application pattern is frequently associated with event sourcing: when doing event sourcing and domain driven design, we event source the aggregates or root entities. Aggregate creates events that are persisted. On top of the simple create, update and read by ID operations, the business requirements want to perform complex queries that can't be answered by a single aggregate. By just using event sourcing to be able to respond to a query like \"what are the orders of a customer\", then we have to rebuild the history of all orders and filter per customer. It is a lot of computation. This is linked to the problem of having conflicting domain models between query and persistence. As introduced in previous section, creations and updates are done as state notification events (change of state), and are persisted in the event log/store. The following figure, presents two separate microservices, one supporting the write model, and multiple other supporting the queries: The query part is separate processes that consume change log events and build a projection for future queries. The \"write\" part may persist in SQL while the read may use document oriented database with strong indexing and query capabilities. Or use in-memory database, or distributed cache... They do not need to be in the same language. With CQRS and ES the projections are retroactives. New query equals implementing new projection and read the events from the beginning of time or the recent committed state and snapshot. Read and write models are strongly decoupled and can evolve independently. It is important to note that the 'Command' part can still handle simple queries, primary-key based, like get order by id, or queries that do not involve joins. The event backbone, use a pub/sub model, and Kafka is a good candidate as an implementation technology. With this structure, the Read model microservice will most likely consume events from multiple topics to build the data projection based on joining those data streams. A query, to assess if the cold-chain was respected on the fresh food order shipment, will go to the voyage, container metrics, and order to be able to answer this question. This is where CQRS shines. We can note that, we can separate the API definition and management in a API gateway. The shipment order microservice is implementing this pattern. Some implementation items to consider: Consistency (ensure the data constraints are respected for each data transaction): CQRS without event sourcing has the same consistency guarantees as the database used to persist data and events. With Event Sourcing the consistency could be different, one for the write model and one for the read model. On write model, strong consistency is important to ensure the current state of the system is correct, so it leverages transaction, lock and sharding. On read side, we need less consistency, as they mostly work on stale data. Locking data on the read operation is not reasonable. Scalability : Separating read and write as two different microservices allows for high availability. Caching at the read level can be used to increase performance response time, and can be deployed as multiple standalone instances (Pods in Kubernetes). It is also possible to separate the query implementations between different services. Functions as service / serverless are good technology choices to implement complex queries. Availability : The write model sacrifices consistency for availability. This is a fact. The read model is eventually consistent so high availability is possible. In case of failure the system disables the writing of data but still is able to read them as they are served by different databases and services. With CQRS, the write model can evolve over time without impacting the read model, as long as the event model doesn't change. The read model requires additional tables, but they are often simpler and easier to understand. CQRS results in an increased number of objects, with commands, operations, events,... and packaging in deployable components or containers. It adds potentially different type of data sources. It is more complex. Some challenges to always consider: How to support event structure version management? How much data to keep in the event store (history)? How to adopt data duplication which results to eventual data consistency?. The CQRS pattern was introduced by Greg Young , and described in Martin Fowler's work on microservices. As you can see in previous figure, as soon as we see two arrows from the same component, we have to ask ourselves how does it work: the write model has to persist Order in its own database and then sends OrderCreated event to the topic... Should those operations be atomic and controlled with transaction? We detail this in next section.","title":"Combining event sourcing and CQRS"},{"location":"patterns/cqrs/index.html#keeping-the-write-model-on-mainframe","text":"It is very important to note that the system of records and transaction processing is still easier to run on mainframe to support strong consistency. But with the move to cloud native development, it does not mean we have to move all the system of records to the cloud. Data pipelines can be put in place, but CQRS should help by keeping the write model on the current system of records and without impacting the current MIPS utilization move data in the eventual consistency world of the cloud native, distributed computing world. In the figure above, the write model follows the current transaction processing on the mainframe, change data capture push data to Event backbone for getting real time visibility into the distributed world. The read is the costly operation, dues to the joins to be done to build the projection views needed to expose data depending of the business use cases. This is more true with distributed microservices. All the write operations for the business entities kept in the mainframe's system of records are still done via the transaction. Reference data can be injected in one load job to topic and persisted in the event store so streaming applications can leverage them by joining with transactional data.","title":"Keeping the write model on Mainframe"},{"location":"patterns/cqrs/index.html#the-consistency-challenges","text":"As introduced in the previous section, there is a potential problem of data inconsistency: once a command saves changes into the database, the consumers do not see the new or updated data until event notification completes processing. With traditional Java service, using JPA and JMS, the save and send operations can be part of the same XA transaction and both succeed or fail. With event sourcing pattern, the source of trust is the event source, which acts as a version control system, as shown in the diagram below. The steps for synchronizing changes to the data are: The write model creates the event and publishes it The consumer receives the event and extracts its payload The consumer updates its local datasource with the payload data If the consumer fails to process the update, it can persist the event to an error log Each error in the log can be replayed A command line interface replays an event via an admin API, which searches in the topic using this order id to replay the save operation This implementation causes a problem for the createOrder(order): string operation: The Order Service is supposed to return the new order complete with the order id that is a unique key, a key most likely created by the database. If updating the database fails, there is no new order yet and so no database key to use as the order ID. To avoid this problem, if the underlying technology supports assigning the new order's key, the service can generate the order ID and use that as the order's key in the database. It is important to clearly study the Kafka consumer API and the different parameters on how to support the read offset. We are addressing those implementation best practices in our consumer note.","title":"The consistency challenges"},{"location":"patterns/cqrs/index.html#cqrs-and-change-data-capture","text":"There are other ways to support this dual operations level: When using Kafka, Kafka Connect has the capability to subscribe to databases via JDBC, allowing to poll tables for updates and then produce events to Kafka. There is an open-source change data capture solution based on extracting change events from database transaction logs, Debezium that helps to respond to insert, update and delete operations on databases and generate events accordingly. It supports databases like MySQL, Postgres, MongoDB and others. Write the order to the database and in the same transaction write to an event table ( \"outbox pattern\" ). Then use a polling to get the events to send to Kafka from this event table and delete the row in the table once the event is sent. Use the Change Data Capture from the database transaction log and generate events from this log. The IBM Infosphere CDC product helps to implement this pattern. For more detail about this solution see this product tour . The CQRS implementation using CDC will look like in the following diagram: What is important to note is that the event needs to be flexible on the data payload. On the view side, updates to the view part need to be idempotent.","title":"CQRS and Change Data Capture"},{"location":"patterns/cqrs/index.html#delay-in-the-view","text":"There is a delay between the data persistence and the availability of the data in the Read model. For most business applications, it is perfectly acceptable. In web based data access most of the data are at stale. When there is a need for the client, calling the query operation, to know if the data is up-to-date, the service can define a versioning strategy. When the order data was entered in a form within a single page application, the \"create order\" operation should return the order with its unique key freshly created and the Single Page Application will have the last data. Here is an example of such operation: @POST public Response create ( OrderCreate dto ) { Order order = new Order ( UUID . randomUUID (). toString (), dto . getProductID (),...); // ... return Response . ok (). entity ( order ). build () }","title":"Delay in the view"},{"location":"patterns/cqrs/index.html#schema-change","text":"What to do when we need to add attribute to event?. So we need to create a versioninig schema for event structure. You need to use flexible schema like json schema, Apache Avro or protocol buffer and may be, add an event adapter (as a function?) to translate between the different event structures.","title":"Schema change"},{"location":"patterns/cqrs/index.html#code-reference","text":"The following project includes two sub modules, each deployable as a microservice to illustrate the command and query part: https://github.com/ibm-cloud-architecture/refarch-kc-order-ms","title":"Code reference"},{"location":"patterns/cqrs/index.html#further-readings","text":"https://www.codeproject.com/Articles/555855/Introduction-to-CQRS http://udidahan.com/2009/12/09/clarified-cqrs https://martinfowler.com/bliki/CQRS.html https://microservices.io/patterns/data/cqrs.html https://community.risingstack.com/when-to-use-cqrs https://dzone.com/articles/concepts-of-cqrs https://martinfowler.com/bliki/CommandQuerySeparation.html https://www.martinfowler.com/eaaCatalog/domainModel.html https://dddcommunity.org/learning-ddd/what_is_ddd/ https://martinfowler.com/bliki/EvansClassification.html https://martinfowler.com/bliki/DDD_Aggregate.html https://martinfowler.com/eaaCatalog/dataTransferObject.html https://en.wikipedia.org/wiki/Command_pattern https://www.pearson.com/us/higher-education/program/Gamma-Design-Patterns-Elements-of-Reusable-Object-Oriented-Software/PGM14333.html","title":"Further readings"},{"location":"patterns/dlq/index.html","text":"Dead Letter Queue Event reprocessing with dead letter pattern With event driven microservice, it is not just about pub/sub: there are use cases where the microservice needs to call existing service via an HTTP or RPC call. The call may fail. So what should be the processing to be done to retry and gracefully fail by leveraging the power of topics and the concept of dead letter. This pattern is influenced by the adoption of Kafka as event backbone and the offset management offered by Kafka. Once a message is read from a Kafka topic by a consumer the offset can be automatically committed so the consumer can poll the next batch of events, or in most the case manually committed, to support business logic to process those events. The figure below demonstrates the problem to address: the reefer management microservice get order event to process by allocating a reefer container to the order. The call to update the container inventory fails. At first glance, the approach is to commit the offset only when the three internal operations are succesful: write to reefer database (2), update the container inventory using legacy application service (3), and produce new event to orders (4) and containers (5) topics. Step (4) and (5) will not be done as no response from (3) happened. In fact a better approach is to commit the read offset on the orders topic, and then starts the processing: the goal is to do not impact the input throughput. In case of step (2) or (3) fails the order data is published to an order-retries topic (4) with some added metadata like number of retries and timestamp. A new order retry service or function consumes the order retry events (5) and do a new call to the remote service using a delay according to the number of retries already done: this is to pace the calls to a service that has issue for longer time. If the call (6) fails this function creates a new event in the order-retries topic with a retry counter increased by one. If the number of retry reaches a certain threshold then the order is sent to order-dead-letter topic (7) for human to work on. A CLI could read from this dead letter topic to deal with the data, or retry automatically once we know the backend service works. Using this approach we delay the call to the remote service without putting too much preassure on it. We have implemented the dead letter pattern when integrating the container manager microservice with an external BPM end point. The integration test detail is in this note and the integration test here . For more detail we recommend this article from Uber engineering: Building Reliable Reprocessing and Dead Letter Queues with Apache Kafka .","title":"Dead letter queue"},{"location":"patterns/dlq/index.html#dead-letter-queue","text":"","title":"Dead Letter Queue"},{"location":"patterns/dlq/index.html#event-reprocessing-with-dead-letter-pattern","text":"With event driven microservice, it is not just about pub/sub: there are use cases where the microservice needs to call existing service via an HTTP or RPC call. The call may fail. So what should be the processing to be done to retry and gracefully fail by leveraging the power of topics and the concept of dead letter. This pattern is influenced by the adoption of Kafka as event backbone and the offset management offered by Kafka. Once a message is read from a Kafka topic by a consumer the offset can be automatically committed so the consumer can poll the next batch of events, or in most the case manually committed, to support business logic to process those events. The figure below demonstrates the problem to address: the reefer management microservice get order event to process by allocating a reefer container to the order. The call to update the container inventory fails. At first glance, the approach is to commit the offset only when the three internal operations are succesful: write to reefer database (2), update the container inventory using legacy application service (3), and produce new event to orders (4) and containers (5) topics. Step (4) and (5) will not be done as no response from (3) happened. In fact a better approach is to commit the read offset on the orders topic, and then starts the processing: the goal is to do not impact the input throughput. In case of step (2) or (3) fails the order data is published to an order-retries topic (4) with some added metadata like number of retries and timestamp. A new order retry service or function consumes the order retry events (5) and do a new call to the remote service using a delay according to the number of retries already done: this is to pace the calls to a service that has issue for longer time. If the call (6) fails this function creates a new event in the order-retries topic with a retry counter increased by one. If the number of retry reaches a certain threshold then the order is sent to order-dead-letter topic (7) for human to work on. A CLI could read from this dead letter topic to deal with the data, or retry automatically once we know the backend service works. Using this approach we delay the call to the remote service without putting too much preassure on it. We have implemented the dead letter pattern when integrating the container manager microservice with an external BPM end point. The integration test detail is in this note and the integration test here . For more detail we recommend this article from Uber engineering: Building Reliable Reprocessing and Dead Letter Queues with Apache Kafka .","title":"Event reprocessing with dead letter pattern"},{"location":"patterns/event-sourcing/index.html","text":"Event Sourcing Problems and Constraints Most business applications are state based persistent where any update changes the previous state of business entities. The database keeps the last committed update. But some business application needs to explain how it reaches its current state . For that the application needs to keep history of business facts. Traditional domain oriented implementation builds a domain data model and map it to a RDBMS. As an example, in the simple Order model below, the database record will keep the last state of the Order entity, with the different addresses and the last ordered items in separate tables. If you need to implement a query that looks at what happened to the order over a time period, you need to change the model and add historical records, basically building a log table. Designing a service to manage the life cycle of this order will, most of the time, add a \"delete operation\" to remove data. For legal reason, most businesses do not remove data. As an example, a business ledger has to include new record(s) to compensate a previous transaction. There is no erasing of previously logged transactions. It is always possible to understand what was done in the past. Most business application needs to keep this capability. Solution and Pattern Event sourcing persists the state of a business entity, such an Order, as a sequence of state-changing events or immutable \"facts\" ordered over time. Event sourcing has its roots in the domain-driven design community. When the state of a system changes, an application issues a notification event of the state change. Any interested parties can become consumers of the event and take required actions. The state-change event is immutable stored in an event log or event store in time order. The event log becomes the principal source of truth. The system state can be recreated from a point in time by reprocessing the events. The history of state changes becomes an audit record for the business and is often a useful source of data for business analysts to gain insights into the business. You can see the \"removing an item\" event in the log is a new event. With this capability, we can count how often a specific product is removed for the shopping cart. In some cases, the event sourcing pattern is implemented completely within the event backbone. Kafka topic and partitions are the building blocks for event sourcing. However, you can also consider implementing the pattern with an external event store, which provides optimizations for how the data may be accessed and used. For example IBM Db2 Event store can provide the handlers and event store connected to the backbone and can provide optimization for down stream analytical processing of the data. An event store needs to store only three pieces of information: The type of event or aggregate. The sequence number of the event. The data as a serialized entity. More data can be added to help with diagnosis and audit, but the core functionality only requires a narrow set of fields. This gives rise to a very simple data design that can be heavily optimized for appending and retrieving sequences of records. With a central event logs, as provides by Kafka, producers append events to the log, and consumers read them from an offset (a sequence number). To get the final state of an entity, the consumer needs to replay all the events, which means replaying the changes to the state from the last committed offset or from the last snapshot or the origin of \"time\". Advantages The main goal is to be able to understand what happens to a business entity over time. But there are a set of interesting things that can be done: We can rebuild the data view within a microservice after it crashes, by reloading the event log. As events are ordered with time, we can apply complex event processing with temporal queries, time window operations, and looking at non-event. Be able to reverse the state and correct data with new events. Considerations When replaying the events, it may be important to avoid generating side effects. A common side effect is to send a notification on state change to other consumers. So the consumer of events need to be adapted to the query and business requirements. For example, if the code needs to answer to the question: \"what happened to the order ID 75 over time?\" then there is no side effect, only a report can be created each time the consumer runs. Sometime it may be too long to replay hundreds of events. In that case we can use snapshot, to capture the current state of an entity, and then replay events from the most recent snapshot. This is an optimization technique not needed for all event sourcing implementations. When state change events are in low volume there is no need for snapshots. Kafka is supporting the event sourcing pattern with the topic and partition . In our reference implementation we are validating event sourcing with Kafka in the Order microservice . The event sourcing pattern is well described in this article on microservices.io . It is a very important pattern to support eventual data consistency between microservices and for data synchronization between system as the event store becomes the source of truth. Another common use case, where event sourcing helps, is when developers push a new code version that corrupts the data: being able to see what was done on the data, and being able to reload from a previous state helps fixing problems. Command sourcing Command sourcing is a similar pattern as the event sourcing one, but the commands that modify the states are persisted instead of the events. This allows commands to be processed asynchronously, which can be relevant when the command execution takes a lot of time. One derived challenge is that the command may be executed multiple times, especially in case of failure. Therefore, it has to be idempotent ( making multiple identical requests has the same effect as making a single request). Finally, there is a need also to perform validation of the command to avoid keeping wrong commands in queue. For example, AddItem command is becoming AddItemValidated , then once persisted to a database it becomes an event as ItemAdded . So mixing command and event sourcing is a common practice. Business transactions are not ACID and span multiple services, they are more a serie of steps, each step is supported by a microservice responsible to update its own entity. We talk about \"eventual data consistency\". The event backbone needs to guarantee that events are delivered at least once and the microservices are responsible to manage their offset from the stream source and deal with inconsistency, by detecting duplicate events. At the microservice level, updating data and emitting event needs to be an atomic operation, to avoid inconsistency if the service crashes after the update to the datasource and before emitting the event. This can be done with an eventTable added to the microservice datasource and an event publisher that reads this table on a regular basis and change the state of the event once published. Another solution is to have a database transaction log reader or miner responsible to publish event on new row added to the log. One other approach to avoid the two-phase commit and inconsistency is to use an Event Store or Event Sourcing pattern to keep track of what is done on the business entity with enough information to rebuild the data state. Events are becoming facts describing state changes done on the business entity. Compendium Martin Fowler - event sourcing pattern Microservice design pattern from Chris Richardson Greg Young video on event sourcing at the goto; conference","title":"Event Sourcing"},{"location":"patterns/event-sourcing/index.html#event-sourcing","text":"","title":"Event Sourcing"},{"location":"patterns/event-sourcing/index.html#problems-and-constraints","text":"Most business applications are state based persistent where any update changes the previous state of business entities. The database keeps the last committed update. But some business application needs to explain how it reaches its current state . For that the application needs to keep history of business facts. Traditional domain oriented implementation builds a domain data model and map it to a RDBMS. As an example, in the simple Order model below, the database record will keep the last state of the Order entity, with the different addresses and the last ordered items in separate tables. If you need to implement a query that looks at what happened to the order over a time period, you need to change the model and add historical records, basically building a log table. Designing a service to manage the life cycle of this order will, most of the time, add a \"delete operation\" to remove data. For legal reason, most businesses do not remove data. As an example, a business ledger has to include new record(s) to compensate a previous transaction. There is no erasing of previously logged transactions. It is always possible to understand what was done in the past. Most business application needs to keep this capability.","title":"Problems and Constraints"},{"location":"patterns/event-sourcing/index.html#solution-and-pattern","text":"Event sourcing persists the state of a business entity, such an Order, as a sequence of state-changing events or immutable \"facts\" ordered over time. Event sourcing has its roots in the domain-driven design community. When the state of a system changes, an application issues a notification event of the state change. Any interested parties can become consumers of the event and take required actions. The state-change event is immutable stored in an event log or event store in time order. The event log becomes the principal source of truth. The system state can be recreated from a point in time by reprocessing the events. The history of state changes becomes an audit record for the business and is often a useful source of data for business analysts to gain insights into the business. You can see the \"removing an item\" event in the log is a new event. With this capability, we can count how often a specific product is removed for the shopping cart. In some cases, the event sourcing pattern is implemented completely within the event backbone. Kafka topic and partitions are the building blocks for event sourcing. However, you can also consider implementing the pattern with an external event store, which provides optimizations for how the data may be accessed and used. For example IBM Db2 Event store can provide the handlers and event store connected to the backbone and can provide optimization for down stream analytical processing of the data. An event store needs to store only three pieces of information: The type of event or aggregate. The sequence number of the event. The data as a serialized entity. More data can be added to help with diagnosis and audit, but the core functionality only requires a narrow set of fields. This gives rise to a very simple data design that can be heavily optimized for appending and retrieving sequences of records. With a central event logs, as provides by Kafka, producers append events to the log, and consumers read them from an offset (a sequence number). To get the final state of an entity, the consumer needs to replay all the events, which means replaying the changes to the state from the last committed offset or from the last snapshot or the origin of \"time\".","title":"Solution and Pattern"},{"location":"patterns/event-sourcing/index.html#advantages","text":"The main goal is to be able to understand what happens to a business entity over time. But there are a set of interesting things that can be done: We can rebuild the data view within a microservice after it crashes, by reloading the event log. As events are ordered with time, we can apply complex event processing with temporal queries, time window operations, and looking at non-event. Be able to reverse the state and correct data with new events.","title":"Advantages"},{"location":"patterns/event-sourcing/index.html#considerations","text":"When replaying the events, it may be important to avoid generating side effects. A common side effect is to send a notification on state change to other consumers. So the consumer of events need to be adapted to the query and business requirements. For example, if the code needs to answer to the question: \"what happened to the order ID 75 over time?\" then there is no side effect, only a report can be created each time the consumer runs. Sometime it may be too long to replay hundreds of events. In that case we can use snapshot, to capture the current state of an entity, and then replay events from the most recent snapshot. This is an optimization technique not needed for all event sourcing implementations. When state change events are in low volume there is no need for snapshots. Kafka is supporting the event sourcing pattern with the topic and partition . In our reference implementation we are validating event sourcing with Kafka in the Order microservice . The event sourcing pattern is well described in this article on microservices.io . It is a very important pattern to support eventual data consistency between microservices and for data synchronization between system as the event store becomes the source of truth. Another common use case, where event sourcing helps, is when developers push a new code version that corrupts the data: being able to see what was done on the data, and being able to reload from a previous state helps fixing problems.","title":"Considerations"},{"location":"patterns/event-sourcing/index.html#command-sourcing","text":"Command sourcing is a similar pattern as the event sourcing one, but the commands that modify the states are persisted instead of the events. This allows commands to be processed asynchronously, which can be relevant when the command execution takes a lot of time. One derived challenge is that the command may be executed multiple times, especially in case of failure. Therefore, it has to be idempotent ( making multiple identical requests has the same effect as making a single request). Finally, there is a need also to perform validation of the command to avoid keeping wrong commands in queue. For example, AddItem command is becoming AddItemValidated , then once persisted to a database it becomes an event as ItemAdded . So mixing command and event sourcing is a common practice. Business transactions are not ACID and span multiple services, they are more a serie of steps, each step is supported by a microservice responsible to update its own entity. We talk about \"eventual data consistency\". The event backbone needs to guarantee that events are delivered at least once and the microservices are responsible to manage their offset from the stream source and deal with inconsistency, by detecting duplicate events. At the microservice level, updating data and emitting event needs to be an atomic operation, to avoid inconsistency if the service crashes after the update to the datasource and before emitting the event. This can be done with an eventTable added to the microservice datasource and an event publisher that reads this table on a regular basis and change the state of the event once published. Another solution is to have a database transaction log reader or miner responsible to publish event on new row added to the log. One other approach to avoid the two-phase commit and inconsistency is to use an Event Store or Event Sourcing pattern to keep track of what is done on the business entity with enough information to rebuild the data state. Events are becoming facts describing state changes done on the business entity.","title":"Command sourcing"},{"location":"patterns/event-sourcing/index.html#compendium","text":"Martin Fowler - event sourcing pattern Microservice design pattern from Chris Richardson Greg Young video on event sourcing at the goto; conference","title":"Compendium"},{"location":"patterns/saga/index.html","text":"Saga Problems and Constraints With the adoption of one data source per microservice, there is an interesting challenge on how to support long running transaction cross microservices. With event backbone two phase commit is not an option. Solution and Pattern Introduced in 1987 by Hector Garcaa-Molrna Kenneth Salem paper the Saga pattern help to support a long running transaction that can be broken up to a collection of sub transactions that can be interleaved any way with other transactions. With microservice each transaction updates data within a single service, each subsequent steps may be triggered by previous completion. The following figure, based on our solution implementation , illustrates those concepts for an order transaction: When the order is created, the business process says, we need to allocate a \"voyage\", assign containers and update the list of containers to load on the ship. Those actions / commands are chained. The final state (in this schema, not in the reality, as the process has more steps) is the Order assigned state in the order microservice. With a unique application implementation, the integrity between order, voyage and container tables will be done via transactions. With distributed system we could not apply two phase commit transaction so the Saga pattern will help. SAGA pattern supports two types of implementation: Choreography and Orchestration. Services choreography With Choreography each service produces and listens to other service\u2019s events and decides if an action should be taken or not. The first service executes a transaction to its own data store and then publishes an event ( OrderCreated event (1)) as fact about its business entity update. It maintains the business entity status, (order.status) to the Pending state until the saga is completed. This event is listened by one or more services which execute local transactions and publish new events (VoyageAllocated (3), ReeferAssigned (4), PaymentProcessed (5)). The distributed transaction ends when the last service executes its local transaction or when a service does not publish any events or the event published is not polled by any of the saga\u2019s participants. For example, the Order microservice gets all the events from the other service and changed the Order state to be Accepted . In case of failure, the source microservice is keeping state and timer to monitor for the expected completion events. When a message from any service is missing, the source service, needs to trigger a compensation process: Rolling back a distributed transaction does not come for free. Normally you have to implement another operation/transaction to compensate for what has been done before. This will be a new event sent by the service responsible of the transaction integrity. In the order example, in the rare case where one of the service is not able to provide a positive response, no voyage found, or no Reefer container found, then the order needs to change to 'Uncompleted' status, and an event to the orders topic will claim the orderID is now uncompleted (OrderUncompleted event Step 1 above) . Any service that has something allocated for this orderId will 'unroll' their changes in their own data source (Steps 2,3,4 below). Also it is important to note, that if one of the service is taking time to answer this may not be a problem as the order is in pending state. If the business requirement stipulates to address an order within a small time period then the compensation process may start. Uncompleted orders can be reviewed by a business user for manual handling. Email can be automatically sent to the customer about issue related to his order. There are a lot of different ways to handle order issue at the business level. Services orchestration With orchestration, one service is responsible to drive each participant on what to do and when. It uses the different topics to control the saga by issuing event commands to the different service. It uses the event backbone as a queue processing to support the asynchronous invocations. In this case the event should be exactly once delivered and idempotent. Each participant produces response in their context and to the order topic. The orchestration layer needs to keep a state machine and acts once all the expected responses are received. If anything fails, the orchestrator is also responsible for coordinating the compensation process by sending rollback events with orderID and their respective impacted entity key (voyageID, reeferID, transactionID). Each participant will undo its previous operations. Orchestrator is a State Machine where each transformation corresponds to a command or message. See also this article from Chris Richardson on the Saga pattern. We have implemented the choreography saga pattern in the order management, voyage and refeer management microservices within the EDA reference implementation solution with a detailed explanation of the integration tests to validate the happy path , and the exception path with compensation.","title":"SAGA"},{"location":"patterns/saga/index.html#saga","text":"","title":"Saga"},{"location":"patterns/saga/index.html#problems-and-constraints","text":"With the adoption of one data source per microservice, there is an interesting challenge on how to support long running transaction cross microservices. With event backbone two phase commit is not an option.","title":"Problems and Constraints"},{"location":"patterns/saga/index.html#solution-and-pattern","text":"Introduced in 1987 by Hector Garcaa-Molrna Kenneth Salem paper the Saga pattern help to support a long running transaction that can be broken up to a collection of sub transactions that can be interleaved any way with other transactions. With microservice each transaction updates data within a single service, each subsequent steps may be triggered by previous completion. The following figure, based on our solution implementation , illustrates those concepts for an order transaction: When the order is created, the business process says, we need to allocate a \"voyage\", assign containers and update the list of containers to load on the ship. Those actions / commands are chained. The final state (in this schema, not in the reality, as the process has more steps) is the Order assigned state in the order microservice. With a unique application implementation, the integrity between order, voyage and container tables will be done via transactions. With distributed system we could not apply two phase commit transaction so the Saga pattern will help. SAGA pattern supports two types of implementation: Choreography and Orchestration.","title":"Solution and Pattern"},{"location":"patterns/saga/index.html#services-choreography","text":"With Choreography each service produces and listens to other service\u2019s events and decides if an action should be taken or not. The first service executes a transaction to its own data store and then publishes an event ( OrderCreated event (1)) as fact about its business entity update. It maintains the business entity status, (order.status) to the Pending state until the saga is completed. This event is listened by one or more services which execute local transactions and publish new events (VoyageAllocated (3), ReeferAssigned (4), PaymentProcessed (5)). The distributed transaction ends when the last service executes its local transaction or when a service does not publish any events or the event published is not polled by any of the saga\u2019s participants. For example, the Order microservice gets all the events from the other service and changed the Order state to be Accepted . In case of failure, the source microservice is keeping state and timer to monitor for the expected completion events. When a message from any service is missing, the source service, needs to trigger a compensation process: Rolling back a distributed transaction does not come for free. Normally you have to implement another operation/transaction to compensate for what has been done before. This will be a new event sent by the service responsible of the transaction integrity. In the order example, in the rare case where one of the service is not able to provide a positive response, no voyage found, or no Reefer container found, then the order needs to change to 'Uncompleted' status, and an event to the orders topic will claim the orderID is now uncompleted (OrderUncompleted event Step 1 above) . Any service that has something allocated for this orderId will 'unroll' their changes in their own data source (Steps 2,3,4 below). Also it is important to note, that if one of the service is taking time to answer this may not be a problem as the order is in pending state. If the business requirement stipulates to address an order within a small time period then the compensation process may start. Uncompleted orders can be reviewed by a business user for manual handling. Email can be automatically sent to the customer about issue related to his order. There are a lot of different ways to handle order issue at the business level.","title":"Services choreography"},{"location":"patterns/saga/index.html#services-orchestration","text":"With orchestration, one service is responsible to drive each participant on what to do and when. It uses the different topics to control the saga by issuing event commands to the different service. It uses the event backbone as a queue processing to support the asynchronous invocations. In this case the event should be exactly once delivered and idempotent. Each participant produces response in their context and to the order topic. The orchestration layer needs to keep a state machine and acts once all the expected responses are received. If anything fails, the orchestrator is also responsible for coordinating the compensation process by sending rollback events with orderID and their respective impacted entity key (voyageID, reeferID, transactionID). Each participant will undo its previous operations. Orchestrator is a State Machine where each transformation corresponds to a command or message. See also this article from Chris Richardson on the Saga pattern. We have implemented the choreography saga pattern in the order management, voyage and refeer management microservices within the EDA reference implementation solution with a detailed explanation of the integration tests to validate the happy path , and the exception path with compensation.","title":"Services orchestration"},{"location":"patterns/transactional-outbox/index.html","text":"Transactional outbox pattern When distributed transaction is not supported by the messaging middleware (like current Kafka version), it is important to ensure consistency between the records in the database and the events published. In the reference implementation we used the approach to publish to the topic as soon as an order is received via the API and then the same code, is consuming this events to persist to the database. With this approach if write to the topic operation fails, the application can return an error to the user, if the write operation to the database fails, the code can reload from the non-committed record. But there is another solution presented by the transactional outbox. For detailed information about this pattern see the documentation of the pattern in Chris Richardson's site: Transactional outbox . To summarize this pattern, the approach is to use an outbox table to keep the messages to sent and a message relay process to publish events inserted into database to the event backbone. In modern solution this relay is a change data capture agent. The following schema illustrates the approach: To get a concrete example of this pattern we have developed a deep dive lab using Quarkus Debezium outbox pattern and a DB2 database, or see our outbox on quarkus with Debezium and Postgresql implementation","title":"Outbox"},{"location":"patterns/transactional-outbox/index.html#transactional-outbox-pattern","text":"When distributed transaction is not supported by the messaging middleware (like current Kafka version), it is important to ensure consistency between the records in the database and the events published. In the reference implementation we used the approach to publish to the topic as soon as an order is received via the API and then the same code, is consuming this events to persist to the database. With this approach if write to the topic operation fails, the application can return an error to the user, if the write operation to the database fails, the code can reload from the non-committed record. But there is another solution presented by the transactional outbox. For detailed information about this pattern see the documentation of the pattern in Chris Richardson's site: Transactional outbox . To summarize this pattern, the approach is to use an outbox table to keep the messages to sent and a message relay process to publish events inserted into database to the event backbone. In modern solution this relay is a change data capture agent. The following schema illustrates the approach: To get a concrete example of this pattern we have developed a deep dive lab using Quarkus Debezium outbox pattern and a DB2 database, or see our outbox on quarkus with Debezium and Postgresql implementation","title":"Transactional outbox pattern"},{"location":"technology/faq/index.html","text":"Basic questions What is Kafka? pub/sub middleware to share data between applications Open source, started in 2011 by Linkedin based on append log to persist immutable records ordered by arrival. support data partitioning, distributed brokers, horizontal scaling, low-latency and high throughput. producer has no knowledge of consumer records stay even after being consumed durability with replication to avoid loosing data for high availability What are the major components? Topic, consumer, producer, brokers Rich API to control the producer semantic, and consumer Consumer groups Kafka streams API to support data streaming with stateful operations and stream processing topology Kafka connect for source and sink connection to external systems Topic replication with Mirror Maker 2 Major use cases? Modern data pipeline with buffering to data lake Data hub, to continuously expose business entities to event-driven applications and microservices Real time analytics with aggregate computation, and complex event processing The communication layer for Event-driven, reactive microservice. Why does Kafka use zookeeper? Kafka as a distributed system using cluster, it needs to keep cluster states, sharing configuration like topic, assess which node is still alive within the cluster, support registering new node added to the cluster, being able to support dynamic restart. Zookeeper is an orchestrator for distributed system, it maintains Kafka cluster integrity, select broker leader... Zookeeper is also used to manage offset commit, and to the leader selection process. What is a replica? A lit of nodes responsible to participate into the data replication process for a given partition. It is a critical feature to ensure durability, be able to continue to consume records, or to ensure a certain level of data loss safety is guaranteed when producing records. What are a leader and follower in Kafka? Topic has 1 to many partition, which are append logs. Every partition in Kafka has a server that plays the role of leader . When replication is set in a topic, follower brokers will pull data from the leader to ensure replication, up to the specified replication factor. If the leader fails, one of the followers needs to take over as the leader\u2019s role. The leader election process involves zookeeper and assess which follower was the most in-synch with the leader. Leader is the end point for read and write operations on the partition. (Exception is the new feature to read from local follower). To get the list of In-synch Replication for a given topic the following tool can be used: kafka-topics.sh --bootstrap-server :9092 --describe --topic <topicname> What is Offset? A unique identifier of records inside a partition. It is automatically created by the broker, and producer can get it from the broker response. Consumer uses it to commit its read. It means, in case of consumer restarts, it will read from the last committed offset. What is a consumer group? It groups consumers of one to many topics. Each partition is consumed by exactly one consumer within each subscribing consumer group. Consumer group is specified via the group.id consumer's property, and when consumers subscribe to topic(s). There is a protocol to manage consumers within a group so that partition can be reallocated when a consumer lefts the group. The group leader is responsible to do the partition assignment. When using the group.instance.id properties, consumer is treated as a static member, which means there will be no partition rebalance when consumer lefts a group for a short time period. When not set the group coordinator (a broker) will allocate ids to group members, and reallocation will occur. For Kafka Streams application it is recommended to use static membership. Brokers keep offsets until a retention period within which consumer group can lose all its consumers. After that period, offsets are discarded. The consumer group can be deleted manually, or automatically when the last committed offset for that group expires. When the group coordinator receives an OffsetCommitRequest, it appends the request to a special compacted Kafka topic named __consumer_offsets. Ack from the broker is done once all replicas on this hidden topics are successful. The tool kafka-consumer-group.sh helps getting details of consumer group: # Inside a Kafka broker container bin/kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group order-group --members --verbose Support to multi-tenancy? Multi-tenant means multiple different groups of application can produce and consumer messages isolated from other. So by constructs, topics and brokers are multi-tenant. Now the control will be at the access control level policy, the use of service account, and naming convention on the topic name. Consumer and producer authenticate themselves using dedicated service account users, with SCRAM user or Mutual TLS user. Each topic can have security policy to control read, write, creation operations. How client access Kafka cluster metadata? Provide a list of Kafka brokers, minimum two, so the client API will get the metadata once connected to one of the broker. How to get at most once delivery? Set producer acknowledge level (acks) property to 0 or 1. How to support exactly once delivery? The goal is to address that if a producer sends a message twice the system will send only one message to the consumer, and once the consumer commits the read offset, it will not receive the message again even if it restarts. See the section in the producer implementation considerations note . The consumer needs to always read from its last committed offset. Also it is important to note that the Kafka Stream API supports exactly once semantics with the config: processing.guarantee=exactly_once . Each task within a read-process-write flow may fail so this setting is important to be sure the right answer is delivered, even in case of task failure, and the process is executed exactly once. Exactly-once delivery for other destination systems generally requires cooperation with such systems which may be possible by using the offset processing. What is range partition assignment strategy? There are multiple partition assignment strategy for a consumer, part of a consumer group , to get its partition to fetch data from. Members of the consumer group subscribe to the topics they are interested in and forward their subscriptions to a Kafka broker serving as the group coordinator. The coordinator selects one member to perform the group assignment and propagates the subscriptions of all members to it. Then assign(Cluster, GroupSubscription) is called to perform the assignment and the results are forwarded back to each respective members. Range assignor works on a per-topic basis: it lays out the available partitions in numeric order and the consumers in lexicographic order, and assign partition to each consumer so partition with the same id will be in the same consumer: topic-1-part-0 and topic-2-part-0 will be processed by consumer-0 What is sticky assignor? The CooperativeStickyAssignor helps supporting incremental cooperative rebalancing to the clients' group protocol, which allows consumers to keep all of their assigned partitions during a rebalance and at the end revoke only those which must be migrated to another consumer for overall cluster balance. The goal is to reduce unnecessary downtime due to unnecessary partition migration, by leveraging the sticky assignor which link consumer to partition id. See KIP 429 for details. How to get an homogeneous distribution of message to partitions? Design the message key and hash coding for even distributed. Or implement a customer partitioner by implementing the Partitioner interface. How to ensure efficient join between two topics? Need to use co-partitioning, which means having the same key in both topic, the same number of partitions and the same producer partitioner, which most likely should be the default one that uses the following formula: partition = hash(key) % numPartitions . What is transaction in Kafka? Producer can use transaction begin, commit and rollback API while publishing events to a multi partition topic. This is done by setting a unique transactionId as part of its configuration (with idempotence and min inflight record set to 1). Either all messages are successfully written or none of them are. There are some producer exception to consider to abort the transaction: any KafkaException for sure, but also OutOfSequenceTx which may happen when the PID is greater than the last one seen by the producer. See explanations here . And the KIP 98 What is the high watermark? The high watermark offset is the offset of the last message that was successfully copied to all of the log\u2019s replicas. A consumer can only read up to the high watermark offset to prevent reading un-replicated messages. Retention time for topic what does it mean? The message sent to a cluster is kept for a max period of time or until a max size is reached. Those topic properties are: retention.ms and retention.bytes . Messages stay in the log even if they are consumed. The oldest messages are marked for deletion or compaction depending of the cleanup policy (delete or compact) set to cleanup.policy topic's parameter. See the Kafka documentation on topic configuration parameters . Here is a command to create a topic with specific retention properties: bin/kafka-configs --zookeeper XX.XX.XX.XX:2181 --entity-type topics --entity-name orders --alter --add-config retention.ms = 55000 --add-config retention.byte = 100000 But there is also the offsets.retention.minutes property, set at the cluster level to control when the offset information will be deleted. It is defaulted to 1 day, but the max possible value is 7 days. This is to avoid keeping too much information in the broker memory and avoid to miss data when consumers do not run continuously. So consumers need to commit their offset. If the consumer settings define: auto.offset.reset=earliest , the consumer will reprocess all the events each time it restarts, (or skips to the latest if set to latest ). When using latest , if the consumers are offline for more than the offsets retention time window, they will lose events. What are the topic characteristics I need to define during requirements? This is a requirement gathering related question, to understand what need to be done for configuration topic configuration but also consumer and producer configuration, as well as retention strategy. Number of brokers in the cluster retention time and size Need for HA, set replicas to number of broker or at least the value of 3, with in-synch replica to 2 Type of data to transport to assess message size Plan to use schema management to control change to the payload definition volume per day with peak and average Need to do geo replication to other Kafka cluster Network filesystem used on the target Kubernetes cluster and current storage class What are the impacts of having not enough resource for Kafka? The table in this Event Streams product documentation illustrates the resource requirements for a getting started cluster. When resources start to be at stress, then Kafka communication to ZooKeeper and/or other Kafka brokers can suffer resulting in out-of-sync partitions and container restarts perpetuating the issue. Resource constraints is one of the first things we consider when diagnosing ES issues. What should we do for queue full exception or timeout exception on producer? The brokers are running behind, so we need to add more brokers and redistribute partitions. How to send large messages? We can set some properties at the broker, topic, consumer and producer level: Broker: consider the message.max.bytes and replica.fetch.max.bytes Consumer: max.partition.fetch.bytes . Records are fetched in batches by the consumer, so this properties gives the max amount of data per partition the server will return. Default 1 Megabyte How to maximize throughput? For producer if you want to maximize throughput over low latency, set batch.size and linger.ms to higher value. Linger delay producer, it will wait for up to the given delay to allow other records to be sent so that the sends can be batched together. Why Kafka Stream applications may impact cluster performance? They may use internal hidden topics to persist their states for Ktable and GlobalKTable. Process input and output topics How message schema version is propagated? The record includes a byte with the version number from the schema registry. Consumers do not see message in topic, what happens? The brokers may have an issue on this partition. If a broker, part of the ISR list fails, then new leader election may delay the broker commit from a producer. The consumer has a communication issue, or fails, so the consumer group rebalance is underway. How compression schema used is known by the consumer? The record header includes such metadata. So it is possible to have different schema per record. What does out-of-synch partition mean and occur? With partition leader and replication to the followers, the number of in-synch replicas is at least the number of expected replicas. For example for a replicas = 3 the in-synch is set to 2, and it represents the minimum number of replicas that must acknowledge a write for the write to be considered successful. The record is considered \u201ccommitted\u201d when all ISRs for a partition wrote to their log. Only committed records are readable from consumer. So out-of-synch will happen if the followers are not able to send their acknowledge to the replica leader as quickly as expected. Security in Kafka Encrypt data in transit between producer and Kafka brokers Client authentication Client authorization How to protect data at rest? Use encrypted file system for each brokers Encrypt data at the producer level, using some API, and then decode at the consumer level. The data in the appeld log will be encrypted. How to remove personal identifying information? From the source connector, it is possible to add processing class to process the records before publishing them to Kafka topic, so that any Kafka Streams apps will not see PII. How to handle variable workload with Kafka Connector source connector? Increase and decrease the number of Kafka connect workers based upon current application load. Competitors to Kafka NATS Redpanda a Modern streaming platform for mission critical workloads, and is compatible with Kafka API. It is a cluster of brokers without any zookeepers. It also leverage the SSD technology to improve I/O operations. AWS Kinesis Cloud service, managed by AWS staff, paid as you go, proportional to the shard (like partition) used. 24h to 7 days persistence Number of shards are adaptable with throughput. Uses the concept of Kinesis data streams, which uses shards: data records are composed of a sequence number, a partition key and a data blob. restrictions on message size (1 MB) and consumption rate of messages (5 reads /s, < 2MB per shard, 1000 write /s) Server side encryption using master key managed by AWS KMS GCP Pub/sub Solace Active MQ: Java based messaging server to be the JMS reference implementation, so it supports transactional messaging. various messaging protocols including AMQP, STOMP, and MQTT It maintains the delivery state of every message resulting in lower throughput. Can apply JMS message selector to consumer specific message Point to point or pub/sub, but servers push messages to consumer/subscribers Performance of both queue and topic degrades as the number of consumers rises Rabbit MQ: Support queues, with messages removed once consumed Add the concept of Exchange to route message to queues Limited throughput, but can send large message Support JMS, AMQP protocols, and participation to transaction Smart broker / dumb consumer model that focuses on consistently delivering messages to consumers. Differences between Akka and Kafka? Akka is a open source toolkit for Scala or Java to simplify multithreading programming and makes application more reactive by adopting an asynchronous mechanism to access to io: database or HTTP request. To support asynchronous communication between 'actors', it uses messaging, internal to the JVM. Kafka is part of the architecture, while Akka is an implementation choice for one of the component of the business application deployed inside the architecture. vert.x is another open source implementation of such internal messaging mechanism but supporting more language: Java, Groovy, Ruby, JavaScript, Ceylon, Scala, and Kotlin. Run Kafka Test Container with TopologyTestDriver Topology Test Driver is used without kafka, so there is no real need to use test container. Event streams resource requirements See the detailed tables in the product documentation. Security setting On Kubernetes, Kafka can be configured with external and internal URLs. With Strimzi internal URLs are using TLS or Plain authentication, then TLS for encryption. If no authentication property is specified then the listener does not authenticate clients which connect through that listener. The listener will accept all connections without authentication. Mutual TLS authentication for internal communication looks like: - name : tls port : 9093 type : internal tls : true authentication : type : tls To connect any app (producer, consumer) we need a TLS user like: piVersion : kafka.strimzi.io/v1beta1 kind : KafkaUser metadata : name : tls-user labels : strimzi.io/cluster : vaccine-kafka spec : authentication : type : tls Then the following configurations need to be done for each app. For example in Quarkus app, we need to specify where to find the client certificate (for each Kafka TLS user a secret is created with the certificate (ca.crt) and a user password) oc describe secret tls-user Data ==== ca.crt: 1164 bytes user.crt: 1009 bytes user.key: 1704 bytes user.p12: 2374 bytes user.password: 12 bytes For Java client we need the following security settings, to specify from which secret to get the keystore password and certificate. The certificate will be mounted to /deployments/certs/user . %prod.kafka.security.protocol = SSL %prod.kafka.ssl.keystore.location = /deployments/certs/user/user.p12 %prod.kafka.ssl.keystore.type = PKCS12 quarkus.openshift.env.mapping.KAFKA_SSL_KEYSTORE_PASSWORD.from-secret = ${ KAFKA_USER : tls -user } quarkus.openshift.env.mapping.KAFKA_SSL_KEYSTORE_PASSWORD.with-key = user.password quarkus.openshift.mounts.user-cert.path = /deployments/certs/user quarkus.openshift.secret-volumes.user-cert.secret-name = ${ KAFKA_USER : tls -user } # To validate server side certificate we will mount it too with the following declaration quarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.from-secret = ${ KAFKA_CA_CERT_NAME : kafka -cluster-ca-cert } quarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.with-key = ca.password quarkus.openshift.mounts.kafka-cert.path = /deployments/certs/server quarkus.openshift.secret-volumes.kafka-cert.secret-name = ${ KAFKA_CA_CERT_NAME : kafka -cluster-ca-cert } For the server side certificate, it will be in a truststore, which is mounted to /deployments/certs/server and from a secret (this secret is created at the cluster level). Also because we also use TLS for encryption we need: %prod.kafka.ssl.protocol=TLSv1.2 Mutual TLS authentication is always used for the communication between Kafka brokers and ZooKeeper pods. For mutual, or two-way, authentication, both the server and the client present certificates. SCRAM: (Salted Challenge Response Authentication Mechanism) is an authentication protocol that can establish mutual authentication using passwords. Strimzi can configure Kafka to use SASL (Simple Authentication and Security Layer) SCRAM-SHA-512 to provide authentication on both unencrypted and encrypted client connections. The listener declaration: - name : external port : 9094 type : route tls : true authentication : type : scram-sha-512 Need a scram-user: apiVersion : kafka.strimzi.io/v1beta1 kind : KafkaUser metadata : name : scram-user labels : strimzi.io/cluster : vaccine-kafka spec : authentication : type : scram-sha-512 Then the app properties need: security.protocol = SASL_SSL %prod.quarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.from-secret = ${ KAFKA_CA_CERT_NAME : kafka -cluster-ca-cert } %prod.quarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.with-key = ca.password %prod.quarkus.openshift.env.mapping.KAFKA_SCRAM_PWD.from-secret = ${ KAFKA_USER : scram -user } %prod.quarkus.openshift.env.mapping.KAFKA_SCRAM_PWD.with-key = password %prod.quarkus.openshift.mounts.kafka-cert.path = /deployments/certs/server %prod.quarkus.openshift.secret-volumes.kafka-cert.secret-name = ${ KAFKA_CA_CERT_NAME : kafka -cluster-ca-cert } Verify consumer connection Here is an example of TLS authentication for Event streams ConsumerConfig values: bootstrap.servers = [eda-dev-kafka-bootstrap.eventstreams.svc:9093] check.crcs = true client.dns.lookup = default client.id = cold-chain-agent-c2c11228-d876-4db2-a16a-ea7826e358d2-StreamThread-1-restore-consumer client.rack = connections.max.idle.ms = 540000 default.api.timeout.ms = 60000 enable.auto.commit = false exclude.internal.topics = true fetch.max.bytes = 52428800 fetch.max.wait.ms = 500 fetch.min.bytes = 1 group.id = null group.instance.id = null heartbeat.interval.ms = 3000 interceptor.classes = [] internal.leave.group.on.close = false isolation.level = read_uncommitted key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer sasl.client.callback.handler.class = null sasl.jaas.config = null sasl.kerberos.kinit.cmd = /usr/bin/kinit sasl.kerberos.min.time.before.relogin = 60000 sasl.kerberos.service.name = null sasl.kerberos.ticket.renew.jitter = 0.05 sasl.kerberos.ticket.renew.window.factor = 0.8 sasl.login.callback.handler.class = null sasl.login.class = null sasl.login.refresh.buffer.seconds = 300 sasl.login.refresh.min.period.seconds = 60 sasl.login.refresh.window.factor = 0.8 sasl.login.refresh.window.jitter = 0.05 sasl.mechanism = GSSAPI security.protocol = SSL security.providers = null send.buffer.bytes = 131072 session.timeout.ms = 10000 ssl.cipher.suites = null ssl.enabled.protocols = [TLSv1.2] ssl.endpoint.identification.algorithm = https ssl.key.password = null ssl.keymanager.algorithm = SunX509 ssl.keystore.location = /deployments/certs/user/user.p12 ssl.keystore.password = [hidden] ssl.keystore.type = PKCS12 ssl.protocol = TLSv1.2 ssl.provider = null ssl.secure.random.implementation = null ssl.trustmanager.algorithm = PKIX ssl.truststore.location = /deployments/certs/server/ca.p12 ssl.truststore.password = [hidden] ssl.truststore.type = PKCS12 value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer Other FAQs IBM Event streams on Cloud FAQ FAQ from Confluent","title":"Kafka FAQ"},{"location":"technology/faq/index.html#basic-questions","text":"","title":"Basic questions"},{"location":"technology/faq/index.html#what-is-kafka","text":"pub/sub middleware to share data between applications Open source, started in 2011 by Linkedin based on append log to persist immutable records ordered by arrival. support data partitioning, distributed brokers, horizontal scaling, low-latency and high throughput. producer has no knowledge of consumer records stay even after being consumed durability with replication to avoid loosing data for high availability","title":"What is Kafka?"},{"location":"technology/faq/index.html#what-are-the-major-components","text":"Topic, consumer, producer, brokers Rich API to control the producer semantic, and consumer Consumer groups Kafka streams API to support data streaming with stateful operations and stream processing topology Kafka connect for source and sink connection to external systems Topic replication with Mirror Maker 2","title":"What are the major components?"},{"location":"technology/faq/index.html#major-use-cases","text":"Modern data pipeline with buffering to data lake Data hub, to continuously expose business entities to event-driven applications and microservices Real time analytics with aggregate computation, and complex event processing The communication layer for Event-driven, reactive microservice.","title":"Major use cases?"},{"location":"technology/faq/index.html#why-does-kafka-use-zookeeper","text":"Kafka as a distributed system using cluster, it needs to keep cluster states, sharing configuration like topic, assess which node is still alive within the cluster, support registering new node added to the cluster, being able to support dynamic restart. Zookeeper is an orchestrator for distributed system, it maintains Kafka cluster integrity, select broker leader... Zookeeper is also used to manage offset commit, and to the leader selection process.","title":"Why does Kafka use zookeeper?"},{"location":"technology/faq/index.html#what-is-a-replica","text":"A lit of nodes responsible to participate into the data replication process for a given partition. It is a critical feature to ensure durability, be able to continue to consume records, or to ensure a certain level of data loss safety is guaranteed when producing records.","title":"What is a replica?"},{"location":"technology/faq/index.html#what-are-a-leader-and-follower-in-kafka","text":"Topic has 1 to many partition, which are append logs. Every partition in Kafka has a server that plays the role of leader . When replication is set in a topic, follower brokers will pull data from the leader to ensure replication, up to the specified replication factor. If the leader fails, one of the followers needs to take over as the leader\u2019s role. The leader election process involves zookeeper and assess which follower was the most in-synch with the leader. Leader is the end point for read and write operations on the partition. (Exception is the new feature to read from local follower). To get the list of In-synch Replication for a given topic the following tool can be used: kafka-topics.sh --bootstrap-server :9092 --describe --topic <topicname>","title":"What are a leader and follower in Kafka?"},{"location":"technology/faq/index.html#what-is-offset","text":"A unique identifier of records inside a partition. It is automatically created by the broker, and producer can get it from the broker response. Consumer uses it to commit its read. It means, in case of consumer restarts, it will read from the last committed offset.","title":"What is Offset?"},{"location":"technology/faq/index.html#what-is-a-consumer-group","text":"It groups consumers of one to many topics. Each partition is consumed by exactly one consumer within each subscribing consumer group. Consumer group is specified via the group.id consumer's property, and when consumers subscribe to topic(s). There is a protocol to manage consumers within a group so that partition can be reallocated when a consumer lefts the group. The group leader is responsible to do the partition assignment. When using the group.instance.id properties, consumer is treated as a static member, which means there will be no partition rebalance when consumer lefts a group for a short time period. When not set the group coordinator (a broker) will allocate ids to group members, and reallocation will occur. For Kafka Streams application it is recommended to use static membership. Brokers keep offsets until a retention period within which consumer group can lose all its consumers. After that period, offsets are discarded. The consumer group can be deleted manually, or automatically when the last committed offset for that group expires. When the group coordinator receives an OffsetCommitRequest, it appends the request to a special compacted Kafka topic named __consumer_offsets. Ack from the broker is done once all replicas on this hidden topics are successful. The tool kafka-consumer-group.sh helps getting details of consumer group: # Inside a Kafka broker container bin/kafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group order-group --members --verbose","title":"What is a consumer group?"},{"location":"technology/faq/index.html#support-to-multi-tenancy","text":"Multi-tenant means multiple different groups of application can produce and consumer messages isolated from other. So by constructs, topics and brokers are multi-tenant. Now the control will be at the access control level policy, the use of service account, and naming convention on the topic name. Consumer and producer authenticate themselves using dedicated service account users, with SCRAM user or Mutual TLS user. Each topic can have security policy to control read, write, creation operations.","title":"Support to multi-tenancy?"},{"location":"technology/faq/index.html#how-client-access-kafka-cluster-metadata","text":"Provide a list of Kafka brokers, minimum two, so the client API will get the metadata once connected to one of the broker.","title":"How client access Kafka cluster metadata?"},{"location":"technology/faq/index.html#how-to-get-at-most-once-delivery","text":"Set producer acknowledge level (acks) property to 0 or 1.","title":"How to get at most once delivery?"},{"location":"technology/faq/index.html#how-to-support-exactly-once-delivery","text":"The goal is to address that if a producer sends a message twice the system will send only one message to the consumer, and once the consumer commits the read offset, it will not receive the message again even if it restarts. See the section in the producer implementation considerations note . The consumer needs to always read from its last committed offset. Also it is important to note that the Kafka Stream API supports exactly once semantics with the config: processing.guarantee=exactly_once . Each task within a read-process-write flow may fail so this setting is important to be sure the right answer is delivered, even in case of task failure, and the process is executed exactly once. Exactly-once delivery for other destination systems generally requires cooperation with such systems which may be possible by using the offset processing.","title":"How to support exactly once delivery?"},{"location":"technology/faq/index.html#what-is-range-partition-assignment-strategy","text":"There are multiple partition assignment strategy for a consumer, part of a consumer group , to get its partition to fetch data from. Members of the consumer group subscribe to the topics they are interested in and forward their subscriptions to a Kafka broker serving as the group coordinator. The coordinator selects one member to perform the group assignment and propagates the subscriptions of all members to it. Then assign(Cluster, GroupSubscription) is called to perform the assignment and the results are forwarded back to each respective members. Range assignor works on a per-topic basis: it lays out the available partitions in numeric order and the consumers in lexicographic order, and assign partition to each consumer so partition with the same id will be in the same consumer: topic-1-part-0 and topic-2-part-0 will be processed by consumer-0","title":"What is range partition assignment strategy?"},{"location":"technology/faq/index.html#what-is-sticky-assignor","text":"The CooperativeStickyAssignor helps supporting incremental cooperative rebalancing to the clients' group protocol, which allows consumers to keep all of their assigned partitions during a rebalance and at the end revoke only those which must be migrated to another consumer for overall cluster balance. The goal is to reduce unnecessary downtime due to unnecessary partition migration, by leveraging the sticky assignor which link consumer to partition id. See KIP 429 for details.","title":"What is sticky assignor?"},{"location":"technology/faq/index.html#how-to-get-an-homogeneous-distribution-of-message-to-partitions","text":"Design the message key and hash coding for even distributed. Or implement a customer partitioner by implementing the Partitioner interface.","title":"How to get an homogeneous distribution of message to partitions?"},{"location":"technology/faq/index.html#how-to-ensure-efficient-join-between-two-topics","text":"Need to use co-partitioning, which means having the same key in both topic, the same number of partitions and the same producer partitioner, which most likely should be the default one that uses the following formula: partition = hash(key) % numPartitions .","title":"How to ensure efficient join between two topics?"},{"location":"technology/faq/index.html#what-is-transaction-in-kafka","text":"Producer can use transaction begin, commit and rollback API while publishing events to a multi partition topic. This is done by setting a unique transactionId as part of its configuration (with idempotence and min inflight record set to 1). Either all messages are successfully written or none of them are. There are some producer exception to consider to abort the transaction: any KafkaException for sure, but also OutOfSequenceTx which may happen when the PID is greater than the last one seen by the producer. See explanations here . And the KIP 98","title":"What is transaction in Kafka?"},{"location":"technology/faq/index.html#what-is-the-high-watermark","text":"The high watermark offset is the offset of the last message that was successfully copied to all of the log\u2019s replicas. A consumer can only read up to the high watermark offset to prevent reading un-replicated messages.","title":"What is the high watermark?"},{"location":"technology/faq/index.html#retention-time-for-topic-what-does-it-mean","text":"The message sent to a cluster is kept for a max period of time or until a max size is reached. Those topic properties are: retention.ms and retention.bytes . Messages stay in the log even if they are consumed. The oldest messages are marked for deletion or compaction depending of the cleanup policy (delete or compact) set to cleanup.policy topic's parameter. See the Kafka documentation on topic configuration parameters . Here is a command to create a topic with specific retention properties: bin/kafka-configs --zookeeper XX.XX.XX.XX:2181 --entity-type topics --entity-name orders --alter --add-config retention.ms = 55000 --add-config retention.byte = 100000 But there is also the offsets.retention.minutes property, set at the cluster level to control when the offset information will be deleted. It is defaulted to 1 day, but the max possible value is 7 days. This is to avoid keeping too much information in the broker memory and avoid to miss data when consumers do not run continuously. So consumers need to commit their offset. If the consumer settings define: auto.offset.reset=earliest , the consumer will reprocess all the events each time it restarts, (or skips to the latest if set to latest ). When using latest , if the consumers are offline for more than the offsets retention time window, they will lose events.","title":"Retention time for topic what does it mean?"},{"location":"technology/faq/index.html#what-are-the-topic-characteristics-i-need-to-define-during-requirements","text":"This is a requirement gathering related question, to understand what need to be done for configuration topic configuration but also consumer and producer configuration, as well as retention strategy. Number of brokers in the cluster retention time and size Need for HA, set replicas to number of broker or at least the value of 3, with in-synch replica to 2 Type of data to transport to assess message size Plan to use schema management to control change to the payload definition volume per day with peak and average Need to do geo replication to other Kafka cluster Network filesystem used on the target Kubernetes cluster and current storage class","title":"What are the topic characteristics I need to define during requirements?"},{"location":"technology/faq/index.html#what-are-the-impacts-of-having-not-enough-resource-for-kafka","text":"The table in this Event Streams product documentation illustrates the resource requirements for a getting started cluster. When resources start to be at stress, then Kafka communication to ZooKeeper and/or other Kafka brokers can suffer resulting in out-of-sync partitions and container restarts perpetuating the issue. Resource constraints is one of the first things we consider when diagnosing ES issues.","title":"What are the impacts of having not enough resource for Kafka?"},{"location":"technology/faq/index.html#what-should-we-do-for-queue-full-exception-or-timeout-exception-on-producer","text":"The brokers are running behind, so we need to add more brokers and redistribute partitions.","title":"What should we do for queue full exception or timeout exception on producer?"},{"location":"technology/faq/index.html#how-to-send-large-messages","text":"We can set some properties at the broker, topic, consumer and producer level: Broker: consider the message.max.bytes and replica.fetch.max.bytes Consumer: max.partition.fetch.bytes . Records are fetched in batches by the consumer, so this properties gives the max amount of data per partition the server will return. Default 1 Megabyte","title":"How to send large messages?"},{"location":"technology/faq/index.html#how-to-maximize-throughput","text":"For producer if you want to maximize throughput over low latency, set batch.size and linger.ms to higher value. Linger delay producer, it will wait for up to the given delay to allow other records to be sent so that the sends can be batched together.","title":"How to maximize throughput?"},{"location":"technology/faq/index.html#why-kafka-stream-applications-may-impact-cluster-performance","text":"They may use internal hidden topics to persist their states for Ktable and GlobalKTable. Process input and output topics","title":"Why Kafka Stream applications may impact cluster performance?"},{"location":"technology/faq/index.html#how-message-schema-version-is-propagated","text":"The record includes a byte with the version number from the schema registry.","title":"How message schema version is propagated?"},{"location":"technology/faq/index.html#consumers-do-not-see-message-in-topic-what-happens","text":"The brokers may have an issue on this partition. If a broker, part of the ISR list fails, then new leader election may delay the broker commit from a producer. The consumer has a communication issue, or fails, so the consumer group rebalance is underway.","title":"Consumers do not see message in topic, what happens?"},{"location":"technology/faq/index.html#how-compression-schema-used-is-known-by-the-consumer","text":"The record header includes such metadata. So it is possible to have different schema per record.","title":"How compression schema used is known by the consumer?"},{"location":"technology/faq/index.html#what-does-out-of-synch-partition-mean-and-occur","text":"With partition leader and replication to the followers, the number of in-synch replicas is at least the number of expected replicas. For example for a replicas = 3 the in-synch is set to 2, and it represents the minimum number of replicas that must acknowledge a write for the write to be considered successful. The record is considered \u201ccommitted\u201d when all ISRs for a partition wrote to their log. Only committed records are readable from consumer. So out-of-synch will happen if the followers are not able to send their acknowledge to the replica leader as quickly as expected.","title":"What does out-of-synch partition mean and occur?"},{"location":"technology/faq/index.html#security-in-kafka","text":"Encrypt data in transit between producer and Kafka brokers Client authentication Client authorization","title":"Security in Kafka"},{"location":"technology/faq/index.html#how-to-protect-data-at-rest","text":"Use encrypted file system for each brokers Encrypt data at the producer level, using some API, and then decode at the consumer level. The data in the appeld log will be encrypted.","title":"How to protect data at rest?"},{"location":"technology/faq/index.html#how-to-remove-personal-identifying-information","text":"From the source connector, it is possible to add processing class to process the records before publishing them to Kafka topic, so that any Kafka Streams apps will not see PII.","title":"How to remove personal identifying information?"},{"location":"technology/faq/index.html#how-to-handle-variable-workload-with-kafka-connector-source-connector","text":"Increase and decrease the number of Kafka connect workers based upon current application load.","title":"How to handle variable workload with Kafka Connector source connector?"},{"location":"technology/faq/index.html#competitors-to-kafka","text":"NATS Redpanda a Modern streaming platform for mission critical workloads, and is compatible with Kafka API. It is a cluster of brokers without any zookeepers. It also leverage the SSD technology to improve I/O operations. AWS Kinesis Cloud service, managed by AWS staff, paid as you go, proportional to the shard (like partition) used. 24h to 7 days persistence Number of shards are adaptable with throughput. Uses the concept of Kinesis data streams, which uses shards: data records are composed of a sequence number, a partition key and a data blob. restrictions on message size (1 MB) and consumption rate of messages (5 reads /s, < 2MB per shard, 1000 write /s) Server side encryption using master key managed by AWS KMS GCP Pub/sub Solace Active MQ: Java based messaging server to be the JMS reference implementation, so it supports transactional messaging. various messaging protocols including AMQP, STOMP, and MQTT It maintains the delivery state of every message resulting in lower throughput. Can apply JMS message selector to consumer specific message Point to point or pub/sub, but servers push messages to consumer/subscribers Performance of both queue and topic degrades as the number of consumers rises Rabbit MQ: Support queues, with messages removed once consumed Add the concept of Exchange to route message to queues Limited throughput, but can send large message Support JMS, AMQP protocols, and participation to transaction Smart broker / dumb consumer model that focuses on consistently delivering messages to consumers.","title":"Competitors to Kafka"},{"location":"technology/faq/index.html#differences-between-akka-and-kafka","text":"Akka is a open source toolkit for Scala or Java to simplify multithreading programming and makes application more reactive by adopting an asynchronous mechanism to access to io: database or HTTP request. To support asynchronous communication between 'actors', it uses messaging, internal to the JVM. Kafka is part of the architecture, while Akka is an implementation choice for one of the component of the business application deployed inside the architecture. vert.x is another open source implementation of such internal messaging mechanism but supporting more language: Java, Groovy, Ruby, JavaScript, Ceylon, Scala, and Kotlin.","title":"Differences between Akka and Kafka?"},{"location":"technology/faq/index.html#run-kafka-test-container-with-topologytestdriver","text":"Topology Test Driver is used without kafka, so there is no real need to use test container.","title":"Run Kafka Test Container with TopologyTestDriver"},{"location":"technology/faq/index.html#event-streams-resource-requirements","text":"See the detailed tables in the product documentation.","title":"Event streams resource requirements"},{"location":"technology/faq/index.html#security-setting","text":"On Kubernetes, Kafka can be configured with external and internal URLs. With Strimzi internal URLs are using TLS or Plain authentication, then TLS for encryption. If no authentication property is specified then the listener does not authenticate clients which connect through that listener. The listener will accept all connections without authentication. Mutual TLS authentication for internal communication looks like: - name : tls port : 9093 type : internal tls : true authentication : type : tls To connect any app (producer, consumer) we need a TLS user like: piVersion : kafka.strimzi.io/v1beta1 kind : KafkaUser metadata : name : tls-user labels : strimzi.io/cluster : vaccine-kafka spec : authentication : type : tls Then the following configurations need to be done for each app. For example in Quarkus app, we need to specify where to find the client certificate (for each Kafka TLS user a secret is created with the certificate (ca.crt) and a user password) oc describe secret tls-user Data ==== ca.crt: 1164 bytes user.crt: 1009 bytes user.key: 1704 bytes user.p12: 2374 bytes user.password: 12 bytes For Java client we need the following security settings, to specify from which secret to get the keystore password and certificate. The certificate will be mounted to /deployments/certs/user . %prod.kafka.security.protocol = SSL %prod.kafka.ssl.keystore.location = /deployments/certs/user/user.p12 %prod.kafka.ssl.keystore.type = PKCS12 quarkus.openshift.env.mapping.KAFKA_SSL_KEYSTORE_PASSWORD.from-secret = ${ KAFKA_USER : tls -user } quarkus.openshift.env.mapping.KAFKA_SSL_KEYSTORE_PASSWORD.with-key = user.password quarkus.openshift.mounts.user-cert.path = /deployments/certs/user quarkus.openshift.secret-volumes.user-cert.secret-name = ${ KAFKA_USER : tls -user } # To validate server side certificate we will mount it too with the following declaration quarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.from-secret = ${ KAFKA_CA_CERT_NAME : kafka -cluster-ca-cert } quarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.with-key = ca.password quarkus.openshift.mounts.kafka-cert.path = /deployments/certs/server quarkus.openshift.secret-volumes.kafka-cert.secret-name = ${ KAFKA_CA_CERT_NAME : kafka -cluster-ca-cert } For the server side certificate, it will be in a truststore, which is mounted to /deployments/certs/server and from a secret (this secret is created at the cluster level). Also because we also use TLS for encryption we need: %prod.kafka.ssl.protocol=TLSv1.2 Mutual TLS authentication is always used for the communication between Kafka brokers and ZooKeeper pods. For mutual, or two-way, authentication, both the server and the client present certificates. SCRAM: (Salted Challenge Response Authentication Mechanism) is an authentication protocol that can establish mutual authentication using passwords. Strimzi can configure Kafka to use SASL (Simple Authentication and Security Layer) SCRAM-SHA-512 to provide authentication on both unencrypted and encrypted client connections. The listener declaration: - name : external port : 9094 type : route tls : true authentication : type : scram-sha-512 Need a scram-user: apiVersion : kafka.strimzi.io/v1beta1 kind : KafkaUser metadata : name : scram-user labels : strimzi.io/cluster : vaccine-kafka spec : authentication : type : scram-sha-512 Then the app properties need: security.protocol = SASL_SSL %prod.quarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.from-secret = ${ KAFKA_CA_CERT_NAME : kafka -cluster-ca-cert } %prod.quarkus.openshift.env.mapping.KAFKA_SSL_TRUSTSTORE_PASSWORD.with-key = ca.password %prod.quarkus.openshift.env.mapping.KAFKA_SCRAM_PWD.from-secret = ${ KAFKA_USER : scram -user } %prod.quarkus.openshift.env.mapping.KAFKA_SCRAM_PWD.with-key = password %prod.quarkus.openshift.mounts.kafka-cert.path = /deployments/certs/server %prod.quarkus.openshift.secret-volumes.kafka-cert.secret-name = ${ KAFKA_CA_CERT_NAME : kafka -cluster-ca-cert }","title":"Security setting"},{"location":"technology/faq/index.html#verify-consumer-connection","text":"Here is an example of TLS authentication for Event streams ConsumerConfig values: bootstrap.servers = [eda-dev-kafka-bootstrap.eventstreams.svc:9093] check.crcs = true client.dns.lookup = default client.id = cold-chain-agent-c2c11228-d876-4db2-a16a-ea7826e358d2-StreamThread-1-restore-consumer client.rack = connections.max.idle.ms = 540000 default.api.timeout.ms = 60000 enable.auto.commit = false exclude.internal.topics = true fetch.max.bytes = 52428800 fetch.max.wait.ms = 500 fetch.min.bytes = 1 group.id = null group.instance.id = null heartbeat.interval.ms = 3000 interceptor.classes = [] internal.leave.group.on.close = false isolation.level = read_uncommitted key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer sasl.client.callback.handler.class = null sasl.jaas.config = null sasl.kerberos.kinit.cmd = /usr/bin/kinit sasl.kerberos.min.time.before.relogin = 60000 sasl.kerberos.service.name = null sasl.kerberos.ticket.renew.jitter = 0.05 sasl.kerberos.ticket.renew.window.factor = 0.8 sasl.login.callback.handler.class = null sasl.login.class = null sasl.login.refresh.buffer.seconds = 300 sasl.login.refresh.min.period.seconds = 60 sasl.login.refresh.window.factor = 0.8 sasl.login.refresh.window.jitter = 0.05 sasl.mechanism = GSSAPI security.protocol = SSL security.providers = null send.buffer.bytes = 131072 session.timeout.ms = 10000 ssl.cipher.suites = null ssl.enabled.protocols = [TLSv1.2] ssl.endpoint.identification.algorithm = https ssl.key.password = null ssl.keymanager.algorithm = SunX509 ssl.keystore.location = /deployments/certs/user/user.p12 ssl.keystore.password = [hidden] ssl.keystore.type = PKCS12 ssl.protocol = TLSv1.2 ssl.provider = null ssl.secure.random.implementation = null ssl.trustmanager.algorithm = PKIX ssl.truststore.location = /deployments/certs/server/ca.p12 ssl.truststore.password = [hidden] ssl.truststore.type = PKCS12 value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer","title":"Verify consumer connection"},{"location":"technology/faq/index.html#other-faqs","text":"IBM Event streams on Cloud FAQ FAQ from Confluent","title":"Other FAQs"},{"location":"technology/kafka-connect/index.html","text":"Kafka connect is an open source component for easily integrate external systems with Kafka. It works with any Kafka product like IBM Event Streams. It uses the concepts of source and sink connectors to ingest or deliver data to / from Kafka topics. The general concepts are detailed in the IBM Event streams product documentation . Here is a quick summary: Connector represents a logical job to move data from / to kafka to / from external systems. A lot of existing connectors can be reused, or you can implement your own . Workers are JVM running the connector. For production deployment workers run in cluster or \"distributed mode\", and leverage the group management protocol to scale task horizontally. Tasks : each worker coordinates a set of tasks to copy data. In distributed mode, task states are saved in kafka topics. They can be started, stopped at any time to support resilience, and scalable data pipeline. REST API to configure the connectors and monitors the tasks. The following figure illustrates a classical 'distributed' deployment of a Kafka Connect cluster. Workers are the running process to execute connectors and tasks. Workers are JVMs. Tasks are threads in a JVM. For fault tolerance and offeset management, Kafka connect uses Kafka topics ( -offsets, -config, -status ) When a connector is first submitted to the cluster, the workers rebalance the full set of connectors in the cluster and their tasks so that each worker has approximately the same amount of work. Connector and tasks are not guaranteed to run on the same instance in the cluster, especially if you have multiple tasks and multiple instances in your cluster. Connector coordinates data streaming by managing tasks The connector may be configured to add Converters (code used to translate data between Connect and the system sending or receiving data), and Transforms : Simple logic to alter each message produced by or sent to a connector. Connector keeps state into three topics, which may be created when the connectors start are: connect-configs : This topic stores the connector and task configurations. connect-offsets : This topic stores offsets for Kafka Connect. connect-status : This topic stores status updates of connectors and tasks. Characteristics Copy vast quantity of data from source to kafka: work at the datasource level. So when the source is a database, it uses JDBC API for example. Support streaming and batch. Scale from standalone, mono connector approach to start small, to run in parallel on distributed cluster. Copy data, externalizing transformation in other framework. Kafka Connect defines three models: data model, worker model and connector model. Fault tolerance When a worker fails: Tasks allocated in a worker that fails are reallocated to existing workers, and the task's state, offset, source record mapping to offset are reloaded from the different topics. Installation The Kafka connect framework fits well into a kubernetes deployment. We have different options for that deployment. We recommend reading the IBM event streams documentation for installing Kafka connect with IBM Event Streams or you can also leverage the Strimzi Kafka connect operator . With IBM Event Streams on premise, the connectors setup is part of the user admin console toolbox: Deploying connectors against an IBM Event Streams cluster, you need to have an API key with Manager role, to be able to create topic, produce and consume messages for all topics. As an extendable framework, Kafka Connect, can have new connector plugins. To deploy new connector, you need to use the kafka docker image which needs to be updated with the connector jars and redeployed to kubernetes cluster or to other environment. With IBM Event Streams on Openshift, the toolbox includes a kafka connect environment packaging, that defines a Dockerfile and configuration files to build your own image with the connectors jar files you need. The configuration files defines the properties to connect to Event Streams kafka brokers using API keys and SASL. The following public IBM messaging github account includes supported, open sourced, connectors (search for connector ). Here is the list of supported connectors for IBM Event Streams. Further Readings Apache Kafka connect documentation Confluent Connector Documentation IBM Event Streams Connectors or the list of supported connectors Our EDA Kafka connect lab for Cloud Object Storage Sink Our EDA Kafka connect lab for Cloud Object Storage S3 MQ to kafka lab repository Kafka Connector Sink lab for Mongodb MongoDB Connector for Apache Kafka","title":"Kafka Konnect"},{"location":"technology/kafka-connect/index.html#characteristics","text":"Copy vast quantity of data from source to kafka: work at the datasource level. So when the source is a database, it uses JDBC API for example. Support streaming and batch. Scale from standalone, mono connector approach to start small, to run in parallel on distributed cluster. Copy data, externalizing transformation in other framework. Kafka Connect defines three models: data model, worker model and connector model.","title":"Characteristics"},{"location":"technology/kafka-connect/index.html#fault-tolerance","text":"When a worker fails: Tasks allocated in a worker that fails are reallocated to existing workers, and the task's state, offset, source record mapping to offset are reloaded from the different topics.","title":"Fault tolerance"},{"location":"technology/kafka-connect/index.html#installation","text":"The Kafka connect framework fits well into a kubernetes deployment. We have different options for that deployment. We recommend reading the IBM event streams documentation for installing Kafka connect with IBM Event Streams or you can also leverage the Strimzi Kafka connect operator . With IBM Event Streams on premise, the connectors setup is part of the user admin console toolbox: Deploying connectors against an IBM Event Streams cluster, you need to have an API key with Manager role, to be able to create topic, produce and consume messages for all topics. As an extendable framework, Kafka Connect, can have new connector plugins. To deploy new connector, you need to use the kafka docker image which needs to be updated with the connector jars and redeployed to kubernetes cluster or to other environment. With IBM Event Streams on Openshift, the toolbox includes a kafka connect environment packaging, that defines a Dockerfile and configuration files to build your own image with the connectors jar files you need. The configuration files defines the properties to connect to Event Streams kafka brokers using API keys and SASL. The following public IBM messaging github account includes supported, open sourced, connectors (search for connector ). Here is the list of supported connectors for IBM Event Streams.","title":"Installation"},{"location":"technology/kafka-connect/index.html#further-readings","text":"Apache Kafka connect documentation Confluent Connector Documentation IBM Event Streams Connectors or the list of supported connectors Our EDA Kafka connect lab for Cloud Object Storage Sink Our EDA Kafka connect lab for Cloud Object Storage S3 MQ to kafka lab repository Kafka Connector Sink lab for Mongodb MongoDB Connector for Apache Kafka","title":"Further Readings"},{"location":"technology/kafka-overview/index.html","text":"Kafka Summary In this article we are summarizing what Apache Kafka is and grouping some references, notes and tips we gathered working with Kafka while producing the different assets for this Event Driven Architecture references. This content does not replace the excellent introduction every developer using Kafka should read. Introduction Kafka is a distributed real time event streaming platform with the following key capabilities: Publish and subscribe streams of records. Data are stored so consuming applications can pull the information they need, and keep track of what they have seen so far. It can handle hundreds of read and write operations per second from many producers and consumers. Atomic broadcast, send a record once, every subscriber gets it once. Store streams of data records on disk and replicate them within the distributed cluster for fault-tolerance. Persist data for a given time period before delete. Can grow elastically and transparently with no downtime. Built on top of the ZooKeeper synchronization service to keep topic, partitions and metadata highly available. Use cases The typical use cases where Kafka helps are: Centralize online data pipeline to decouple applications and microservices. Expose data to any application to consume. Pub/sub messaging for cloud native applications to improve communication inter microservices. Aggregation of event coming from multiple producers. Monitor distributed applications to produce centralized feed of operational data. Logs collector from multiple services. Implement event sourcing pattern out of the box, using configuration to keep message for a long time period. Data are replicated between brokers within the cluster and cross availability zones if needed. Manage loosely coupled communication between microservices. (See this note where we present a way to support a service mesh solution using asynchronous event) Architecture The diagram below presents Kafka's key components: Brokers Kafka runs as a cluster of broker servers that can, in theory, span multiple data centers. Each brokers manages data replication, topic/partition management, offset management and all the interesting delivery semantic. To cover multiple data centers within the same cluster, the network latency between data centers needs to be very low, at the 15ms or less, as there is a lot of communication between Kafka brokers and between Kafka brokers and zookeeper servers. The Kafka cluster stores streams of records in topics . Topic is referenced by producer to send data to, and subscribed by consumers to get data. Data in topic is persisted to file systems for a retention time period (Defined at the topic level). The file system can be network based. In the figure above, the Kafka brokers are allocated on three servers, with data within the topic are replicated two times. In production, it is recommended to use at least five nodes to authorize planned failure and un-planned failure, and when doing replicas, use a replica factor at least equals to three. Topics Topics represent end points to publish and consume records. Each record consists of a key, a value (the data payload as byte array), a timestamp and some metadata. Producers publish data records to topic and consumers subscribe to topics. When a record is produced without specifying a partition, a partition will be chosen using a hash of the key. If the record did not provide a timestamp, the producer will stamp the record with its current time (creation time or log append time). Producers hold a pool of buffers to keep records not yet transmitted to the server. Kafka store log data in its log.dir and topic maps to subdirectories in this log directory. Kafka uses topics with a pub/sub combined with queue model: it uses the concept of consumer group to divide the processing over a collection of consumer processes, running in parallel, and messages can be broadcasted to multiple groups. Consumer performs asynchronous pull to the connected brokers via the subscription to a topic. The figure below illustrates one topic having multiple partitions, replicated within the broker cluster: Partitions Partitions are basically used to parallelize the event processing when a single server would not be able to process all events, using the broker clustering. So to manage increase in the load of messages, Kafka uses partitions. Each broker may have zero or more partitions per topic. When creating topic we specify the number of partition to use. Kafka tolerates up to N-1 server failures without losing any messages. N is the replication factor for a given partition. Each partition is a time ordered immutable sequence of records, that are persisted for a long time period. It is a log. Topic is a labelled log. Consumers see messages in the order they are stored in the log. Each partition is replicated across a configurable number of servers for fault tolerance. The number of partition will depend on characteristics like the number of consumers, the traffic pattern, etc... You can have 2000 partitions per broker. Each partitioned message has a unique sequence id called offset (\"abcde, ab, a ...\" in the figure above are offsets). Those offset ids are defined when events arrived at the broker level, and are local to the partition. They are immutable. When a consumer reads a topic, it actually reads data from all the partitions. As a consumer reads data from a partition, it advances its offset. To read an event the consumer needs to use the topic name, the partition number and the last offset to read from. Brokers keep offset information in an hidden topic. Partitions guarantee that data with the same keys will be sent to the same consumer and in order. Partitions are saved to disk as append log. The older records are deleted after a given time period or if the size of log goes over a limit. It is possible to compact the log. The log compaction means, the last known value for each message key is kept. Compacted Topics are used in Streams processing for stateful operator to keep aggregate or grouping by key. You can read more about log compaction from the Kafka doc . Replication Each partition can be replicated across a number of servers. The replication factor is captured by the number of brokers to be used for replication. To ensure high availability it should be set to at least a value of three. Partitions have one leader and zero or more followers. The leader manages all the read and write requests for the partition. The followers replicate the leader content. We are addressing data replication in the high availability section below. Zookeeper Zookeeper is used to persist the component and platform states and it runs in cluster to ensure high availability. One zookeeper server is the leader and other are used in backup. Kafka does not keep state regarding consumers and producers. Depends on Kafka version, offsets are maintained in Zookeeper or in Kafka : newer versions use an internal Kafka topic called __consumer_offsets. In any case consumers can read next message (or from a specific offset) correctly even during broker server outrages. Access Controls are saved in Zookeeper Consumer group This is the way to group consumers so the processing of event is parallelized. The number of consumers in a group is the same as the number of partition defined in a topic. We are detailing consumer group implementation in this note High Availability As a distributed cluster, Kafka brokers ensure high availability to process new events. Topic has replication factor to support not loosing data in case of broker failure. You need at least 3 brokers to ensure availability and a replication factor set to 3 for each topic, so no data should be lost. In production it is recommended to use 5 brokers cluster to ensure the quorum is always set, but replica factor can still be set to 3. The brokers need to run on separate physical machines, and when cluster extends over multiple availability zone, a rack awareness configuration can be defined. Partition enables data locality, elasticity, scalability, high performance, parallelism, and fault tolerance. Each partition is replicated at least 3 times and allocated in different brokers. One replicas is the leader . In the case of broker failure (broker 1 in figure below), one of the existing partition in the remaining running brokers will take the leader role (e.g. red partition in broker 3): Replication and partition leadership The keys in the data record determine the partitioning of data in Kafka . The records with the same key will be in the same partition. As Kafka is keeping its cluster states in Apache Zookeeper , you also need to have at least a three node cluster for zookeeper. Writes to Zookeeper are only be performed on changes to the membership of consumer groups or on changes to the Kafka cluster itself. Assuming you are using the most recent Kafka version (after 0.9), it is possible to have a unique zookeeper cluster for multiple Kafka clusters. But the latency between Kafka and zookeeper needs to be under few milliseconds (< 15ms) anyway. Zookeepers and Brokers should have high availability communication via dual network, and each broker and node allocated on different racks and blades. Consumers and producers are using a list of bootstrap server names (also named advertiser.listeners) to contact the cluster. The list is used for cluster discovery, it does not need to keep the full set of server names or ip addresses. A Kafka cluster has exactly one broker that acts as the controller. Per design Kafka aims to run within a single data center. But it is still recommended to use multiple racks connected with low latency dual networks. With multiple racks you will have better fault tolerance, as one rack failure will impact only one broker. There is a configuration property to assign Kafka broker using rack awareness. (See this configuration from the product documentation). As introduced on the topic section above, data are replicated between brokers. The following diagram illustrates the best case scenario where followers fetch data from the partition leader, acknowledge the replications: Usually replicas is done in-sync, and the configuration settings specify the number of replicas in-sync needed: for example, a replicas 3 can have a minimum in-sync of 2, to tolerate 1 out of sync replica (1 broker outage). The leader maintains a set of in-sync-replicas (ISR) brokers: all the nodes which are up-to-date with the leader\u2019s log, and actively acknowledging new writes. Every write goes through the leader and is propagated to every node in the In Sync Replica set, or ISR. Followers consume messages from the leader just as a normal Kafka consumer would and apply them to their own log. Having the followers pull from the leader has the nice property of allowing the follower to naturally batch together log entries they are applying to their log. Once all nodes in the ISR have acknowledged the request, the leader considers it committed, and can acknowledge to the client. A message is considered committed when all in-sync replicas for that partition have applied it to their log. If a leader fails, followers elect a new one. The leadership of partitions is dynamic and changes as servers come and go. Applications do not need to take specific actions to handle the change in the leadership of a partition. The Kafka client library automatically reconnects to the new leader, although you will see increased latency while the cluster settles. Any replica in the ISR is eligible to be elected leader. When a leader waits to get acknowledge before committing a message there will be more potential leaders. With (#failure + 1) replicas there is no data lost. But there is a risk of having the single broker separated from the zookeeper cluster when network partition occurs. To tolerate f failures, both the majority vote and the ISR approach will wait for the same number of replicas to acknowledge before committing a message. Having higher replicas number like 5, will duplicate 5 times the data (more disk used) and impact throughput as data is sent 1+4 times over the network. Another important design distinction is that Kafka does not require that crashed nodes recover with all their data intact. Kafka protocol for allowing a replica to rejoin the ISR ensures that before rejoining, it must fully re-sync again even if it lost un-flushed data in its crash. When a producer sends message, it can control how to get the response from the committed message: wait for all replicas to succeed, wait for one acknowledge, fire and forget. Consumers receive only committed messages. Always assess the latency requirements and consumers needs. Throughput is linked to the number of partitions within a topic and having more consumers running in parallel. Consumers and producers should better run on separate servers than the brokers nodes. Running in parallel, also means the order of event arrivals will be lost. Most of the time, consumers are processing events from a unique partition and Kafka record to partition assignment will guarantee that records with the same key hashcode will be in the same partition. So orders are preserved within a partition. But if consumer needs to read from multiple partitions then if ordered records is needed, the consumer needs to rebuild the order with some complex logic. For high availability assess any potential single point of failure, such as server, rack, network, power supply... We recommend reading this event stream article for planning your Kafka on Kubernetes installation. For the consumers code update, the recreation of the consumer instance within the consumer group will trigger the partition rebalancing. This includes all the state of the aggregated data calculations that were persisted on disk. Until this process is finished real-time events are not processed. It is possible to limit this impact by setting the group.initial.rebalance.delay.ms to delay the rebalancing process one one instance of the consumer dies. Nevertheless the rebalancing will still occur when the updated consumer will rejoin the consumer group. When consumers are stream processing using Kafka streams, it is important to note that during the rollover the downstream processing will see a lag in event arrival: the time for the consumer to reread from the last committed offset. So if end to end timing is becoming important, we need to setup a standby consumer cluster (cluster B). This consumer group has different name, but does the same processing logic, and is consuming the same events from the same topic as the active consumer group cluster (cluster A). The difference is that they do not send events to the downstream topic until they are set up active. So to process the release cluster B is set active while cluster A is set inactive. The downstream will not be that much impacted. Finally to be exhaustive, the control of the segment size for the change log topic, may be considered to avoid having the stream processing doing a lot of computation to reload its state when it restarts. To add new broker, you can deploy the runtime to a new server / rack / blade, and give it a unique ID. Broker will process new topic, but it is possible to use tool to migrate some existing topic/ partitions to the new server. The tool is used to reassign partitions across brokers. An ideal partition distribution would ensure even data load and partition sizes across all brokers. High Availability in the context of Kubernetes deployment The combination of Kafka with Kubernetes seems to be a sound approach, but it is not that easy to achieve. Kubernetes workloads prefer to be stateless, Kafka is a stateful platform and manages its own brokers, and replications across known servers. It knows the underlying infrastructure. In Kubernetes, nodes and pods may change dynamically. Clients need to be able to access each of the broker directly once they get the connection metadata. Having a service which will round robin across all brokers in the cluster will not work with Kafka. The figure below illustrates a Kubernetes deployment, where zookeeper and Kafka brokers are allocated to 3 worker nodes, with some event driven microservices deployed in separate worker nodes. Those microservices are consumers and producers of events from one to many topics. The advantages of deploying Kafka on Kubernetes cluster is to facilitate the management of stateful sets, by scheduling both the persistence volume and broker pods in a clean rolling rehydration. Services add a logical name to access brokers for any deployed workload within the cluster. The virtual network also enables transparent TLS communication between components. For any Kubernetes deployment real high availability is constrained by the application / workload deployed on it. The Kubernetes platform supports high availability by having at least the following configuration: At least three master nodes (always an odd number of nodes). One is active at master, the others are in standby. The election of the master is using the quorum algorithm. Three proxy nodes. At least three worker nodes, but with zookeeper and Kafka clusters, we may need to have at least three more nodes as we do not want to have zookeeper and Kafka brokers sharing the same host as other pods if the Kafka traffic is supposed to grow. Externalize the management stack to three manager nodes Shared storage outside of the cluster to support private image registry, audit logs, and statefulset data persistence (like the Kafka broker file systems). Use etcd cluster: See recommendations from this article . The virtual IP manager assigns virtual IP addresses to master and proxy nodes and monitors the health of the cluster. It leverages etcd for storing information, so it is important that etcd is high available too and connected to low latency network below 10ms. Traditionally disaster recovery and high availability were always consider separated subjects. Now active/active deployment where workloads are deployed in different data centers, is becoming a common request. For sure, you need multiple Kafka Brokers, which will connect to the same ZooKeeper Ensemble running at least five nodes (you can tolerate the loss of one server during the planned maintenance of another server). One Zookeeper server acts as a lead and the two others as stand-by. The diagram above illustrates a simple deployment where zookeeper servers and Kafka brokers are running in pods, in different worker nodes. It is a viable solution to start deploying solution on top of Kafka. When you have bigger cluster, it may be interesting to separate Zookeeper from Kafka nodes to limit the risk of failover, as zookeeper keeps state of the Kafka cluster topology and metadata. You will limit to have both the zookeeper leader and one Kafka broker dying at the same time. We use Kubernetes anti-affinity to ensure they are scheduled onto separate worker nodes that the ones used by zookeeper. It uses the labels on pods with a rule like: **Kafka** pod should not run on same node as zookeeper pods . Here is an example of such spec: apiVersion : v1 kind : Pod metadata : name : with-pod-affinity spec : affinity : podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : labelSelector : matchExpressions : - key : name operator : In values : - gc-zookeeper topologyKey : kubernetes.io/hostname We recommend reading the \"running zookeeper in k8s tutorial\" for understanding such configuration. For optimum performance, provision a fast storage class for persistence volume. Kafka uses the log.dirs property to configure the driver to persist logs. So you need to define multiple volumes/ drives to support log.dirs . Zookeeper should not be used by other applications deployed in k8s cluster, it has to be dedicated for one Kafka cluster only. In a multi-cluster configuration being used for disaster recovery purposes, messages sent between clusters will have different offsets in the two clusters. It is usual to use timestamps for position information when restarting applications for recovery after a disaster. For Kafka streaming with stateful processing like joins, event aggregation and correlation coming from multiple partitions, it is not easy to achieve high availability cross clusters: in the strictest case every event must be processed by the streaming service exactly once. Which means: producer emits data to different sites and be able to re-emit in case of failure. Brokers are known by producer via a list of hostnames and port numbers. communications between zookeepers and cluster nodes are redundant and safe for data losses consumers ensure idempotence... They have to tolerate data duplication and manage data integrity in their persistence layer. Within Kafka's boundary, data will not be lost, when doing proper configuration, also to support high availability the complexity moves to the producer and the consumer implementation. Kafka configuration is an art and you need to tune the parameters by use case: Partition replication for at least 3 replicas. Recall that in case of node failure, coordination of partition re-assignments is provided with ZooKeeper. End to end latency needs to be measured from producer (when a message is sent) to consumer (when it is read). A consumer is able to get a message when the brokers finish replicating to all in-synch replicas. Use the producer buffering capability to pace the message to the broker. Can use memory or time based threshold via producer properties. Define the number of partitions to drive consumer parallelism. More consumers running in parallel the higher is the throughput. When using multiple partitions the global ordering of message is lost. Assess the retention hours to control when old messages in topic can be deleted. It is possible to keep messages forever, and for some application it makes fully sense. Control the maximum message size the server can receive. Zookeeper is not CPU intensive and each server should have a least 2 GB of heap space and 4GB reserved. Two CPUs per server should be sufficient. Servers keep their entire state machine in memory, and write every mutation to a durable WAL (Write Ahead Log) on persistent storage. To prevent the WAL from growing without bound, ZooKeeper servers periodically snapshot their in memory state to storage. Use fast and dynamically provisioned persistence storage for both WAL and snapshot. Performance Considerations Performance will vary depending of the current Kafka broker nodes load: in Kubernetes deployment, with small production topology, nodes may shared with other pods. It is recommended to control the environment with dedicated nodes for Kafka to achieve higher throughput. Performance will always depend on numerous factors including message throughput, message size, hardware, configuration settings, ... Performance may be linked to different focuses: Resilience: ensuring replication and not loosing data Throughput: ensuring message processing performance Payload size: support larger message Resilience When defining a topic, we need to specify the replicas factor to match the be at least 3 and then set the minimum number of in-sync replicas that specifies how may replicas must acknowledge a write to satisfy a producer that requests acknowledgments from all replicas. ( min.insync.replicas ). The replication of message data between brokers can consume a lot of network bandwidth so isolating replication traffic from application traffic can benefit performance. To achieve this, all replication traffic is configured to flow on a dedicated internal network. Throughput To achieve higher throughput the messages are not replicated across brokers and the acknowledgement can be set to only one broker. Expose resiliency to failures. The number of producers and consumers are aligned, and the number of partitions matches the number of consumers. All consumers are in the same consumer group. Measurement has to be done from the producer code. With 12 producers on a 3 brokers cluster and small payload (128 bytes), with 24 consumers the measured throughput is around 2.3 M messages / second. Payload size From measurement tests done using Kafka producer performance tool, there is a 1/log(s) curve, where below 10k bytes the performances are correct and then slowly degrade from 3000 msg /s (10k bytes msg) to 65 msg/s (515kb msg). To do performance test the event-streams-sample-producer github provides producer tool in Java, using a group of threads to run in multi cores machine. This project can be dockerized, and deployed in k8s. It uses the Kafka tool named: ProducerPerformance.java in the jar: <dependency> <groupId> org.apache.kafka </groupId> <artifactId> Kafka-tools </artifactId> </dependency> Parameter considerations There are a lot of factors and parameters that needs to be tuned to improve performance at the brokers threading level ( num.replica.fetchers, num.io.threads, num.network.threads, log.cleaner.threads ) and the pod resources constraints. See configuration documentation . OpenShift specifics When exposing the Kafka broker via Routes, the traffic is encrypted with TLS, so client needs to deal with TLS certificates and encryption. Routes are exposed via DNS and HAProxy router. The router will act as middleman between Kafka clients and brokers, adding latency, and it can become bottleneck. The traffic generated by client needs to be sized and in case of the router needs to be scaled up, and even isolate the routing by adding a separate router for the Kafka routes. Disaster Recovery With the current implementation it is recommended to have one cluster per data center / availability zone. Consumers and producers are co-located to the brokers cluster. When there are needs to keep some part of the data replicated in both data center, you need to assess what kind of data can be aggregated, and if Kafka mirroring tool can be used. The tool consumes from a source cluster, from a given topic, and produces to a destination cluster with the same named topic. It keeps the message key for partitioning, so order is preserved. The above diagram is using Kafka MirrorMaker with a master to slave deployment. Within the data center 2, the brokers are here to manage the topics and events. When there is no consumer running, nothing happen. Consumers and producers can be started when DC1 fails. This is the active/passive model. In fact, we could have consumers within the DC2 processing topics to manage a read-only model, keeping in memory their projection view, as presented in the CQRS pattern . The second solution is to use one mirror maker in each site, for each topic. This is an active - active topology: consumers and producers are on both sites. But to avoid infinite loop, we need to use naming convention for the topic, or only produce in the cluster of the main topic. Consumers consume from the replicated topic. When you want to deploy solution that spreads over multiple regions to support global streaming, you need to address the following challenges: How do you make data available to applications across multiple data centers? How to serve data closer to the geography? How to be compliant on regulations, like GDPR? How to address no duplication of records? Kafka 2.4 introduces the capability for a consumer to read messages from the closest replica using some rack-id and specific algorithm. This capability will help to extend the cluster to multiple data center and avoid having consumers going over WAN communication. Solution Considerations There are a set of design considerations to assess for each Kafka solution: Topics Performance is more a function of number of partitions than topics. Expect that each topic has at least one partition. When considering latency you should aim for limiting to hundreds of topic-partition per broker node. What of the most important question is what topics to use?. What is an event type? Should we use one topic to support multiple event types? Let define that an event type is linked to a main business entity like an Order, a ship, a FridgeredContainer. OrderCreated, OrderCancelled, OrderUpdated, OrderClosed are events linked to the states of the Order. The order of those events matter. So the natural approach is to use one topic per data type or schema, specially when using the topic as Event Sourcing where event order is important to build the audit log. You will use a unique partition to support that. The orderID is the partition key and all events related to the order are in the same topic. The important requirement to consider is the sequencing or event order. When event order is very important then use a unique partition, and use the entity unique identifier as key. Ordering is not preserved across partitions. When dealing with entity, independent entities may be in separate topics, when strongly related one may stay together. Other best practices: When event order is important use the same topic and use the entity unique identifier as partition key. When two entities are related together by containment relationship then they can be in the same topic. Different entities are separated to different topics. It is possible to group topics in coarse grained one when we discover that several consumers are listening to the same topics. Clearly define the partition key as it could be an compound key based on multiple entities. With Kafka stream, state store or KTable, you should separate the changelog topic from the others. Producers When developing a record producer you need to assess the following: What is the expected throughput to send events? Event size * average throughput combined with the expected latency help to compute buffer size. Can the producer batch events together to send them in batch over one send operation? Is there a risk for loosing communication? Tune the RETRIES_CONFIG and buffer size Assess once to exactly once delivery requirement. Look at idempotent producer. See implementation considerations discussion Consumers From the consumer point of view a set of items need to be addressed during design phase: Do you need to group consumers for parallel consumption of events? What is the processing done once the record is processed out of the topic? And how a record is supposed to be consumed?. How to persist consumer committed position? (the last offset that has been stored securely) Assess if offsets need to be persisted outside of Kafka?. From version 0.9 offset management is more efficient, and synchronous or asynchronous operations can be done from the consumer code. Does record time sensitive, and it is possible that consumers fall behind, so when a consumer restarts he can bypass missed records? Do the consumer needs to perform joins, aggregations between multiple partitions? See implementation considerations discussion Deployment In this section we provide the instructions for getting Kafka deployed in your vanilla Kubernetes environment through the Strimzi Kubernetes operator or getting the IBM Event Streams product (based on Kafka) deployed on your IBM Cloud Private/OpenShift cluster or in your IBM Cloud account as a managed service. Kubernetes Operator It is important to note that the deployment and management of stateful application in Kubernetes should, now, use the proposed Operator Framework introduced by Red Hat and Google. One important contribution is the Strimzi Kafka operator that simplifies the deployment of Kafka within k8s by adding a set of operators to deploy and manage Kafka clusters, topics, users and more.","title":"Kafka Summary"},{"location":"technology/kafka-overview/index.html#kafka-summary","text":"In this article we are summarizing what Apache Kafka is and grouping some references, notes and tips we gathered working with Kafka while producing the different assets for this Event Driven Architecture references. This content does not replace the excellent introduction every developer using Kafka should read.","title":"Kafka Summary"},{"location":"technology/kafka-overview/index.html#introduction","text":"Kafka is a distributed real time event streaming platform with the following key capabilities: Publish and subscribe streams of records. Data are stored so consuming applications can pull the information they need, and keep track of what they have seen so far. It can handle hundreds of read and write operations per second from many producers and consumers. Atomic broadcast, send a record once, every subscriber gets it once. Store streams of data records on disk and replicate them within the distributed cluster for fault-tolerance. Persist data for a given time period before delete. Can grow elastically and transparently with no downtime. Built on top of the ZooKeeper synchronization service to keep topic, partitions and metadata highly available.","title":"Introduction"},{"location":"technology/kafka-overview/index.html#use-cases","text":"The typical use cases where Kafka helps are: Centralize online data pipeline to decouple applications and microservices. Expose data to any application to consume. Pub/sub messaging for cloud native applications to improve communication inter microservices. Aggregation of event coming from multiple producers. Monitor distributed applications to produce centralized feed of operational data. Logs collector from multiple services. Implement event sourcing pattern out of the box, using configuration to keep message for a long time period. Data are replicated between brokers within the cluster and cross availability zones if needed. Manage loosely coupled communication between microservices. (See this note where we present a way to support a service mesh solution using asynchronous event)","title":"Use cases"},{"location":"technology/kafka-overview/index.html#architecture","text":"The diagram below presents Kafka's key components:","title":"Architecture"},{"location":"technology/kafka-overview/index.html#brokers","text":"Kafka runs as a cluster of broker servers that can, in theory, span multiple data centers. Each brokers manages data replication, topic/partition management, offset management and all the interesting delivery semantic. To cover multiple data centers within the same cluster, the network latency between data centers needs to be very low, at the 15ms or less, as there is a lot of communication between Kafka brokers and between Kafka brokers and zookeeper servers. The Kafka cluster stores streams of records in topics . Topic is referenced by producer to send data to, and subscribed by consumers to get data. Data in topic is persisted to file systems for a retention time period (Defined at the topic level). The file system can be network based. In the figure above, the Kafka brokers are allocated on three servers, with data within the topic are replicated two times. In production, it is recommended to use at least five nodes to authorize planned failure and un-planned failure, and when doing replicas, use a replica factor at least equals to three.","title":"Brokers"},{"location":"technology/kafka-overview/index.html#topics","text":"Topics represent end points to publish and consume records. Each record consists of a key, a value (the data payload as byte array), a timestamp and some metadata. Producers publish data records to topic and consumers subscribe to topics. When a record is produced without specifying a partition, a partition will be chosen using a hash of the key. If the record did not provide a timestamp, the producer will stamp the record with its current time (creation time or log append time). Producers hold a pool of buffers to keep records not yet transmitted to the server. Kafka store log data in its log.dir and topic maps to subdirectories in this log directory. Kafka uses topics with a pub/sub combined with queue model: it uses the concept of consumer group to divide the processing over a collection of consumer processes, running in parallel, and messages can be broadcasted to multiple groups. Consumer performs asynchronous pull to the connected brokers via the subscription to a topic. The figure below illustrates one topic having multiple partitions, replicated within the broker cluster:","title":"Topics"},{"location":"technology/kafka-overview/index.html#partitions","text":"Partitions are basically used to parallelize the event processing when a single server would not be able to process all events, using the broker clustering. So to manage increase in the load of messages, Kafka uses partitions. Each broker may have zero or more partitions per topic. When creating topic we specify the number of partition to use. Kafka tolerates up to N-1 server failures without losing any messages. N is the replication factor for a given partition. Each partition is a time ordered immutable sequence of records, that are persisted for a long time period. It is a log. Topic is a labelled log. Consumers see messages in the order they are stored in the log. Each partition is replicated across a configurable number of servers for fault tolerance. The number of partition will depend on characteristics like the number of consumers, the traffic pattern, etc... You can have 2000 partitions per broker. Each partitioned message has a unique sequence id called offset (\"abcde, ab, a ...\" in the figure above are offsets). Those offset ids are defined when events arrived at the broker level, and are local to the partition. They are immutable. When a consumer reads a topic, it actually reads data from all the partitions. As a consumer reads data from a partition, it advances its offset. To read an event the consumer needs to use the topic name, the partition number and the last offset to read from. Brokers keep offset information in an hidden topic. Partitions guarantee that data with the same keys will be sent to the same consumer and in order. Partitions are saved to disk as append log. The older records are deleted after a given time period or if the size of log goes over a limit. It is possible to compact the log. The log compaction means, the last known value for each message key is kept. Compacted Topics are used in Streams processing for stateful operator to keep aggregate or grouping by key. You can read more about log compaction from the Kafka doc .","title":"Partitions"},{"location":"technology/kafka-overview/index.html#replication","text":"Each partition can be replicated across a number of servers. The replication factor is captured by the number of brokers to be used for replication. To ensure high availability it should be set to at least a value of three. Partitions have one leader and zero or more followers. The leader manages all the read and write requests for the partition. The followers replicate the leader content. We are addressing data replication in the high availability section below.","title":"Replication"},{"location":"technology/kafka-overview/index.html#zookeeper","text":"Zookeeper is used to persist the component and platform states and it runs in cluster to ensure high availability. One zookeeper server is the leader and other are used in backup. Kafka does not keep state regarding consumers and producers. Depends on Kafka version, offsets are maintained in Zookeeper or in Kafka : newer versions use an internal Kafka topic called __consumer_offsets. In any case consumers can read next message (or from a specific offset) correctly even during broker server outrages. Access Controls are saved in Zookeeper","title":"Zookeeper"},{"location":"technology/kafka-overview/index.html#consumer-group","text":"This is the way to group consumers so the processing of event is parallelized. The number of consumers in a group is the same as the number of partition defined in a topic. We are detailing consumer group implementation in this note","title":"Consumer group"},{"location":"technology/kafka-overview/index.html#high-availability","text":"As a distributed cluster, Kafka brokers ensure high availability to process new events. Topic has replication factor to support not loosing data in case of broker failure. You need at least 3 brokers to ensure availability and a replication factor set to 3 for each topic, so no data should be lost. In production it is recommended to use 5 brokers cluster to ensure the quorum is always set, but replica factor can still be set to 3. The brokers need to run on separate physical machines, and when cluster extends over multiple availability zone, a rack awareness configuration can be defined. Partition enables data locality, elasticity, scalability, high performance, parallelism, and fault tolerance. Each partition is replicated at least 3 times and allocated in different brokers. One replicas is the leader . In the case of broker failure (broker 1 in figure below), one of the existing partition in the remaining running brokers will take the leader role (e.g. red partition in broker 3):","title":"High Availability"},{"location":"technology/kafka-overview/index.html#replication-and-partition-leadership","text":"The keys in the data record determine the partitioning of data in Kafka . The records with the same key will be in the same partition. As Kafka is keeping its cluster states in Apache Zookeeper , you also need to have at least a three node cluster for zookeeper. Writes to Zookeeper are only be performed on changes to the membership of consumer groups or on changes to the Kafka cluster itself. Assuming you are using the most recent Kafka version (after 0.9), it is possible to have a unique zookeeper cluster for multiple Kafka clusters. But the latency between Kafka and zookeeper needs to be under few milliseconds (< 15ms) anyway. Zookeepers and Brokers should have high availability communication via dual network, and each broker and node allocated on different racks and blades. Consumers and producers are using a list of bootstrap server names (also named advertiser.listeners) to contact the cluster. The list is used for cluster discovery, it does not need to keep the full set of server names or ip addresses. A Kafka cluster has exactly one broker that acts as the controller. Per design Kafka aims to run within a single data center. But it is still recommended to use multiple racks connected with low latency dual networks. With multiple racks you will have better fault tolerance, as one rack failure will impact only one broker. There is a configuration property to assign Kafka broker using rack awareness. (See this configuration from the product documentation). As introduced on the topic section above, data are replicated between brokers. The following diagram illustrates the best case scenario where followers fetch data from the partition leader, acknowledge the replications: Usually replicas is done in-sync, and the configuration settings specify the number of replicas in-sync needed: for example, a replicas 3 can have a minimum in-sync of 2, to tolerate 1 out of sync replica (1 broker outage). The leader maintains a set of in-sync-replicas (ISR) brokers: all the nodes which are up-to-date with the leader\u2019s log, and actively acknowledging new writes. Every write goes through the leader and is propagated to every node in the In Sync Replica set, or ISR. Followers consume messages from the leader just as a normal Kafka consumer would and apply them to their own log. Having the followers pull from the leader has the nice property of allowing the follower to naturally batch together log entries they are applying to their log. Once all nodes in the ISR have acknowledged the request, the leader considers it committed, and can acknowledge to the client. A message is considered committed when all in-sync replicas for that partition have applied it to their log. If a leader fails, followers elect a new one. The leadership of partitions is dynamic and changes as servers come and go. Applications do not need to take specific actions to handle the change in the leadership of a partition. The Kafka client library automatically reconnects to the new leader, although you will see increased latency while the cluster settles. Any replica in the ISR is eligible to be elected leader. When a leader waits to get acknowledge before committing a message there will be more potential leaders. With (#failure + 1) replicas there is no data lost. But there is a risk of having the single broker separated from the zookeeper cluster when network partition occurs. To tolerate f failures, both the majority vote and the ISR approach will wait for the same number of replicas to acknowledge before committing a message. Having higher replicas number like 5, will duplicate 5 times the data (more disk used) and impact throughput as data is sent 1+4 times over the network. Another important design distinction is that Kafka does not require that crashed nodes recover with all their data intact. Kafka protocol for allowing a replica to rejoin the ISR ensures that before rejoining, it must fully re-sync again even if it lost un-flushed data in its crash. When a producer sends message, it can control how to get the response from the committed message: wait for all replicas to succeed, wait for one acknowledge, fire and forget. Consumers receive only committed messages. Always assess the latency requirements and consumers needs. Throughput is linked to the number of partitions within a topic and having more consumers running in parallel. Consumers and producers should better run on separate servers than the brokers nodes. Running in parallel, also means the order of event arrivals will be lost. Most of the time, consumers are processing events from a unique partition and Kafka record to partition assignment will guarantee that records with the same key hashcode will be in the same partition. So orders are preserved within a partition. But if consumer needs to read from multiple partitions then if ordered records is needed, the consumer needs to rebuild the order with some complex logic. For high availability assess any potential single point of failure, such as server, rack, network, power supply... We recommend reading this event stream article for planning your Kafka on Kubernetes installation. For the consumers code update, the recreation of the consumer instance within the consumer group will trigger the partition rebalancing. This includes all the state of the aggregated data calculations that were persisted on disk. Until this process is finished real-time events are not processed. It is possible to limit this impact by setting the group.initial.rebalance.delay.ms to delay the rebalancing process one one instance of the consumer dies. Nevertheless the rebalancing will still occur when the updated consumer will rejoin the consumer group. When consumers are stream processing using Kafka streams, it is important to note that during the rollover the downstream processing will see a lag in event arrival: the time for the consumer to reread from the last committed offset. So if end to end timing is becoming important, we need to setup a standby consumer cluster (cluster B). This consumer group has different name, but does the same processing logic, and is consuming the same events from the same topic as the active consumer group cluster (cluster A). The difference is that they do not send events to the downstream topic until they are set up active. So to process the release cluster B is set active while cluster A is set inactive. The downstream will not be that much impacted. Finally to be exhaustive, the control of the segment size for the change log topic, may be considered to avoid having the stream processing doing a lot of computation to reload its state when it restarts. To add new broker, you can deploy the runtime to a new server / rack / blade, and give it a unique ID. Broker will process new topic, but it is possible to use tool to migrate some existing topic/ partitions to the new server. The tool is used to reassign partitions across brokers. An ideal partition distribution would ensure even data load and partition sizes across all brokers.","title":"Replication and partition leadership"},{"location":"technology/kafka-overview/index.html#high-availability-in-the-context-of-kubernetes-deployment","text":"The combination of Kafka with Kubernetes seems to be a sound approach, but it is not that easy to achieve. Kubernetes workloads prefer to be stateless, Kafka is a stateful platform and manages its own brokers, and replications across known servers. It knows the underlying infrastructure. In Kubernetes, nodes and pods may change dynamically. Clients need to be able to access each of the broker directly once they get the connection metadata. Having a service which will round robin across all brokers in the cluster will not work with Kafka. The figure below illustrates a Kubernetes deployment, where zookeeper and Kafka brokers are allocated to 3 worker nodes, with some event driven microservices deployed in separate worker nodes. Those microservices are consumers and producers of events from one to many topics. The advantages of deploying Kafka on Kubernetes cluster is to facilitate the management of stateful sets, by scheduling both the persistence volume and broker pods in a clean rolling rehydration. Services add a logical name to access brokers for any deployed workload within the cluster. The virtual network also enables transparent TLS communication between components. For any Kubernetes deployment real high availability is constrained by the application / workload deployed on it. The Kubernetes platform supports high availability by having at least the following configuration: At least three master nodes (always an odd number of nodes). One is active at master, the others are in standby. The election of the master is using the quorum algorithm. Three proxy nodes. At least three worker nodes, but with zookeeper and Kafka clusters, we may need to have at least three more nodes as we do not want to have zookeeper and Kafka brokers sharing the same host as other pods if the Kafka traffic is supposed to grow. Externalize the management stack to three manager nodes Shared storage outside of the cluster to support private image registry, audit logs, and statefulset data persistence (like the Kafka broker file systems). Use etcd cluster: See recommendations from this article . The virtual IP manager assigns virtual IP addresses to master and proxy nodes and monitors the health of the cluster. It leverages etcd for storing information, so it is important that etcd is high available too and connected to low latency network below 10ms. Traditionally disaster recovery and high availability were always consider separated subjects. Now active/active deployment where workloads are deployed in different data centers, is becoming a common request. For sure, you need multiple Kafka Brokers, which will connect to the same ZooKeeper Ensemble running at least five nodes (you can tolerate the loss of one server during the planned maintenance of another server). One Zookeeper server acts as a lead and the two others as stand-by. The diagram above illustrates a simple deployment where zookeeper servers and Kafka brokers are running in pods, in different worker nodes. It is a viable solution to start deploying solution on top of Kafka. When you have bigger cluster, it may be interesting to separate Zookeeper from Kafka nodes to limit the risk of failover, as zookeeper keeps state of the Kafka cluster topology and metadata. You will limit to have both the zookeeper leader and one Kafka broker dying at the same time. We use Kubernetes anti-affinity to ensure they are scheduled onto separate worker nodes that the ones used by zookeeper. It uses the labels on pods with a rule like: **Kafka** pod should not run on same node as zookeeper pods . Here is an example of such spec: apiVersion : v1 kind : Pod metadata : name : with-pod-affinity spec : affinity : podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : labelSelector : matchExpressions : - key : name operator : In values : - gc-zookeeper topologyKey : kubernetes.io/hostname We recommend reading the \"running zookeeper in k8s tutorial\" for understanding such configuration. For optimum performance, provision a fast storage class for persistence volume. Kafka uses the log.dirs property to configure the driver to persist logs. So you need to define multiple volumes/ drives to support log.dirs . Zookeeper should not be used by other applications deployed in k8s cluster, it has to be dedicated for one Kafka cluster only. In a multi-cluster configuration being used for disaster recovery purposes, messages sent between clusters will have different offsets in the two clusters. It is usual to use timestamps for position information when restarting applications for recovery after a disaster. For Kafka streaming with stateful processing like joins, event aggregation and correlation coming from multiple partitions, it is not easy to achieve high availability cross clusters: in the strictest case every event must be processed by the streaming service exactly once. Which means: producer emits data to different sites and be able to re-emit in case of failure. Brokers are known by producer via a list of hostnames and port numbers. communications between zookeepers and cluster nodes are redundant and safe for data losses consumers ensure idempotence... They have to tolerate data duplication and manage data integrity in their persistence layer. Within Kafka's boundary, data will not be lost, when doing proper configuration, also to support high availability the complexity moves to the producer and the consumer implementation. Kafka configuration is an art and you need to tune the parameters by use case: Partition replication for at least 3 replicas. Recall that in case of node failure, coordination of partition re-assignments is provided with ZooKeeper. End to end latency needs to be measured from producer (when a message is sent) to consumer (when it is read). A consumer is able to get a message when the brokers finish replicating to all in-synch replicas. Use the producer buffering capability to pace the message to the broker. Can use memory or time based threshold via producer properties. Define the number of partitions to drive consumer parallelism. More consumers running in parallel the higher is the throughput. When using multiple partitions the global ordering of message is lost. Assess the retention hours to control when old messages in topic can be deleted. It is possible to keep messages forever, and for some application it makes fully sense. Control the maximum message size the server can receive. Zookeeper is not CPU intensive and each server should have a least 2 GB of heap space and 4GB reserved. Two CPUs per server should be sufficient. Servers keep their entire state machine in memory, and write every mutation to a durable WAL (Write Ahead Log) on persistent storage. To prevent the WAL from growing without bound, ZooKeeper servers periodically snapshot their in memory state to storage. Use fast and dynamically provisioned persistence storage for both WAL and snapshot.","title":"High Availability in the context of Kubernetes deployment"},{"location":"technology/kafka-overview/index.html#performance-considerations","text":"Performance will vary depending of the current Kafka broker nodes load: in Kubernetes deployment, with small production topology, nodes may shared with other pods. It is recommended to control the environment with dedicated nodes for Kafka to achieve higher throughput. Performance will always depend on numerous factors including message throughput, message size, hardware, configuration settings, ... Performance may be linked to different focuses: Resilience: ensuring replication and not loosing data Throughput: ensuring message processing performance Payload size: support larger message","title":"Performance Considerations"},{"location":"technology/kafka-overview/index.html#resilience","text":"When defining a topic, we need to specify the replicas factor to match the be at least 3 and then set the minimum number of in-sync replicas that specifies how may replicas must acknowledge a write to satisfy a producer that requests acknowledgments from all replicas. ( min.insync.replicas ). The replication of message data between brokers can consume a lot of network bandwidth so isolating replication traffic from application traffic can benefit performance. To achieve this, all replication traffic is configured to flow on a dedicated internal network.","title":"Resilience"},{"location":"technology/kafka-overview/index.html#throughput","text":"To achieve higher throughput the messages are not replicated across brokers and the acknowledgement can be set to only one broker. Expose resiliency to failures. The number of producers and consumers are aligned, and the number of partitions matches the number of consumers. All consumers are in the same consumer group. Measurement has to be done from the producer code. With 12 producers on a 3 brokers cluster and small payload (128 bytes), with 24 consumers the measured throughput is around 2.3 M messages / second.","title":"Throughput"},{"location":"technology/kafka-overview/index.html#payload-size","text":"From measurement tests done using Kafka producer performance tool, there is a 1/log(s) curve, where below 10k bytes the performances are correct and then slowly degrade from 3000 msg /s (10k bytes msg) to 65 msg/s (515kb msg). To do performance test the event-streams-sample-producer github provides producer tool in Java, using a group of threads to run in multi cores machine. This project can be dockerized, and deployed in k8s. It uses the Kafka tool named: ProducerPerformance.java in the jar: <dependency> <groupId> org.apache.kafka </groupId> <artifactId> Kafka-tools </artifactId> </dependency>","title":"Payload size"},{"location":"technology/kafka-overview/index.html#parameter-considerations","text":"There are a lot of factors and parameters that needs to be tuned to improve performance at the brokers threading level ( num.replica.fetchers, num.io.threads, num.network.threads, log.cleaner.threads ) and the pod resources constraints. See configuration documentation .","title":"Parameter considerations"},{"location":"technology/kafka-overview/index.html#openshift-specifics","text":"When exposing the Kafka broker via Routes, the traffic is encrypted with TLS, so client needs to deal with TLS certificates and encryption. Routes are exposed via DNS and HAProxy router. The router will act as middleman between Kafka clients and brokers, adding latency, and it can become bottleneck. The traffic generated by client needs to be sized and in case of the router needs to be scaled up, and even isolate the routing by adding a separate router for the Kafka routes.","title":"OpenShift specifics"},{"location":"technology/kafka-overview/index.html#disaster-recovery","text":"With the current implementation it is recommended to have one cluster per data center / availability zone. Consumers and producers are co-located to the brokers cluster. When there are needs to keep some part of the data replicated in both data center, you need to assess what kind of data can be aggregated, and if Kafka mirroring tool can be used. The tool consumes from a source cluster, from a given topic, and produces to a destination cluster with the same named topic. It keeps the message key for partitioning, so order is preserved. The above diagram is using Kafka MirrorMaker with a master to slave deployment. Within the data center 2, the brokers are here to manage the topics and events. When there is no consumer running, nothing happen. Consumers and producers can be started when DC1 fails. This is the active/passive model. In fact, we could have consumers within the DC2 processing topics to manage a read-only model, keeping in memory their projection view, as presented in the CQRS pattern . The second solution is to use one mirror maker in each site, for each topic. This is an active - active topology: consumers and producers are on both sites. But to avoid infinite loop, we need to use naming convention for the topic, or only produce in the cluster of the main topic. Consumers consume from the replicated topic. When you want to deploy solution that spreads over multiple regions to support global streaming, you need to address the following challenges: How do you make data available to applications across multiple data centers? How to serve data closer to the geography? How to be compliant on regulations, like GDPR? How to address no duplication of records? Kafka 2.4 introduces the capability for a consumer to read messages from the closest replica using some rack-id and specific algorithm. This capability will help to extend the cluster to multiple data center and avoid having consumers going over WAN communication.","title":"Disaster Recovery"},{"location":"technology/kafka-overview/index.html#solution-considerations","text":"There are a set of design considerations to assess for each Kafka solution:","title":"Solution Considerations"},{"location":"technology/kafka-overview/index.html#topics_1","text":"Performance is more a function of number of partitions than topics. Expect that each topic has at least one partition. When considering latency you should aim for limiting to hundreds of topic-partition per broker node. What of the most important question is what topics to use?. What is an event type? Should we use one topic to support multiple event types? Let define that an event type is linked to a main business entity like an Order, a ship, a FridgeredContainer. OrderCreated, OrderCancelled, OrderUpdated, OrderClosed are events linked to the states of the Order. The order of those events matter. So the natural approach is to use one topic per data type or schema, specially when using the topic as Event Sourcing where event order is important to build the audit log. You will use a unique partition to support that. The orderID is the partition key and all events related to the order are in the same topic. The important requirement to consider is the sequencing or event order. When event order is very important then use a unique partition, and use the entity unique identifier as key. Ordering is not preserved across partitions. When dealing with entity, independent entities may be in separate topics, when strongly related one may stay together. Other best practices: When event order is important use the same topic and use the entity unique identifier as partition key. When two entities are related together by containment relationship then they can be in the same topic. Different entities are separated to different topics. It is possible to group topics in coarse grained one when we discover that several consumers are listening to the same topics. Clearly define the partition key as it could be an compound key based on multiple entities. With Kafka stream, state store or KTable, you should separate the changelog topic from the others.","title":"Topics"},{"location":"technology/kafka-overview/index.html#producers","text":"When developing a record producer you need to assess the following: What is the expected throughput to send events? Event size * average throughput combined with the expected latency help to compute buffer size. Can the producer batch events together to send them in batch over one send operation? Is there a risk for loosing communication? Tune the RETRIES_CONFIG and buffer size Assess once to exactly once delivery requirement. Look at idempotent producer. See implementation considerations discussion","title":"Producers"},{"location":"technology/kafka-overview/index.html#consumers","text":"From the consumer point of view a set of items need to be addressed during design phase: Do you need to group consumers for parallel consumption of events? What is the processing done once the record is processed out of the topic? And how a record is supposed to be consumed?. How to persist consumer committed position? (the last offset that has been stored securely) Assess if offsets need to be persisted outside of Kafka?. From version 0.9 offset management is more efficient, and synchronous or asynchronous operations can be done from the consumer code. Does record time sensitive, and it is possible that consumers fall behind, so when a consumer restarts he can bypass missed records? Do the consumer needs to perform joins, aggregations between multiple partitions? See implementation considerations discussion","title":"Consumers"},{"location":"technology/kafka-overview/index.html#deployment","text":"In this section we provide the instructions for getting Kafka deployed in your vanilla Kubernetes environment through the Strimzi Kubernetes operator or getting the IBM Event Streams product (based on Kafka) deployed on your IBM Cloud Private/OpenShift cluster or in your IBM Cloud account as a managed service.","title":"Deployment"},{"location":"technology/kafka-overview/index.html#kubernetes-operator","text":"It is important to note that the deployment and management of stateful application in Kubernetes should, now, use the proposed Operator Framework introduced by Red Hat and Google. One important contribution is the Strimzi Kafka operator that simplifies the deployment of Kafka within k8s by adding a set of operators to deploy and manage Kafka clusters, topics, users and more.","title":"Kubernetes Operator"},{"location":"technology/kafka-producers-consumers/index.html","text":"Kafka Producers & Consumers Kafka Producers A producer is a thread safe Kafka client API that publishes records to the cluster. It uses buffers, thread pool, and serializers to send data. They are stateless: the consumers is responsible to manage the offsets of the message they read. When the producer connects via the initial bootstrap connection, it gets the metadata about the topic - partition and the leader broker to connect to. The assignment of messages to partition is done following different algorithms: round-robin if there is no key specified, using the hash code of the key, or custom defined. We recommend reading IBM Event streams producer guidelines to understand how producers work with its configuration parameters. Design considerations When developing a record producer you need to assess the followings: What is the event payload to send? Is is a root aggregate, as defined in domain driven design, with value objects? Does it need to be kept in sequence to be used as event sourcing? or order does not matter? Remember that when order is important, messages need to go to the same topic. When multiple partitions are used, the messages with the same key will go to the same partition to guaranty the order. See related discussions from Martin Kleppmann on confluent web site . Also to be exhaustive, it is possible to get a producer doing retries that could generate duplicate records as acknowledges may take time to come: within a batch of n records, if the producer did not get all the n acknowledges on time, it may resend the batch. This is where 'idempotence' becomes important (see later section). Is there a strong requirement to manage the schema definition? If using one topic to manage all events about a business entity, then be sure to support a flexible avro schema . What is the expected throughput to send events? Event size * average throughput combined with the expected latency help to compute buffer size. By default, the buffer size is set at 32Mb, but can be configured with buffer.memory property. (See producer configuration API Can the producer batches events together to send them in batch over one send operation? By design Kafka producers batch events. Is there a risk for loosing communication? Tune the RETRIES_CONFIG and buffer size, and ensure to have at least 3 or even better 5, brokers within the cluster to maintain quorum in case of one failure. The client API is implemented to support reconnection. When deploying Kafka on Kubernetes, it is important to proxy the broker URLs with a proxy server outside of kubernetes. The HAProxy needs to scale, and as the Kafka traffic may be important, it may make sense to have a dedicated HAProxy for clients to brokers traffic. Assess exactly once delivery requirement. Look at idempotent producer: retries will not introduce duplicate records (see section below). Partitions help to scale the consumer processing of messages, but it also helps the producer to be more efficient as it can send message in parallel to different partition. Where the event timestamp comes from? Should the producer send operation set it or is it loaded from external data? Remember that LogAppendTime is considered to be processing time, and CreateTime is considered to be event time. Typical producer code structure The producer code, using java or python API, does the following steps: define producer properties create a producer instance Connect to the bootstrap URL, get a broker leader send event records and get resulting metadata. Producers are thread safe. The send() operation is asynchronous and returns immediately once record has been stored in the buffer of records, and it is possible to add a callback to process the broker acknowledgements. Here is an example of producer code from the our quick start. Kafka useful Producer APIs Here is a list of common API to use in your producer and consumer code. KafkaProducer A Kafka client that publishes records to the Kafka cluster. The send method is asynchronous. A producer is thread safe so we can have per topic to interface. ProducerRecord to be published to a topic RecordMetadata metadata for a record that has been acknowledged by the server. Properties to consider The following properties are helpful to tune at each topic and producer and will vary depending on the requirements: Properties Description BOOTSTRAP_SERVERS_CONFIG A comma-separated list of host:port values for all the brokers deployed. So producer may use any brokers KEY_SERIALIZER_CLASS_CONFIG and VALUE_SERIALIZER_CLASS_CONFIG convert the keys and values into byte arrays. Using default String serializer should be a good solution for Json payload. For streaming app, use customer serializer. ACKS_CONFIG specifies the minimum number of acknowledgments from a broker that the producer will wait for before considering a record send completed. Values = all, 0, and 1. 0 is for fire and forget. RETRIES_CONFIG specifies the number of times to attempt to resend a batch of events. ENABLE_IDEMPOTENCE_CONFIG Set to true, the number of retries will be maximized, and the acks will be set to All . TRANSACTION_ID A unique identifier for a producer. In case of multiple producer instances, a same ID will mean a second producers can commit the transaction. Epoch number, linked to the process ID, avoid having two producers doing this commit. If no transaction ID is specified, the transaction will be valid within a single session. How to support exactly once delivery Knowing that exactly once delivery is one of the hardest problems to solve in distributed systems, how Kafka does it?. Broker can fail or a network may respond slowly while a producer is trying to send events. Producer can set acknowledge level to control the delivery semantic to ensure not loosing data. The following semantic is supported: At least once : means the producer set ACKS_CONFIG=1 and get an acknowledgement message when the message sent, has been written to at least one time in the cluster (assume replicas = 3). If the ack is not received, the producer may retry, which may generate duplicate records in case the broker stops after saving to the topic and before sending back the acknowledgement message. At most semantic : means the producer will not do retry in case of no acknowledge received. It may create log and compensation, but the message may be lost. Exactly once means even if the producer sends the message twice the system will send only one message to the consumer. Once the consumer commits the read offset, it will not receive the message again, even if it restarts. Consumer offset needs to be in sync with produced event. At the best case scenario, with a replica factor set to 3, a broker responding on time to the producer, and with a consumer committing its offset and reading from the last committed offset it is possible to get only one message end to end. Sometime the brokers will not send acknowledge in expected time, and the producer may decide to send the records again, generating duplicate... To avoid duplicate message at the broker level, when acknowledge is set to ALL, the producer can also set idempotence flag: ENABLE_IDEMPOTENCE_CONFIG = true. With the idempotence property, the record sent, has a sequence number and a producer id, so that the broker keeps the last sequence number per producer and per partition. If a message is received with a lower sequence number, it means a producer is doing some retries on record already processed, so the broker will drop it, to avoid having duplicate records per partition. If the id is greater than current id known by the broker, the broker will create an OutOfSequence exception, which may be fatal as records may have been lost. The sequence number is persisted in a log so even in case of broker leader failure, the new leader will have a good view of the states of the system. The replication mechanism guarantees that, when a message is written to the leader replica, it will be replicated to all available replicas. As soon as you want to get acknowledge of all replicates, it is obvious to set idempotence to true. It does not impact performance. To add to this discussion, as topic may have multiple partitions, idempotent producers do not provide guarantees for writes across multiple Topic-Partition. For that Kafka supports atomic writes to all partitions, so that all records are saved or none of them are visible to consumers. This transaction control is done by using the producer transactional API, and a transactional protocol with coordinator and control message. Here is an example of such configuration that can be done in a producer constructor method: producerProps . put ( \"enable.idempotence\" , \"true\" ); producerProps . put ( \"transactional.id\" , \"prod-1\" ); KafkaProducer . initTransactions () initTransactions() registers the producer with the broker as one that can use transaction, identifying it by its transactional.id and a sequence number, or epoch. Epoch is used to avoid an old producer to commit a transaction while a new producer instance was created for that and continues its work. Kafka streams with consume-process-produce loop requires transaction and exactly once. Even committing its read offset is part of the transaction. So Producer API has a sendOffsetsToTransaction method . See the KIP 98 for details. In case of multiple partitions, the broker will store a list of all updated partitions for a given transaction. To support transaction a transaction coordinator keeps its states into an internal topic (TransactionLog). Control messages are added to the main topic but never exposed to the 'user', so that consumers have the knowledge if a transaction is committed or not. See the code in order command microservice. The consumer is also interested to configure the reading of the transactional messages by defining the isolation level. Consumer waits to read transactional messages until the associated transaction has been committed. Here is an example of consumer code and configuration consumerProps . put ( \"enable.auto.commit\" , \"false\" ); consumerProps . put ( \"isolation.level\" , \"read_committed\" ); With read_committed , no message that was written to the input topic in the same transaction will be read by this consumer until message replicas are all written. In consume-process-produce loop, producer commits its offset with code, and specifies the last offset to read. offsetsToCommit . put ( partition , new OffsetAndMetadata ( offset + 1 )) producer . sendOffsetsToTransaction ( offsetsToCommit , \"consumer-group-id\" ); The producer then commits the transaction. try { KafkaProducer . beginTransaction (); ProducerRecord < String , String > record = new ProducerRecord <> ( ApplicationConfig . ORDER_COMMAND_TOPIC , key , value ); Future < RecordMetadata > send = KafkaProducer . send ( record , callBackFunction ); KafkaProducer . commitTransaction (); } catch ( KafkaException e ){ KafkaProducer . abortTransaction (); } There is an interesting article from the Baeldung team about exactly once processing in Kafka with code example which we have re-used to implement the order processing in our Reefer Container Shipment reference application and explained here More readings Creating advanced Kafka producer in java - Cloudurable Confluent blog: Exactly-once Semantics are Possible: Here\u2019s How Kafka Does it Kafka Consumers This note includes some quick summary of different practices we discovered and studied over time. It may be useful for beginner or seasoned developers who want a refresh after some time far away from Kafka... Important concepts Consumers belong to consumer groups . You specify the group name as part of the consumer connection parameters using the group.id configuration: properties . put ( ConsumerConfig . GROUP_ID_CONFIG , groupid ); Consumer groups are grouping consumers to cooperate to consume messages from one or more topics. Consumers can run in separate hosts and separate processes. The figure below represents 2 consumer apps belonging to one consumer group. Consumer 1 is getting data from 2 partitions, while consumer 2 is getting from one partition. When a consumer is unique in a group, it will get data from all partitions. There is always at least one consumer per partition. One broker is responsible to be the consumer group coordinator which is responsible for assigning partitions to the consumers in the group. The first consumer to join the group will be the group leader. It will get the list of consumers and it is responsible for assigning a subset of partitions to each consumer Membership in a consumer group is maintained dynamically. Consumers send hearbeats to the group coordinator broker (see configuration like heartbeat.interval.ms ) and session.timeout.ms . Partition assignement is done by different strategies from range, round robin, sticky and cooperative sticky (See partition assignement strategy ). When a consumer fails, the partitions assigned to it will be reassigned to another consumer in the same group. When a new consumer joins the group, partitions will be moved from existing consumers to the new one. Group rebalancing is also used when new partitions are added to one of the subscribed topics. The group will automatically detect the new partitions through periodic metadata refreshes and assign them to members of the group. During a rebalance, depending of the strategy, consumers may not consume messages (Need Kafka 2.4+ to get cooperative balancing feature). Implementing a Topic consumer is using the Kafka KafkaConsumer class which the API documentation is a must read. It is interesting to note that: To support the same semantic of a queue processing like other integration messaging systems, you need to have all the consumers assigned to a single consumer group, so that each record delivery would be balanced over the group like with a queue. To support pub/sub like other messaging systems, each consumer would have its own consumer group, and subscribes to all the records published to the topic. With client.rack setting a consumer can consume from a local replica, which will have better latency when using a stretched cluster. The implementation is simple for a single thread consumer, and the code structure looks like: prepare the consumer properties create an instance of KafkaConsumer to subscribe to at least one topic loop on polling events: the consumer ensures its liveness with the broker via the poll API. It will get n records per poll. process the ConsumerRecords and commit the offset by code or use the autocommit attribute of the consumer As long as the consumer continues to call poll(), it will stay in the group and continue to receive messages from the partitions it was assigned. When the consumer does not send heartbeats for a duration of session.timeout.ms , then it is considered dead and its partitions will be reassigned. Examples of Java consumers can be found in the order management microservice project under the order-command-ms folder. We are proposing a deep dive study on this manual offset commit in this consumer code that persists events to cassandra. Example of Javascript implementation is in this repository/folder But the complexity comes from the offset management and multithreading needs. So the following important considerations need to be addressed while implementing a consumer. Assess number of consumers needed The KafkaConsumer is not thread safe so it is recommended to run in a unique thread. But if needed you can implement a multi-threads solution, but as each thread will open a TCP connection to the Kafka broker, be sure to close the connection to avoid memory leak. The alternate is to start n processes (JVM process). If you need multiple consumers running in parallel to scale horizontally, you have to define multiple partitions while configuring the topic and use fine-grained control over offset persistence. You\u2019ll use one consumer per partition of a topic. This consumer-per-partition pattern maximizes throughput. When consumers run in parallel and you use multiple threads per consumer you need to be sure the total number of threads across all instances do not exceed the total number of partitions in the topic. Also, a consumer can subscribe to multiple topics. The brokers are doing rebalancing of the assignment of topic-partition to a consumer that belong to a group. When creating a new consumer you can specify the group id in the options. Offset management Recall that offset is just a numeric identifier of a consumer position of the last record read within a partition. Consumers periodically need to commit the offsets they have received, to present a recovery point in case of failure. To commit offset (via API or automatically) the consumer sends a message to Kafka broker to the special topic named __consumer_offsets to keep the committed offset for each partition. Consumers do a read commit for the last processed record: When a consumer starts and is assigned a partition to consume, it will start at its group's committed offset or latest or earliest as auto.offset.reset (When there is a committed offset, the auto.offset.reset property is not used). As shown in the figure below, it is possible to get duplicates if the last message processed by the consumer before crashing and committing its offset, is bigger than the last commited offset. Source: Kafka definitive guide book from Todd Palino, Gwen Shapira In the opposite, if the last committed offset is after the last processed messages and there were multiple messages returned in the poll, then those messages may be lost. It is possible to commit by calling API or by setting some properties at the consumer creation level to enable autocommit offset enable.auto.commit . When doing manual offset commit, there are two types of approaches: offsets\u2014synchronous asynchronous As soon as you are coding manual commit, it is strongly recommended to implement the ConsumerRebalanceListener interface to be able to do the state modifications when the topic is rebalanced. If a consumer fails after processing a message but before committing its offset, the committed offset information will not reflect the processing of the message. This means that the message will be processed again by the next consumer in that group to be assigned the partition. Assess if it is acceptable to loose messages from topic. If so, when a consumer restarts it will start consuming the topic from the latest committed offset within the partition allocated to itself. As storing a message to an external system and storing the offsets are two separate operations, and in case of failure between them, it is possible to have stale offsets, which will introduce duplicate messages when consumers restart to process from last known committed offset. In this case, consumer's idempotence is needed to support updating the same row in the table, or use the event timestamp as update timestamp in the database record or use other clever solution. As presented in the producer coding practice, using transaction to support \"exactly-once\", also means the consumers should read committed data only. This can be achieved by setting the isolation.level=read_committed in the consumer's configuration. The last offset will be the first message in the partition belonging to an open not yet committed transaction. This offset is known as the 'Last Stable Offset'(LSO). Finally in the case where consumers are set to auto commit, it means the offset if committed at the poll() level and if the service crashed while processing of this record as: then the record (partition 0 - offset 4) will never be processed. Producer transaction When consuming from a Kafka topic and producing to another topic, like in Kafka Stream, but also in CQRS implementation, we can use the producer's transaction feature to send the committed offset message and the new records in the second topic in the same transaction. This can be seen as a consume-transform-produce loop pattern so that every input event is processed exactly once. An example of such pattern in done in the order management microservice - command part . Consumer lag The consumer lag for a partition is the difference between the offset of the most recently published message and the consumer's committed offset. If the lag starts to grow, it means the consumer is not able to keep up with the producer's pace. The risk, is that slow consumer may fall behind, and when partition management may remove old log segments, leading the consumer to jump forward to continnue on the next log segment. Consumer may have lost messages. You can use the Kafka-consumer-groups tool to see the consumer lag. Kafka useful Consumer APIs KafkaConsumer a topic consumer which support: transparently handles brokers failure transparently adapt to partition migration within the cluster support grouping for load balancing among consumers maintains TCP connections to the necessary brokers to fetch data subscribe to multiple topics and being part of consumer groups each partition is assigned to exactly one consumer in the group if a process fails, the partitions assigned to it will be reassigned to other consumers in the same group ConsumerRecords holds the list ConsumerRecord per partition for a particular topic. ConsumerRecord A key/value pair to be received from Kafka. This also consists of a topic name and a partition number from which the record is being received, an offset that points to the record in a Kafka partition, and a timestamp References IBM Event Streams - Consuming messages KafkaConsumer class","title":"Producer - Consumer"},{"location":"technology/kafka-producers-consumers/index.html#kafka-producers-consumers","text":"","title":"Kafka Producers &amp; Consumers"},{"location":"technology/kafka-producers-consumers/index.html#kafka-producers","text":"A producer is a thread safe Kafka client API that publishes records to the cluster. It uses buffers, thread pool, and serializers to send data. They are stateless: the consumers is responsible to manage the offsets of the message they read. When the producer connects via the initial bootstrap connection, it gets the metadata about the topic - partition and the leader broker to connect to. The assignment of messages to partition is done following different algorithms: round-robin if there is no key specified, using the hash code of the key, or custom defined. We recommend reading IBM Event streams producer guidelines to understand how producers work with its configuration parameters.","title":"Kafka Producers"},{"location":"technology/kafka-producers-consumers/index.html#design-considerations","text":"When developing a record producer you need to assess the followings: What is the event payload to send? Is is a root aggregate, as defined in domain driven design, with value objects? Does it need to be kept in sequence to be used as event sourcing? or order does not matter? Remember that when order is important, messages need to go to the same topic. When multiple partitions are used, the messages with the same key will go to the same partition to guaranty the order. See related discussions from Martin Kleppmann on confluent web site . Also to be exhaustive, it is possible to get a producer doing retries that could generate duplicate records as acknowledges may take time to come: within a batch of n records, if the producer did not get all the n acknowledges on time, it may resend the batch. This is where 'idempotence' becomes important (see later section). Is there a strong requirement to manage the schema definition? If using one topic to manage all events about a business entity, then be sure to support a flexible avro schema . What is the expected throughput to send events? Event size * average throughput combined with the expected latency help to compute buffer size. By default, the buffer size is set at 32Mb, but can be configured with buffer.memory property. (See producer configuration API Can the producer batches events together to send them in batch over one send operation? By design Kafka producers batch events. Is there a risk for loosing communication? Tune the RETRIES_CONFIG and buffer size, and ensure to have at least 3 or even better 5, brokers within the cluster to maintain quorum in case of one failure. The client API is implemented to support reconnection. When deploying Kafka on Kubernetes, it is important to proxy the broker URLs with a proxy server outside of kubernetes. The HAProxy needs to scale, and as the Kafka traffic may be important, it may make sense to have a dedicated HAProxy for clients to brokers traffic. Assess exactly once delivery requirement. Look at idempotent producer: retries will not introduce duplicate records (see section below). Partitions help to scale the consumer processing of messages, but it also helps the producer to be more efficient as it can send message in parallel to different partition. Where the event timestamp comes from? Should the producer send operation set it or is it loaded from external data? Remember that LogAppendTime is considered to be processing time, and CreateTime is considered to be event time.","title":"Design considerations"},{"location":"technology/kafka-producers-consumers/index.html#typical-producer-code-structure","text":"The producer code, using java or python API, does the following steps: define producer properties create a producer instance Connect to the bootstrap URL, get a broker leader send event records and get resulting metadata. Producers are thread safe. The send() operation is asynchronous and returns immediately once record has been stored in the buffer of records, and it is possible to add a callback to process the broker acknowledgements. Here is an example of producer code from the our quick start.","title":"Typical producer code structure"},{"location":"technology/kafka-producers-consumers/index.html#kafka-useful-producer-apis","text":"Here is a list of common API to use in your producer and consumer code. KafkaProducer A Kafka client that publishes records to the Kafka cluster. The send method is asynchronous. A producer is thread safe so we can have per topic to interface. ProducerRecord to be published to a topic RecordMetadata metadata for a record that has been acknowledged by the server.","title":"Kafka useful Producer APIs"},{"location":"technology/kafka-producers-consumers/index.html#properties-to-consider","text":"The following properties are helpful to tune at each topic and producer and will vary depending on the requirements: Properties Description BOOTSTRAP_SERVERS_CONFIG A comma-separated list of host:port values for all the brokers deployed. So producer may use any brokers KEY_SERIALIZER_CLASS_CONFIG and VALUE_SERIALIZER_CLASS_CONFIG convert the keys and values into byte arrays. Using default String serializer should be a good solution for Json payload. For streaming app, use customer serializer. ACKS_CONFIG specifies the minimum number of acknowledgments from a broker that the producer will wait for before considering a record send completed. Values = all, 0, and 1. 0 is for fire and forget. RETRIES_CONFIG specifies the number of times to attempt to resend a batch of events. ENABLE_IDEMPOTENCE_CONFIG Set to true, the number of retries will be maximized, and the acks will be set to All . TRANSACTION_ID A unique identifier for a producer. In case of multiple producer instances, a same ID will mean a second producers can commit the transaction. Epoch number, linked to the process ID, avoid having two producers doing this commit. If no transaction ID is specified, the transaction will be valid within a single session.","title":"Properties to consider"},{"location":"technology/kafka-producers-consumers/index.html#how-to-support-exactly-once-delivery","text":"Knowing that exactly once delivery is one of the hardest problems to solve in distributed systems, how Kafka does it?. Broker can fail or a network may respond slowly while a producer is trying to send events. Producer can set acknowledge level to control the delivery semantic to ensure not loosing data. The following semantic is supported: At least once : means the producer set ACKS_CONFIG=1 and get an acknowledgement message when the message sent, has been written to at least one time in the cluster (assume replicas = 3). If the ack is not received, the producer may retry, which may generate duplicate records in case the broker stops after saving to the topic and before sending back the acknowledgement message. At most semantic : means the producer will not do retry in case of no acknowledge received. It may create log and compensation, but the message may be lost. Exactly once means even if the producer sends the message twice the system will send only one message to the consumer. Once the consumer commits the read offset, it will not receive the message again, even if it restarts. Consumer offset needs to be in sync with produced event. At the best case scenario, with a replica factor set to 3, a broker responding on time to the producer, and with a consumer committing its offset and reading from the last committed offset it is possible to get only one message end to end. Sometime the brokers will not send acknowledge in expected time, and the producer may decide to send the records again, generating duplicate... To avoid duplicate message at the broker level, when acknowledge is set to ALL, the producer can also set idempotence flag: ENABLE_IDEMPOTENCE_CONFIG = true. With the idempotence property, the record sent, has a sequence number and a producer id, so that the broker keeps the last sequence number per producer and per partition. If a message is received with a lower sequence number, it means a producer is doing some retries on record already processed, so the broker will drop it, to avoid having duplicate records per partition. If the id is greater than current id known by the broker, the broker will create an OutOfSequence exception, which may be fatal as records may have been lost. The sequence number is persisted in a log so even in case of broker leader failure, the new leader will have a good view of the states of the system. The replication mechanism guarantees that, when a message is written to the leader replica, it will be replicated to all available replicas. As soon as you want to get acknowledge of all replicates, it is obvious to set idempotence to true. It does not impact performance. To add to this discussion, as topic may have multiple partitions, idempotent producers do not provide guarantees for writes across multiple Topic-Partition. For that Kafka supports atomic writes to all partitions, so that all records are saved or none of them are visible to consumers. This transaction control is done by using the producer transactional API, and a transactional protocol with coordinator and control message. Here is an example of such configuration that can be done in a producer constructor method: producerProps . put ( \"enable.idempotence\" , \"true\" ); producerProps . put ( \"transactional.id\" , \"prod-1\" ); KafkaProducer . initTransactions () initTransactions() registers the producer with the broker as one that can use transaction, identifying it by its transactional.id and a sequence number, or epoch. Epoch is used to avoid an old producer to commit a transaction while a new producer instance was created for that and continues its work. Kafka streams with consume-process-produce loop requires transaction and exactly once. Even committing its read offset is part of the transaction. So Producer API has a sendOffsetsToTransaction method . See the KIP 98 for details. In case of multiple partitions, the broker will store a list of all updated partitions for a given transaction. To support transaction a transaction coordinator keeps its states into an internal topic (TransactionLog). Control messages are added to the main topic but never exposed to the 'user', so that consumers have the knowledge if a transaction is committed or not. See the code in order command microservice. The consumer is also interested to configure the reading of the transactional messages by defining the isolation level. Consumer waits to read transactional messages until the associated transaction has been committed. Here is an example of consumer code and configuration consumerProps . put ( \"enable.auto.commit\" , \"false\" ); consumerProps . put ( \"isolation.level\" , \"read_committed\" ); With read_committed , no message that was written to the input topic in the same transaction will be read by this consumer until message replicas are all written. In consume-process-produce loop, producer commits its offset with code, and specifies the last offset to read. offsetsToCommit . put ( partition , new OffsetAndMetadata ( offset + 1 )) producer . sendOffsetsToTransaction ( offsetsToCommit , \"consumer-group-id\" ); The producer then commits the transaction. try { KafkaProducer . beginTransaction (); ProducerRecord < String , String > record = new ProducerRecord <> ( ApplicationConfig . ORDER_COMMAND_TOPIC , key , value ); Future < RecordMetadata > send = KafkaProducer . send ( record , callBackFunction ); KafkaProducer . commitTransaction (); } catch ( KafkaException e ){ KafkaProducer . abortTransaction (); } There is an interesting article from the Baeldung team about exactly once processing in Kafka with code example which we have re-used to implement the order processing in our Reefer Container Shipment reference application and explained here","title":"How to support exactly once delivery"},{"location":"technology/kafka-producers-consumers/index.html#more-readings","text":"Creating advanced Kafka producer in java - Cloudurable Confluent blog: Exactly-once Semantics are Possible: Here\u2019s How Kafka Does it","title":"More readings"},{"location":"technology/kafka-producers-consumers/index.html#kafka-consumers","text":"This note includes some quick summary of different practices we discovered and studied over time. It may be useful for beginner or seasoned developers who want a refresh after some time far away from Kafka...","title":"Kafka Consumers"},{"location":"technology/kafka-producers-consumers/index.html#important-concepts","text":"Consumers belong to consumer groups . You specify the group name as part of the consumer connection parameters using the group.id configuration: properties . put ( ConsumerConfig . GROUP_ID_CONFIG , groupid ); Consumer groups are grouping consumers to cooperate to consume messages from one or more topics. Consumers can run in separate hosts and separate processes. The figure below represents 2 consumer apps belonging to one consumer group. Consumer 1 is getting data from 2 partitions, while consumer 2 is getting from one partition. When a consumer is unique in a group, it will get data from all partitions. There is always at least one consumer per partition. One broker is responsible to be the consumer group coordinator which is responsible for assigning partitions to the consumers in the group. The first consumer to join the group will be the group leader. It will get the list of consumers and it is responsible for assigning a subset of partitions to each consumer Membership in a consumer group is maintained dynamically. Consumers send hearbeats to the group coordinator broker (see configuration like heartbeat.interval.ms ) and session.timeout.ms . Partition assignement is done by different strategies from range, round robin, sticky and cooperative sticky (See partition assignement strategy ). When a consumer fails, the partitions assigned to it will be reassigned to another consumer in the same group. When a new consumer joins the group, partitions will be moved from existing consumers to the new one. Group rebalancing is also used when new partitions are added to one of the subscribed topics. The group will automatically detect the new partitions through periodic metadata refreshes and assign them to members of the group. During a rebalance, depending of the strategy, consumers may not consume messages (Need Kafka 2.4+ to get cooperative balancing feature). Implementing a Topic consumer is using the Kafka KafkaConsumer class which the API documentation is a must read. It is interesting to note that: To support the same semantic of a queue processing like other integration messaging systems, you need to have all the consumers assigned to a single consumer group, so that each record delivery would be balanced over the group like with a queue. To support pub/sub like other messaging systems, each consumer would have its own consumer group, and subscribes to all the records published to the topic. With client.rack setting a consumer can consume from a local replica, which will have better latency when using a stretched cluster. The implementation is simple for a single thread consumer, and the code structure looks like: prepare the consumer properties create an instance of KafkaConsumer to subscribe to at least one topic loop on polling events: the consumer ensures its liveness with the broker via the poll API. It will get n records per poll. process the ConsumerRecords and commit the offset by code or use the autocommit attribute of the consumer As long as the consumer continues to call poll(), it will stay in the group and continue to receive messages from the partitions it was assigned. When the consumer does not send heartbeats for a duration of session.timeout.ms , then it is considered dead and its partitions will be reassigned. Examples of Java consumers can be found in the order management microservice project under the order-command-ms folder. We are proposing a deep dive study on this manual offset commit in this consumer code that persists events to cassandra. Example of Javascript implementation is in this repository/folder But the complexity comes from the offset management and multithreading needs. So the following important considerations need to be addressed while implementing a consumer.","title":"Important concepts"},{"location":"technology/kafka-producers-consumers/index.html#assess-number-of-consumers-needed","text":"The KafkaConsumer is not thread safe so it is recommended to run in a unique thread. But if needed you can implement a multi-threads solution, but as each thread will open a TCP connection to the Kafka broker, be sure to close the connection to avoid memory leak. The alternate is to start n processes (JVM process). If you need multiple consumers running in parallel to scale horizontally, you have to define multiple partitions while configuring the topic and use fine-grained control over offset persistence. You\u2019ll use one consumer per partition of a topic. This consumer-per-partition pattern maximizes throughput. When consumers run in parallel and you use multiple threads per consumer you need to be sure the total number of threads across all instances do not exceed the total number of partitions in the topic. Also, a consumer can subscribe to multiple topics. The brokers are doing rebalancing of the assignment of topic-partition to a consumer that belong to a group. When creating a new consumer you can specify the group id in the options.","title":"Assess number of consumers needed"},{"location":"technology/kafka-producers-consumers/index.html#offset-management","text":"Recall that offset is just a numeric identifier of a consumer position of the last record read within a partition. Consumers periodically need to commit the offsets they have received, to present a recovery point in case of failure. To commit offset (via API or automatically) the consumer sends a message to Kafka broker to the special topic named __consumer_offsets to keep the committed offset for each partition. Consumers do a read commit for the last processed record: When a consumer starts and is assigned a partition to consume, it will start at its group's committed offset or latest or earliest as auto.offset.reset (When there is a committed offset, the auto.offset.reset property is not used). As shown in the figure below, it is possible to get duplicates if the last message processed by the consumer before crashing and committing its offset, is bigger than the last commited offset. Source: Kafka definitive guide book from Todd Palino, Gwen Shapira In the opposite, if the last committed offset is after the last processed messages and there were multiple messages returned in the poll, then those messages may be lost. It is possible to commit by calling API or by setting some properties at the consumer creation level to enable autocommit offset enable.auto.commit . When doing manual offset commit, there are two types of approaches: offsets\u2014synchronous asynchronous As soon as you are coding manual commit, it is strongly recommended to implement the ConsumerRebalanceListener interface to be able to do the state modifications when the topic is rebalanced. If a consumer fails after processing a message but before committing its offset, the committed offset information will not reflect the processing of the message. This means that the message will be processed again by the next consumer in that group to be assigned the partition. Assess if it is acceptable to loose messages from topic. If so, when a consumer restarts it will start consuming the topic from the latest committed offset within the partition allocated to itself. As storing a message to an external system and storing the offsets are two separate operations, and in case of failure between them, it is possible to have stale offsets, which will introduce duplicate messages when consumers restart to process from last known committed offset. In this case, consumer's idempotence is needed to support updating the same row in the table, or use the event timestamp as update timestamp in the database record or use other clever solution. As presented in the producer coding practice, using transaction to support \"exactly-once\", also means the consumers should read committed data only. This can be achieved by setting the isolation.level=read_committed in the consumer's configuration. The last offset will be the first message in the partition belonging to an open not yet committed transaction. This offset is known as the 'Last Stable Offset'(LSO). Finally in the case where consumers are set to auto commit, it means the offset if committed at the poll() level and if the service crashed while processing of this record as: then the record (partition 0 - offset 4) will never be processed.","title":"Offset management"},{"location":"technology/kafka-producers-consumers/index.html#producer-transaction","text":"When consuming from a Kafka topic and producing to another topic, like in Kafka Stream, but also in CQRS implementation, we can use the producer's transaction feature to send the committed offset message and the new records in the second topic in the same transaction. This can be seen as a consume-transform-produce loop pattern so that every input event is processed exactly once. An example of such pattern in done in the order management microservice - command part .","title":"Producer transaction"},{"location":"technology/kafka-producers-consumers/index.html#consumer-lag","text":"The consumer lag for a partition is the difference between the offset of the most recently published message and the consumer's committed offset. If the lag starts to grow, it means the consumer is not able to keep up with the producer's pace. The risk, is that slow consumer may fall behind, and when partition management may remove old log segments, leading the consumer to jump forward to continnue on the next log segment. Consumer may have lost messages. You can use the Kafka-consumer-groups tool to see the consumer lag.","title":"Consumer lag"},{"location":"technology/kafka-producers-consumers/index.html#kafka-useful-consumer-apis","text":"KafkaConsumer a topic consumer which support: transparently handles brokers failure transparently adapt to partition migration within the cluster support grouping for load balancing among consumers maintains TCP connections to the necessary brokers to fetch data subscribe to multiple topics and being part of consumer groups each partition is assigned to exactly one consumer in the group if a process fails, the partitions assigned to it will be reassigned to other consumers in the same group ConsumerRecords holds the list ConsumerRecord per partition for a particular topic. ConsumerRecord A key/value pair to be received from Kafka. This also consists of a topic name and a partition number from which the record is being received, an offset that points to the record in a Kafka partition, and a timestamp","title":"Kafka useful Consumer APIs"},{"location":"technology/kafka-producers-consumers/index.html#references","text":"IBM Event Streams - Consuming messages KafkaConsumer class","title":"References"},{"location":"technology/spring/index.html","text":"Spring Cloud and Spring Cloud Stream Audience : Developers Spring Cloud Spring Cloud is based on Spring boot programming model but focusing on cloud native deployment and distributed computing. As other spring boot app it includes jetty or tomcat, health checks, metrics... It supports the following patterns: Distributed/versioned configuration : externalize config in distributed system with config server. Service registration and discovery: uses Netflix Eureka, Apache Zookeeper or Consul to keep service information. Routing: supports HTTP (Open Feign or Netflix Ribbon for load balancing) and messaging (RabbitMQ and Kafka) Service-to-service calls: Sptring Cloud Gateway and Netflix Zuul is used Load balancing Circuit Breakers: based on Netflix Hystrix: if the request fails for n time, the circuit open. Global locks Leadership election and cluster state Distributed messaging It also supports pipelines for ci/cd and contract testing for interface validation. Getting started Use start.spring.io to create the application starting code using Kafka, Actuator, Cloud Stream or add the Spring Cloud BOM to your maven pom.xml file. See the Adding Spring Cloud To An Existing Spring Boot Application section. As most of the microservices expose REST resource, we may need to add the starter web: <dependency> <groupId> org.springframework.boot </groupId> <artifactId> spring-boot-starter-web </artifactId> </dependency> We also need to install the Spring Cloud CLI . Then add the Spring cloud starter as dependency. When using config server, we need to add the config client. <dependency> <groupId> org.springframework.cloud </groupId> <artifactId> spring-cloud-conflig-client </artifactId> </dependency> For centralized tracing uses, starter-sleuth, and zipkin. <dependency> <groupId> org.springframework.cloud </groupId> <artifactId> spring-cloud-starter-sleuth </artifactId> </dependency> <dependency> <groupId> org.springframework.cloud </groupId> <artifactId> spring-cloud-starter-zipkin </artifactId> </dependency> For service discovery add netflix-eureka-client. <dependency> <groupId> org.springframework.cloud </groupId> <artifactId> spring-cloud-starter-eureka </artifactId> </dependency> Using the Spring Cloud CLI we can get the service registry, config server, central tracing started in one command: spring cloud eureka configserver zipkin Spring Cloud config Use the concept of Config Server you have a central place to manage external properties for applications across all environments. As an application moves through the deployment pipeline from dev to test and into production you can manage the configuration between those environments and be certain that applications have everything they need to run when they migrate. @Value ( \"${config.in.topic}\" ) String topicName = \"orders\" ; The value of the config.in.topic comes from local configuration or remote config server. The config server will serve content from a git. See this sample for such server. Spring Cloud Stream Spring Cloud Stream is a framework for building highly scalable event-driven microservices connected with shared messaging systems. It unifies lots of popular messaging platforms behind one easy to use API including RabbitMQ, Apache Kafka, Amazon Kinesis, Google PubSub, Solace PubSub+, Azure Event Hubs, and Apache RocketMQ. Spring Cloud Stream is an abstraction that uses the following important concepts to supporet middleware encapsulation: destination binders (integration with messaging systems like Kafka or RabbitMQ), destination bindings (bridge code to external systems) and message (canonical data model to communicate between producer and consumer). As other Spring boot application, it uses extrernal properties to manage most of the configuration of the binders and binding. Spring Cloud Stream Applications are standalone executable applications that communicate over messaging middleware such as Apache Kafka and RabbitMQ. The app is using uber-jars to get the minimal required library and code The following diagram illustrates those concepts for a Spring cloud app: Attention Spring Cloud Stream is not Kafka Streams or Kafka API, it is similar but it represents another abstraction. From a Kafka developer's point of view, it does not seem relevant, as why not using Kafka API and Kafka Streams API, but this is a way to encapsulate any middleware supporting pub/sub and queueing. It may be more comparable to Microprofile reactive messaging specifications and APIs, but not compatible with it. For example binding can be compared to channel of the microprofile reactive messaging constructs. So the development decision will be around middleware abstraction and the way to simplify going from one middleware to another. Now with Kafka, because of its long retention time, it means we can have any type of consumers to read the messages at any time. Those consumers may use Kafka API (Python app or nodejs apps), in this case using the Kafka API within the Spring boot application is a better approach, as the way the abstraction is used may not be fully compatible to any Kafka consumer types. With Kafka based application the best practice is also to define the message structure, using Avro or Protbuf, and use schema registry to ensure compatibility management between applications. To support that Spring Cloud Stream support using native (to the middleware) serialization, which in the case of Kafka could be any serdes APIs or avro API. We will cover that in later section . Example of Kafka binding The order service spring cloud template is a simple example of order service that exposes CRUD operations on the Order entity via a controller. Instead of writing to a database, this service immediately generates a message to Kafka and then the repository class consumes the message to get the data to write to the database. This is a simple way to implement 'transaction' by using the Append log of Kafka partition as a transaction log. The way to generate code from a POST or an internal processing is to use StreamBridge , which exposes a send function to send the record. @Autowired private StreamBridge streamBridge ; public Order processANewOrder ( Order order ) { order . status = OrderStatus . OPEN ; order . orderID = UUID . randomUUID (). toString (); order . creationDate = LocalDate . now (); streamBridge . send ( BINDING_NAME , order ); return order ; } As a good practice is to send a Kafka Record with a Key, which is specialy needed when sending messages to a multi partition topic: The messages with the same key will always go to the same partition. If the partition key is not present, messages will be partitioned in round-robin fashion. Spring Cloud Stream is little bit confusing as it created two concepts for partitioning: the partitionKey and the message key. The partition key is the way to support the same mechanism as Kafka is doing but for other middleware. So for Kafka we do not need to use partitionKey, but then it is important to use the message key construct. As Kafka is evolving on the partition allocation, it is recommended to do not interfere with Kafka mechanims and use the following approach: Provide the message key as a SpEL expression property for example in the header: spring.cloud.stream.bindings.<binding-name>.producer.message-key-expression : headers['messageKey'] Then in your application, when publishing the message, add a header called kafka_messagekey with the value set from the attribute to use as key. Spring Cloud Stream will use the value for this header to assign it to the Kafka record Key: Message < Order > toSend = MessageBuilder . withPayload ( order ) . setHeader ( KafkaHeaders . MESSAGE_KEY , order . customerID . getBytes ()) . setHeader ( MessageHeaders . CONTENT_TYPE , MimeTypeUtils . APPLICATION_JSON ). build (); streamBridge . send ( BINDING_NAME , toSend ); You can also build composite key with a special java bean class for that and use instance of this class as key. ``` java CustomerCompanyKey cck = new CustomerCompanyKey ( order . customerID , customer . company ); Message < Order > toSend = MessageBuilder . withPayload ( order ) . setHeader ( KafkaHeaders . MESSAGE_KEY , cck ) . setHeader ( MessageHeaders . CONTENT_TYPE , MimeTypeUtils . APPLICATION_JSON ) . build (); streamBridge . send ( BINDING_NAME , toSend ); ``` The following screen shot illustrates that all records with the same \"customerID\" are in the same partition: If you want to use the partition key as an alternate way to do partition allocation using Spring Cloud Stream strategy then use a partitionKey: spring.cloud.stream.bindings.<binding-name>.producer.partition-key-expression : headers['partitionKey'] and then in the code: Message < Order > toSend = MessageBuilder . withPayload ( order ) . setHeader ( \"partitionKey\" , order . customerID . getBytes ()) . setHeader ( MessageHeaders . CONTENT_TYPE , MimeTypeUtils . APPLICATION_JSON ). build (); streamBridge . send ( BINDING_NAME , toSend ); Consuming message With the last release of Spring Cloud Stream, consumers are single beans of type Function , Consumer or Supplier . Here is an example of consumer only. @Bean public Consumer < Message < Order >> consumeOrderEvent (){ return msg -> saveOrder ( msg . getPayload ()); } For thew binding configuration the name of the method gives the name of the binding: spring.cloud.stream : bindings : consumeOrderEvent-in-0 : destination : orders contentType : application/json group : orderms-grp useNativeDecoding : true kafka : bindings : consumeOrderEvent-in-0 : consumer : ackMode : MANUAL configuration : value.deserializer : ibm.eda.demo.infrastructure.events.OrderDeserializer The deserialization is declared in a specific class: package ibm.eda.demo.infrastructure.events ; import org.springframework.kafka.support.serializer.JsonDeserializer ; public class OrderDeserializer extends JsonDeserializer < Order > { } In this example above as the goal is to save to the database, we should not auto commit the offset reading. So the following settings are needed on the consumer side: spring.cloud.stream.kafka : bindings : consumeOrderEvent-in-0 : consumer : autoCommitOffset : false startOffset : latest ackMode : MANUAL And the consumer code is now looking at the acknowledge header property if present or not and perform manual acknowledge once the save operation is successful. @Bean public Consumer < Message < Order >> consumeOrderEvent (){ return msg -> { Acknowledgment acknowledgment = msg . getHeaders (). get ( KafkaHeaders . ACKNOWLEDGMENT , Acknowledgment . class ); saveOrder ( msg . getPayload ()); if ( acknowledgment != null ) { acknowledgment . acknowledge (); } }; Kafka spring cloud stream app basic The approach to develop such application includes the following steps: A spring boot application, with REST spring web starter Define a resource and a controller for the REST API. Define inbound and/or outbound binding to communicate to underlying middleware Add method to process incoming message, taking into account the underlying middleware and serialization. For example with Kafka, most of the consumers may not auto commit the read offset but control the commit by using manual commit. Add logic to produce message using middleware To add a consumer from a Kafka topic for example, we can add a function that will process the message, and declare it as a Bean. @Bean public Consumer < Message < CloudEvent >> consumeCloudEventEvent (){ return msg -> { Acknowledgment acknowledgment = msg . getHeaders (). get ( KafkaHeaders . ACKNOWLEDGMENT , Acknowledgment . class ); saveOrder (( Order ) msg . getPayload (). getData ()); if ( acknowledgment != null ) { System . out . println ( \"Acknowledgment provided\" ); acknowledgment . acknowledge (); } }; } This previous code is also illustrating manual offset commit. Then we add configuration to link to the binders queue or topic: consumeOrderEvent-in-0 : consumer : autoCommitOffset : false startOffset : latest ackMode : MANUAL configuration : value.deserializer : ibm.eda.demo.infrastructure.events.CloudEventDeserializer Avro serialization producer : useNativeEncoding : true","title":"Spring cloud"},{"location":"technology/spring/index.html#spring-cloud-and-spring-cloud-stream","text":"Audience : Developers","title":"Spring Cloud and Spring Cloud Stream"},{"location":"technology/spring/index.html#spring-cloud","text":"Spring Cloud is based on Spring boot programming model but focusing on cloud native deployment and distributed computing. As other spring boot app it includes jetty or tomcat, health checks, metrics... It supports the following patterns: Distributed/versioned configuration : externalize config in distributed system with config server. Service registration and discovery: uses Netflix Eureka, Apache Zookeeper or Consul to keep service information. Routing: supports HTTP (Open Feign or Netflix Ribbon for load balancing) and messaging (RabbitMQ and Kafka) Service-to-service calls: Sptring Cloud Gateway and Netflix Zuul is used Load balancing Circuit Breakers: based on Netflix Hystrix: if the request fails for n time, the circuit open. Global locks Leadership election and cluster state Distributed messaging It also supports pipelines for ci/cd and contract testing for interface validation.","title":"Spring Cloud"},{"location":"technology/spring/index.html#getting-started","text":"Use start.spring.io to create the application starting code using Kafka, Actuator, Cloud Stream or add the Spring Cloud BOM to your maven pom.xml file. See the Adding Spring Cloud To An Existing Spring Boot Application section. As most of the microservices expose REST resource, we may need to add the starter web: <dependency> <groupId> org.springframework.boot </groupId> <artifactId> spring-boot-starter-web </artifactId> </dependency> We also need to install the Spring Cloud CLI . Then add the Spring cloud starter as dependency. When using config server, we need to add the config client. <dependency> <groupId> org.springframework.cloud </groupId> <artifactId> spring-cloud-conflig-client </artifactId> </dependency> For centralized tracing uses, starter-sleuth, and zipkin. <dependency> <groupId> org.springframework.cloud </groupId> <artifactId> spring-cloud-starter-sleuth </artifactId> </dependency> <dependency> <groupId> org.springframework.cloud </groupId> <artifactId> spring-cloud-starter-zipkin </artifactId> </dependency> For service discovery add netflix-eureka-client. <dependency> <groupId> org.springframework.cloud </groupId> <artifactId> spring-cloud-starter-eureka </artifactId> </dependency> Using the Spring Cloud CLI we can get the service registry, config server, central tracing started in one command: spring cloud eureka configserver zipkin","title":"Getting started"},{"location":"technology/spring/index.html#spring-cloud-config","text":"Use the concept of Config Server you have a central place to manage external properties for applications across all environments. As an application moves through the deployment pipeline from dev to test and into production you can manage the configuration between those environments and be certain that applications have everything they need to run when they migrate. @Value ( \"${config.in.topic}\" ) String topicName = \"orders\" ; The value of the config.in.topic comes from local configuration or remote config server. The config server will serve content from a git. See this sample for such server.","title":"Spring Cloud config"},{"location":"technology/spring/index.html#spring-cloud-stream","text":"Spring Cloud Stream is a framework for building highly scalable event-driven microservices connected with shared messaging systems. It unifies lots of popular messaging platforms behind one easy to use API including RabbitMQ, Apache Kafka, Amazon Kinesis, Google PubSub, Solace PubSub+, Azure Event Hubs, and Apache RocketMQ. Spring Cloud Stream is an abstraction that uses the following important concepts to supporet middleware encapsulation: destination binders (integration with messaging systems like Kafka or RabbitMQ), destination bindings (bridge code to external systems) and message (canonical data model to communicate between producer and consumer). As other Spring boot application, it uses extrernal properties to manage most of the configuration of the binders and binding. Spring Cloud Stream Applications are standalone executable applications that communicate over messaging middleware such as Apache Kafka and RabbitMQ. The app is using uber-jars to get the minimal required library and code The following diagram illustrates those concepts for a Spring cloud app: Attention Spring Cloud Stream is not Kafka Streams or Kafka API, it is similar but it represents another abstraction. From a Kafka developer's point of view, it does not seem relevant, as why not using Kafka API and Kafka Streams API, but this is a way to encapsulate any middleware supporting pub/sub and queueing. It may be more comparable to Microprofile reactive messaging specifications and APIs, but not compatible with it. For example binding can be compared to channel of the microprofile reactive messaging constructs. So the development decision will be around middleware abstraction and the way to simplify going from one middleware to another. Now with Kafka, because of its long retention time, it means we can have any type of consumers to read the messages at any time. Those consumers may use Kafka API (Python app or nodejs apps), in this case using the Kafka API within the Spring boot application is a better approach, as the way the abstraction is used may not be fully compatible to any Kafka consumer types. With Kafka based application the best practice is also to define the message structure, using Avro or Protbuf, and use schema registry to ensure compatibility management between applications. To support that Spring Cloud Stream support using native (to the middleware) serialization, which in the case of Kafka could be any serdes APIs or avro API. We will cover that in later section .","title":"Spring Cloud Stream"},{"location":"technology/spring/index.html#example-of-kafka-binding","text":"The order service spring cloud template is a simple example of order service that exposes CRUD operations on the Order entity via a controller. Instead of writing to a database, this service immediately generates a message to Kafka and then the repository class consumes the message to get the data to write to the database. This is a simple way to implement 'transaction' by using the Append log of Kafka partition as a transaction log. The way to generate code from a POST or an internal processing is to use StreamBridge , which exposes a send function to send the record. @Autowired private StreamBridge streamBridge ; public Order processANewOrder ( Order order ) { order . status = OrderStatus . OPEN ; order . orderID = UUID . randomUUID (). toString (); order . creationDate = LocalDate . now (); streamBridge . send ( BINDING_NAME , order ); return order ; } As a good practice is to send a Kafka Record with a Key, which is specialy needed when sending messages to a multi partition topic: The messages with the same key will always go to the same partition. If the partition key is not present, messages will be partitioned in round-robin fashion. Spring Cloud Stream is little bit confusing as it created two concepts for partitioning: the partitionKey and the message key. The partition key is the way to support the same mechanism as Kafka is doing but for other middleware. So for Kafka we do not need to use partitionKey, but then it is important to use the message key construct. As Kafka is evolving on the partition allocation, it is recommended to do not interfere with Kafka mechanims and use the following approach: Provide the message key as a SpEL expression property for example in the header: spring.cloud.stream.bindings.<binding-name>.producer.message-key-expression : headers['messageKey'] Then in your application, when publishing the message, add a header called kafka_messagekey with the value set from the attribute to use as key. Spring Cloud Stream will use the value for this header to assign it to the Kafka record Key: Message < Order > toSend = MessageBuilder . withPayload ( order ) . setHeader ( KafkaHeaders . MESSAGE_KEY , order . customerID . getBytes ()) . setHeader ( MessageHeaders . CONTENT_TYPE , MimeTypeUtils . APPLICATION_JSON ). build (); streamBridge . send ( BINDING_NAME , toSend ); You can also build composite key with a special java bean class for that and use instance of this class as key. ``` java CustomerCompanyKey cck = new CustomerCompanyKey ( order . customerID , customer . company ); Message < Order > toSend = MessageBuilder . withPayload ( order ) . setHeader ( KafkaHeaders . MESSAGE_KEY , cck ) . setHeader ( MessageHeaders . CONTENT_TYPE , MimeTypeUtils . APPLICATION_JSON ) . build (); streamBridge . send ( BINDING_NAME , toSend ); ``` The following screen shot illustrates that all records with the same \"customerID\" are in the same partition: If you want to use the partition key as an alternate way to do partition allocation using Spring Cloud Stream strategy then use a partitionKey: spring.cloud.stream.bindings.<binding-name>.producer.partition-key-expression : headers['partitionKey'] and then in the code: Message < Order > toSend = MessageBuilder . withPayload ( order ) . setHeader ( \"partitionKey\" , order . customerID . getBytes ()) . setHeader ( MessageHeaders . CONTENT_TYPE , MimeTypeUtils . APPLICATION_JSON ). build (); streamBridge . send ( BINDING_NAME , toSend );","title":"Example of Kafka binding"},{"location":"technology/spring/index.html#consuming-message","text":"With the last release of Spring Cloud Stream, consumers are single beans of type Function , Consumer or Supplier . Here is an example of consumer only. @Bean public Consumer < Message < Order >> consumeOrderEvent (){ return msg -> saveOrder ( msg . getPayload ()); } For thew binding configuration the name of the method gives the name of the binding: spring.cloud.stream : bindings : consumeOrderEvent-in-0 : destination : orders contentType : application/json group : orderms-grp useNativeDecoding : true kafka : bindings : consumeOrderEvent-in-0 : consumer : ackMode : MANUAL configuration : value.deserializer : ibm.eda.demo.infrastructure.events.OrderDeserializer The deserialization is declared in a specific class: package ibm.eda.demo.infrastructure.events ; import org.springframework.kafka.support.serializer.JsonDeserializer ; public class OrderDeserializer extends JsonDeserializer < Order > { } In this example above as the goal is to save to the database, we should not auto commit the offset reading. So the following settings are needed on the consumer side: spring.cloud.stream.kafka : bindings : consumeOrderEvent-in-0 : consumer : autoCommitOffset : false startOffset : latest ackMode : MANUAL And the consumer code is now looking at the acknowledge header property if present or not and perform manual acknowledge once the save operation is successful. @Bean public Consumer < Message < Order >> consumeOrderEvent (){ return msg -> { Acknowledgment acknowledgment = msg . getHeaders (). get ( KafkaHeaders . ACKNOWLEDGMENT , Acknowledgment . class ); saveOrder ( msg . getPayload ()); if ( acknowledgment != null ) { acknowledgment . acknowledge (); } };","title":"Consuming message"},{"location":"technology/spring/index.html#kafka-spring-cloud-stream-app-basic","text":"The approach to develop such application includes the following steps: A spring boot application, with REST spring web starter Define a resource and a controller for the REST API. Define inbound and/or outbound binding to communicate to underlying middleware Add method to process incoming message, taking into account the underlying middleware and serialization. For example with Kafka, most of the consumers may not auto commit the read offset but control the commit by using manual commit. Add logic to produce message using middleware To add a consumer from a Kafka topic for example, we can add a function that will process the message, and declare it as a Bean. @Bean public Consumer < Message < CloudEvent >> consumeCloudEventEvent (){ return msg -> { Acknowledgment acknowledgment = msg . getHeaders (). get ( KafkaHeaders . ACKNOWLEDGMENT , Acknowledgment . class ); saveOrder (( Order ) msg . getPayload (). getData ()); if ( acknowledgment != null ) { System . out . println ( \"Acknowledgment provided\" ); acknowledgment . acknowledge (); } }; } This previous code is also illustrating manual offset commit. Then we add configuration to link to the binders queue or topic: consumeOrderEvent-in-0 : consumer : autoCommitOffset : false startOffset : latest ackMode : MANUAL configuration : value.deserializer : ibm.eda.demo.infrastructure.events.CloudEventDeserializer","title":"Kafka spring cloud stream app basic"},{"location":"technology/spring/index.html#avro-serialization","text":"producer : useNativeEncoding : true","title":"Avro serialization"}]}